{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15184c8e",
   "metadata": {},
   "source": [
    "### 사용한 방법:\n",
    "#### 결측치 처리:\n",
    "1. 다른 값으로부터 예측 - lightGBM\n",
    "2. iterative imputer\n",
    "3. median, mean, constant, mode\n",
    "\n",
    "#### 인코딩\n",
    "1. labelencoding\n",
    "2. targetencoding\n",
    "\n",
    "#### 칼럼선택\n",
    "1. 전진선택법\n",
    "2. 후진선택법\n",
    "\n",
    "### 조합한 방법 - 성능이 좋은 순으로 나열함:\n",
    "1. iterative imputer + labelencoding + 전진선택법 + lightGBM\n",
    "2. 결측치 안채움 + labelencoding + lightGBM\n",
    "3. iterative imputer + targetencoding + 전진선택법 + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c393d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/yongchanchun/Desktop/MacBook_Pro_Desktop/TAVE/머신러닝/DACON/여행_상품_신청_여부/data'\n",
    "submission_path = 'sample_submission.csv'\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(file_path, train_path), index_col = 'id')\n",
    "test_df = pd.read_csv(os.path.join(file_path, test_path), index_col = 'id')\n",
    "sub_file = pd.read_csv(os.path.join(file_path, submission_path), index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f548fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b80592",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_na = []\n",
    "for col in train_df.columns:\n",
    "    num_na.append(train_df[train_df[col].isna()].shape[0])\n",
    "num_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f35bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female 과 Fe Male 동일시하기\n",
    "train_df.loc[train_df['Gender'] == 'Fe Male', 'Gender'] = 'Female'\n",
    "train_df[train_df['Gender'] == 'Fe Male'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6738b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Female 과 Fe Male 동일시하기\n",
    "test_df.loc[test_df['Gender'] == 'Fe Male', 'Gender'] = 'Female'\n",
    "test_df[test_df['Gender'] == 'Fe Male'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['Gender'].unique())\n",
    "print(test_df['Gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669671c",
   "metadata": {},
   "source": [
    "## train_df와 test_df의 범주 값 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(train_df.columns):\n",
    "    print(f'{i}) colname: {col}, \\n unique value of col: {train_df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(test_df.columns):\n",
    "    print(f'{i}) colname: {col}, \\n unique value of col: {test_df[col].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf993864",
   "metadata": {},
   "source": [
    "## 결측치를 제거하지 않은 상태로 GBDT를 적용해보자\n",
    "1. lightgbm 사용\n",
    "2. xgboost 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e62152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col])\n",
    "        train_df_all_label_encoded[col] = le.transform(train_df[col])\n",
    "    else:\n",
    "        train_df_all_label_encoded[col] = train_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(test_df[col])\n",
    "        test_df_all_label_encoded[col] = le.transform(test_df[col])\n",
    "    else:\n",
    "        test_df_all_label_encoded[col] = test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_all_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0c5db",
   "metadata": {},
   "source": [
    "### 1. lightgbm 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78484774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_x = train_df_all_label_encoded.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df_all_label_encoded['ProdTaken']\n",
    "lightgbm_model = lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df_all_label_encoded)\n",
    "file_name = 'lightgbm_with_null_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30ca61",
   "metadata": {},
   "source": [
    "### 2. xgboost 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=18, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19330e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_x.shape[0])\n",
    "print(va_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# 특징과 목적변수를 xgboost의 데이터 구조로 변환\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_df_all_label_encoded)\n",
    "\n",
    "\n",
    "# 매개변수의 탐색범위\n",
    "param_space = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'error',\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 9, 1),    \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "     # 여유가 있으면 alpha, lambda도 조정\n",
    "    'alpha' : hp.loguniform('alpha', np.log(1e-8), np.log(1.0)),\n",
    "    'lambda' : hp.loguniform('lambda', np.log(1e-6), np.log(10.0)),\n",
    "    'random_state': 71\n",
    "\n",
    "}\n",
    "\n",
    "num_round=500\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "# define classifier\n",
    "def classifier(value_list):\n",
    "    result = []\n",
    "    for value in value_list:\n",
    "        if value > 0.5:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    result = np.array(result)\n",
    "    return result\n",
    "    \n",
    "def score(params):\n",
    "    # max_depth의 형을 정수형으로 수정\n",
    "    params['max_depth'] = int(params['max_depth']) \n",
    "    model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=50)\n",
    "    # model.fit(tr_x, tr_y)\n",
    "    va_pred = model.predict(dvalid)\n",
    "    print(type(va_pred), type(va_y))\n",
    "    va_pred = classifier(va_pred)\n",
    "    score = accuracy_score(va_y, va_pred)\n",
    "    print(f'params: {params}, AS: {score:.4f}')\n",
    "\n",
    "    # 정보를 기록\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperopt에 의한 매개변수 탐색 실행\n",
    "max_evals = 15\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "# 기록한 정보에서 매개변수와 점수를 출력\n",
    "# (trials에서도 정보를 취득할 수 있지만 매개변수의 취득이 다소 어려움)\n",
    "history = sorted(history, key=lambda tpl: tpl[1], reverse=True)\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667ee2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'alpha': 0.00015592994331247996, 'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.006801021591153265, 'lambda': 0.17461896494143567, 'max_depth': 7, 'min_child_weight': 2.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}\n",
    "# dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "# dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round,\n",
    "                  evals=watchlist,\n",
    "                  early_stopping_rounds=50)\n",
    "\n",
    "# 최적의 결정 트리의 개수로 예측\n",
    "\n",
    "pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "pred = classifier(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = pred\n",
    "file_name = 'xgboost_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7401da3",
   "metadata": {},
   "source": [
    "### 3. 전진선택법 + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62d15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ad0a74",
   "metadata": {},
   "source": [
    "## 결측치를 제거한 상태로 GBDT를 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4326b",
   "metadata": {},
   "source": [
    "### iterative incomputer를 사용하여 결측치를 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "train_df_iter = deepcopy(train_df)\n",
    "test_df_iter = deepcopy(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98105c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn에서 제공하는 label encoder로 object형을 int형으로 labeling\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "for col in train_df_iter.columns:\n",
    "    if train_df_iter[col].dtype == 'object':\n",
    "        train_df_iter[col] = label_encoder.fit_transform(train_df_iter[col])\n",
    "\n",
    "for col in test_df_iter.columns:\n",
    "    if test_df_iter[col].dtype == 'object':\n",
    "        test_df_iter[col] = label_encoder.fit_transform(test_df_iter[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ebaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter.info()\n",
    "test_df_iter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb541c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer(estimator = LinearRegression(), \n",
    "                       tol= 1e-10, \n",
    "                       max_iter=30, \n",
    "                       verbose=2, \n",
    "                       initial_strategy='median',\n",
    "                       imputation_order='ascending')\n",
    "\n",
    "train_df_iter2 = pd.DataFrame(imp.fit_transform(train_df_iter))\n",
    "test_df_iter2 = pd.DataFrame(imp.fit_transform(test_df_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_iter2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e620f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter2.columns = train_df_iter.columns\n",
    "test_df_iter2.columns = test_df_iter.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76878a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_iter2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb356b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, col in enumerate(train_df_iter2.columns, 1):\n",
    "    # row = 5\n",
    "    plt.subplot(5,4,i)\n",
    "    plt.hist(train_df_iter2[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ea2fa",
   "metadata": {},
   "source": [
    "## 전진 선택법으로 필요없는 칼럼제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149d1ef",
   "metadata": {},
   "source": [
    "### iterative imputer를 통해 결측치 값을 채운 데이터에 먼저 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24affa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df_iter2.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df_iter2['ProdTaken']\n",
    "\n",
    "## 전진 단계별 선택법\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# feature 및 target\n",
    "variables = train_x.columns.tolist() \n",
    "y = train_y \n",
    "\n",
    "# 선택된 변수들 list 생성\n",
    "forward_variables = []\n",
    "\n",
    "# 전진선택시 P 값을 고려할 때, 선택과 제거 임계치 설정    \n",
    "sl_enter = 0.05\n",
    "sl_remove = 0.05\n",
    "\n",
    "# 각 스텝별로 선택된 변수들\n",
    "sv_per_step = [] \n",
    "# 각 스텝별 수정된 결정계수\n",
    "adj_r_squared_list = []\n",
    "# 스텝\n",
    "steps = []\n",
    "step = 0\n",
    "\n",
    "\n",
    "while len(variables) > 0:\n",
    "    remainder = list(set(variables) - set(forward_variables))\n",
    "    pval = pd.Series(index=remainder) ## 변수의 p-value\n",
    "    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n",
    "    ## 선형 모형을 적합한다.\n",
    "    for col in remainder: \n",
    "        X = train_x[forward_variables+[col]]\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y,X).fit(disp=0)\n",
    "        pval[col] = model.pvalues[col]\n",
    " \n",
    "    min_pval = pval.min()\n",
    "    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n",
    "        forward_variables.append(pval.idxmin())\n",
    "        ## 선택된 변수들에대해서\n",
    "        ## 어떤 변수를 제거할지 고른다.\n",
    "        while len(forward_variables) > 0:\n",
    "            selected_X = train_x[forward_variables]\n",
    "            selected_X = sm.add_constant(selected_X)\n",
    "            selected_pval = sm.OLS(y,selected_X).fit(disp=0).pvalues[1:] ## 절편항의 p-value는 뺀다\n",
    "            max_pval = selected_pval.max()\n",
    "            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n",
    "                remove_variable = selected_pval.idxmax()\n",
    "                forward_variables.remove(remove_variable)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        step += 1\n",
    "        steps.append(step)\n",
    "        adj_r_squared = sm.OLS(y,sm.add_constant(train_x[forward_variables])).fit(disp=0).rsquared_adj\n",
    "        adj_r_squared_list.append(adj_r_squared)\n",
    "        sv_per_step.append(forward_variables.copy())\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forward_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_forward = train_x[forward_variables]\n",
    "test_df_iter2_forward = test_df_iter2[forward_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_x_forward.columns))\n",
    "print(len(test_df_iter2_forward.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c367642",
   "metadata": {},
   "source": [
    "#### lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3f17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x_forward, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e7d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df_iter2_forward)\n",
    "file_name = 'lightgbm_with_iter_forward_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9efb8",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, train_y, test_df만 수정하면 됨\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x_forward))[0]\n",
    "tr_x, va_x = train_x_forward.iloc[tr_idx], train_x_forward.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "# 특징과 목적변수를 xgboost의 데이터 구조로 변환\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_df_iter2_forward)\n",
    "\n",
    "\n",
    "# 매개변수의 탐색범위\n",
    "param_space = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'error',\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 9, 1),    \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "     # 여유가 있으면 alpha, lambda도 조정\n",
    "    'alpha' : hp.loguniform('alpha', np.log(1e-8), np.log(1.0)),\n",
    "    'lambda' : hp.loguniform('lambda', np.log(1e-6), np.log(10.0)),\n",
    "    'random_state': 71\n",
    "\n",
    "}\n",
    "\n",
    "num_round=500\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "# define classifier\n",
    "def classifier(value_list):\n",
    "    result = []\n",
    "    for value in value_list:\n",
    "        if value > 0.5:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    result = np.array(result)\n",
    "    return result\n",
    "    \n",
    "def score(params):\n",
    "    # max_depth의 형을 정수형으로 수정\n",
    "    params['max_depth'] = int(params['max_depth']) \n",
    "    model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=50)\n",
    "    # model.fit(tr_x, tr_y)\n",
    "    va_pred = model.predict(dvalid)\n",
    "    print(type(va_pred), type(va_y))\n",
    "    va_pred = classifier(va_pred)\n",
    "    score = accuracy_score(va_y, va_pred)\n",
    "    print(f'params: {params}, AS: {score:.4f}')\n",
    "\n",
    "    # 정보를 기록\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperopt에 의한 매개변수 탐색 실행\n",
    "max_evals = 15\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "# 기록한 정보에서 매개변수와 점수를 출력\n",
    "# (trials에서도 정보를 취득할 수 있지만 매개변수의 취득이 다소 어려움)\n",
    "history = sorted(history, key=lambda tpl: tpl[1], reverse=True)\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9bc06",
   "metadata": {},
   "source": [
    "### 결측치를 제거하지 않은 데이터에 적용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded_forward = train_df_all_label_encoded[forward_variables]\n",
    "test_df_all_label_encoded_forward = test_df_all_label_encoded[forward_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_all_label_encoded_forward.shape[1])\n",
    "print(test_df_all_label_encoded_forward.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f62b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_df_all_label_encoded_forward, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df_all_label_encoded_forward)\n",
    "file_name = 'lightgbm_with_nan_forward_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3e98a",
   "metadata": {},
   "source": [
    "## 결측치 하나씩 확인해보기\n",
    "1. 결측치가 들어있는 열을 직접 확인해보기\n",
    "2. 열마다 결측치를 처리할 적절한 방법 찾아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후에 iterative imputer를 이용해 결측치를 채울 것을 대비해 데이터를 복사함\n",
    "train_df_iter3 = deepcopy(train_df)\n",
    "test_df_iter3 = deepcopy(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed843ce",
   "metadata": {},
   "source": [
    "### 1. 결측치가 들어있는 열을 직접 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14593443",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = train_df.columns\n",
    "na_col = []\n",
    "no_na_col = []\n",
    "for i, num in enumerate(num_na):\n",
    "    if num != 0:\n",
    "        na_col.append(col_names[i])\n",
    "    else:\n",
    "        no_na_col.append(col_names[i])\n",
    "\n",
    "train_df[na_col]        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683608f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[na_col].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c31338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, col in enumerate(na_col, 1):\n",
    "    row = int(np.sqrt(len(na_col)))\n",
    "    plt.subplot(int(np.sqrt(len(na_col))), int(len(na_col)/row), i)\n",
    "    if col == 'TypeofContact':\n",
    "        plt.hist(label_encoder.fit_transform(train_df[col]), bins=20)\n",
    "        plt.title(col)\n",
    "        continue\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41165b",
   "metadata": {},
   "source": [
    "### 2. 열마다 결측치를 처리할 적절한 방법 찾아보기\n",
    "1. 여행상품을 신청하는 여부를 결정하는데 중요하게 작용하는 특징 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc35aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col])\n",
    "        train_df_all_label_encoded[col] = le.transform(train_df[col])\n",
    "    else:\n",
    "        train_df_all_label_encoded[col] = train_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541deb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values = pd.DataFrame(lightgbm_model.feature_importances_)\n",
    "feature_columns = pd.DataFrame(train_x.columns)\n",
    "feature_im = pd.concat([feature_values,feature_columns],axis=1)\n",
    "feature_im.columns=['importance','column']\n",
    "feature_im=feature_im.sort_values('importance',ascending=False)\n",
    "feature_im # 랜덤포레스트 모델에서 특징중요도로 선택한 특징들을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2809b",
   "metadata": {},
   "source": [
    "2. monthlyincome, age, numberoftrips는 다른 변수로부터 결측값을 예측함 - LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['MonthlyIncome', 'Age', 'NumberOfTrips']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMRegressor(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165185c",
   "metadata": {},
   "source": [
    "3. numberoffollowups, PreferredPropertyStar, NumberOfChildrenVisiting 는 다른 변수로 부터 값을 예측함 - LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[train_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cafab",
   "metadata": {},
   "source": [
    "4. 'TypeofContact'는 다른 변수로 부터 값을 예측함 - LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ad2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['TypeofContact']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    back2col = lambda x: 'Company Invited' if x==0 else 'Self Enquiry'\n",
    "    feature_pred = list(map(back2col, feature_pred))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[train_df[col].isna()].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608f167",
   "metadata": {},
   "source": [
    "5. durationofpitch는 중앙값을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93123758",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_val = np.median(train_df[train_df['DurationOfPitch'].isna().apply(lambda x:not x)]['DurationOfPitch'])\n",
    "train_df.loc[train_df[train_df['DurationOfPitch'].isna()].index,'DurationOfPitch'] = med_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()\n",
    "# 모든 결측치 값 채워넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62221425",
   "metadata": {},
   "source": [
    "6. test data도 같은 방식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['MonthlyIncome', 'Age', 'NumberOfTrips']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMRegressor(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1019870",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[test_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_na_col = ['TypeofContact']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    back2col = lambda x: 'Company Invited' if x==0 else 'Self Enquiry'\n",
    "    feature_pred = list(map(back2col, feature_pred))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[test_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_val = np.median(test_df[test_df['DurationOfPitch'].isna().apply(lambda x:not x)]['DurationOfPitch'])\n",
    "test_df.loc[test_df[test_df['DurationOfPitch'].isna()].index,'DurationOfPitch'] = med_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5cdb6",
   "metadata": {},
   "source": [
    "## 3. 범주형 데이터 변환\n",
    "### 타깃 인코딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef61405",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_col = []\n",
    "for col in train_df.columns:\n",
    "    # print(type(train_df[col].dtypes))\n",
    "    if train_df[col].dtypes == 'object':\n",
    "        obj_col.append(col)\n",
    "\n",
    "categorical_col = ['CityTier', 'NumberOfPersonVisiting', 'Passport', 'PitchSatisfactionScore', \n",
    "                   'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting',\n",
    "                  'OwnCar']\n",
    "obj_col += categorical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "for c in obj_col:\n",
    "    # target encode test data \n",
    "    data_tmp = pd.DataFrame({c:train_df[c], 'target': train_df['ProdTaken']})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    test_df[c] = test_df[c].map(target_mean)\n",
    "    \n",
    "    # target encode train data\n",
    "    tmp = np.repeat(np.nan, train_df.shape[0])\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_df):\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        tmp[idx_2] = train_df[c].iloc[idx_2].map(target_mean)\n",
    "        \n",
    "    train_df[c] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b58598",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dedd5",
   "metadata": {},
   "source": [
    "## 4. 결측치 채움 + 타깃인코딩한 데이터에 GBDT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6efe3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_x = train_df.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df['ProdTaken']\n",
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df)\n",
    "file_name = 'lightgbm_without_null_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f54110",
   "metadata": {},
   "source": [
    "#### 성능이 결측치 채움 + 타깃인코딩한 데이터에서 더 좋지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb307b4",
   "metadata": {},
   "source": [
    "## 4.5 결측치를 iterative imputer로 채움 + 전진선택법 + 타깃인코딩한 데이터에 GBDT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_col = []\n",
    "for col in train_df_iter3.columns:\n",
    "    # print(type(train_df_iter3[col].dtypes))\n",
    "    if train_df_iter3[col].dtypes == 'object':\n",
    "        obj_col.append(col)\n",
    "\n",
    "categorical_col = ['CityTier', 'NumberOfPersonVisiting', 'Passport', 'PitchSatisfactionScore', \n",
    "                   'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting',\n",
    "                  'OwnCar']\n",
    "\n",
    "obj_col += categorical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "for c in obj_col:\n",
    "    # target encode test data \n",
    "    data_tmp = pd.DataFrame({c:train_df_iter3[c], 'target': train_df_iter3['ProdTaken']})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    test_df_iter3[c] = test_df_iter3[c].map(target_mean)\n",
    "    \n",
    "    # target encode train data\n",
    "    tmp = np.repeat(np.nan, train_df_iter3.shape[0])\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_df_iter3):\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        tmp[idx_2] = train_df_iter3[c].iloc[idx_2].map(target_mean)\n",
    "        \n",
    "    train_df_iter3[c] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_iter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb90d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = IterativeImputer(estimator = LinearRegression(), \n",
    "                       tol= 1e-10, \n",
    "                       max_iter=30, \n",
    "                       verbose=2, \n",
    "                       initial_strategy='median',\n",
    "                       imputation_order='ascending')\n",
    "\n",
    "train_df_iter4 = pd.DataFrame(imp.fit_transform(train_df_iter3))\n",
    "test_df_iter4 = pd.DataFrame(imp.fit_transform(test_df_iter3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ac638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter4.columns = train_df.columns\n",
    "test_df_iter4.columns = test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f42cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_iter4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47d6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df_iter4.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df_iter4['ProdTaken']\n",
    "\n",
    "## 전진 단계별 선택법\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# feature 및 target\n",
    "variables = train_x.columns.tolist() \n",
    "y = train_y \n",
    "\n",
    "# 선택된 변수들 list 생성\n",
    "forward_variables = []\n",
    "\n",
    "# 전진선택시 P 값을 고려할 때, 선택과 제거 임계치 설정    \n",
    "sl_enter = 0.05\n",
    "sl_remove = 0.05\n",
    "\n",
    "# 각 스텝별로 선택된 변수들\n",
    "sv_per_step = [] \n",
    "# 각 스텝별 수정된 결정계수\n",
    "adj_r_squared_list = []\n",
    "# 스텝\n",
    "steps = []\n",
    "step = 0\n",
    "\n",
    "\n",
    "while len(variables) > 0:\n",
    "    remainder = list(set(variables) - set(forward_variables))\n",
    "    pval = pd.Series(index=remainder) ## 변수의 p-value\n",
    "    ## 기존에 포함된 변수와 새로운 변수 하나씩 돌아가면서 \n",
    "    ## 선형 모형을 적합한다.\n",
    "    for col in remainder: \n",
    "        X = train_x[forward_variables+[col]]\n",
    "        X = sm.add_constant(X)\n",
    "        model = sm.OLS(y,X).fit(disp=0)\n",
    "        pval[col] = model.pvalues[col]\n",
    " \n",
    "    min_pval = pval.min()\n",
    "    if min_pval < sl_enter: ## 최소 p-value 값이 기준 값보다 작으면 포함\n",
    "        forward_variables.append(pval.idxmin())\n",
    "        ## 선택된 변수들에대해서\n",
    "        ## 어떤 변수를 제거할지 고른다.\n",
    "        while len(forward_variables) > 0:\n",
    "            selected_X = train_x[forward_variables]\n",
    "            selected_X = sm.add_constant(selected_X)\n",
    "            selected_pval = sm.OLS(y,selected_X).fit(disp=0).pvalues[1:] ## 절편항의 p-value는 뺀다\n",
    "            max_pval = selected_pval.max()\n",
    "            if max_pval >= sl_remove: ## 최대 p-value값이 기준값보다 크거나 같으면 제외\n",
    "                remove_variable = selected_pval.idxmax()\n",
    "                forward_variables.remove(remove_variable)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        step += 1\n",
    "        steps.append(step)\n",
    "        adj_r_squared = sm.OLS(y,sm.add_constant(train_x[forward_variables])).fit(disp=0).rsquared_adj\n",
    "        adj_r_squared_list.append(adj_r_squared)\n",
    "        sv_per_step.append(forward_variables.copy())\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fa075",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(forward_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d150444",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_forward = train_x[forward_variables]\n",
    "test_df_iter4_forward = test_df_iter4[forward_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1666750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x_forward, train_y)\n",
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df_iter4_forward)\n",
    "file_name = 'lightgbm_with_targetencoded_iter_forward_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8aa990",
   "metadata": {},
   "source": [
    "## 5. 신경망 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "train_col = train_df.columns\n",
    "for i, col in enumerate(train_col, 1):\n",
    "    row = int(np.sqrt(len(train_col)))\n",
    "    plt.subplot(row, int(len(train_col)/row)+1, i)\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffa793",
   "metadata": {},
   "source": [
    "num_cols = ['Age', 'DurationOfPitch', 'NumberOfTrips', 'MonthlyIncome']\n",
    "1. Age는 rankgauss 방법을 이용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "transformer.fit(pd.concat([train_df[['Age']], test_df[['Age']]]))\n",
    "\n",
    "train_df[['Age']] = transformer.transform(train_df[['Age']])\n",
    "test_df[['Age']] = transformer.transform(test_df[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaca1c",
   "metadata": {},
   "source": [
    "2. DurationOfPitch는 박스-칵스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "pt.fit(pd.concat([train_df[['DurationOfPitch']], test_df[['DurationOfPitch']]]))\n",
    "\n",
    "train_df[['DurationOfPitch']] = pt.transform(train_df[['DurationOfPitch']])\n",
    "test_df[['DurationOfPitch']] = pt.transform(test_df[['DurationOfPitch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f28b9",
   "metadata": {},
   "source": [
    "3. NumberOfTrips, MonthlyIncome는 클리핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p01 = pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]).quantile(0.01)\n",
    "p99 = pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]).quantile(0.90)\n",
    "\n",
    "train_df[['NumberOfTrips', 'MonthlyIncome']] = train_df[['NumberOfTrips', 'MonthlyIncome']].clip(p01, p99, axis=1)\n",
    "test_df[['NumberOfTrips', 'MonthlyIncome']] = test_df[['NumberOfTrips', 'MonthlyIncome']].clip(p01, p99, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f4b3f",
   "metadata": {},
   "source": [
    "4. NumberOfTrips, MonthlyIncome는 rankgauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "transformer.fit(pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]))\n",
    "\n",
    "train_df[['NumberOfTrips', 'MonthlyIncome']] = transformer.transform(train_df[['NumberOfTrips', 'MonthlyIncome']])\n",
    "test_df[['NumberOfTrips', 'MonthlyIncome']] = transformer.transform(test_df[['NumberOfTrips', 'MonthlyIncome']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ee267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "train_col = train_df.columns\n",
    "for i, col in enumerate(train_col, 1):\n",
    "    row = int(np.sqrt(len(train_col)))\n",
    "    plt.subplot(row, int(len(train_col)/row)+1, i)\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e4d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "test_col = test_df.columns\n",
    "for i, col in enumerate(test_col, 1):\n",
    "    row = int(np.sqrt(len(test_col)))\n",
    "    plt.subplot(row, int(len(test_col)/row)+1, i)\n",
    "    plt.hist(test_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb53399",
   "metadata": {},
   "source": [
    "5. 신경망 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[-train_df['Occupation'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c215cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[-train_df['NumberOfPersonVisiting'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b447b",
   "metadata": {},
   "source": [
    "### pytorch 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F                       \n",
    "from sklearn.metrics import mean_squared_error   \n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "x = train_df.drop('ProdTaken', axis=1).to_numpy()\n",
    "y = train_df[['ProdTaken']].astype('float64').to_numpy().reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.y_data = torch.FloatTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.x_data[index], self.y_data[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorData(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class travelNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(travelNet, self).__init__()\n",
    "        self.l1 = nn.Linear(18, 192, bias=True)\n",
    "        self.l2 = nn.Linear(192, 150, bias=True)\n",
    "        self.l3 = nn.Linear(150, 100, bias=True)\n",
    "        self.l4 = nn.Linear(100, 50, bias=True)\n",
    "        self.l5 = nn.Linear(50, 2, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm1d\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.classifier = torch.softmax\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.classifier(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = travelNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "lr3 = 0.0001\n",
    "\n",
    "params_ft = []\n",
    "params_ft.append({'params': mlp_model.l1.parameters(), 'lr': lr3})\n",
    "params_ft.append({'params': mlp_model.l2.parameters(), 'lr': lr2})\n",
    "params_ft.append({'params': mlp_model.l3.parameters(), 'lr': lr2})\n",
    "params_ft.append({'params': mlp_model.l4.parameters(), 'lr': lr1})\n",
    "\n",
    "optimizer = optim.Adam(params_ft)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "epoch_num = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c95bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train(True)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f'Epoch {epoch+1}/{epoch_num} is running ---------------')\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model.forward(data)\n",
    "        \n",
    "        # view([r, c])는 크기 -1이 아닌 [r, c]만큼의 행렬로 만들어준다.\n",
    "        label = label.view([1,-1]).squeeze()\n",
    "        label = label.long()\n",
    "        \n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "    \n",
    "    # print(running_loss)\n",
    "    epoch_loss = running_loss / i\n",
    "    epoch_corrects = running_corrects / train_x.shape[0]\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'loss:{epoch_loss}, accuracy:{epoch_corrects}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dec18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TensorData(valid_x, valid_y)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train(False)\n",
    "#mlp_model.dropout = nn.Sequential()\n",
    "\n",
    "for i, (data, label) in enumerate(valid_dataloader):\n",
    "    \n",
    "    output = mlp_model.forward(data)\n",
    "    label = label.view([1,-1]).squeeze()\n",
    "    label = label.long()\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    \n",
    "accuracy = torch.sum(preds == label) / len(valid_dataset)\n",
    "print(f'accuracy:{accuracy}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee5b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_dataset = TensorData(x, y)\n",
    "total_train_dataloader = DataLoader(total_train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = travelNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "lr3 = 0.0001\n",
    "best_lr = 7.9094022518724e-05\n",
    "\n",
    "params_ft = []\n",
    "params_ft.append({'params': mlp_model.l1.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l2.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l3.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l4.parameters(), 'lr': best_lr})\n",
    "\n",
    "optimizer = optim.Adam(params_ft)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "epoch_num = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5efea",
   "metadata": {},
   "source": [
    "'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.05, 'optimizer': {'lr': 7.9094022518724e-05, 'type': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train(True)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f'Epoch {epoch+1}/{epoch_num} is running ---------------')\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(total_train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model.forward(data)\n",
    "        \n",
    "        # view([r, c])는 크기 -1이 아닌 [r, c]만큼의 행렬로 만들어준다.\n",
    "        label = label.view([1,-1]).squeeze()\n",
    "        label = label.long()\n",
    "        \n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "    \n",
    "    # print(running_loss)\n",
    "    epoch_loss = running_loss / train_x.shape[0]\n",
    "    epoch_corrects = running_corrects / train_x.shape[0]\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'loss:{epoch_loss}, accuracy:{epoch_corrects}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_torch = np.zeros(test_df.shape[0], dtype=float)\n",
    "test_dataset = TensorData(test_df.to_numpy(), empty_torch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train(False)\n",
    "\n",
    "for i, (data,label) in enumerate(test_dataloader):\n",
    "    \n",
    "    output = mlp_model.forward(data)\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    \n",
    "sub_file['ProdTaken'] = preds.detach().numpy()\n",
    "file_name = 'mlp_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bf619",
   "metadata": {},
   "source": [
    "### keras 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from hyperopt import hp\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 기본이 되는 매개변수\n",
    "base_param = {\n",
    "    'input_dropout': 0.0,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_units': 96,\n",
    "    'hidden_activation': 'relu',\n",
    "    'hidden_dropout': 0.2,\n",
    "    'batch_norm': 'before_act',\n",
    "    'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "# 탐색할 매개변수 공간을 지정\n",
    "param_space = {\n",
    "    'input_dropout': hp.quniform('input_dropout', 0, 0.2, 0.05),\n",
    "    'hidden_layers': hp.quniform('hidden_layers', 2, 4, 1),\n",
    "    'hidden_units': hp.quniform('hidden_units', 32, 256, 32),\n",
    "    'hidden_activation': hp.choice('hidden_activation', ['prelu', 'relu']),\n",
    "    'hidden_dropout': hp.quniform('hidden_dropout', 0, 0.3, 0.05),\n",
    "    'batch_norm': hp.choice('batch_norm', ['before_act', 'no']),\n",
    "    'optimizer': hp.choice('optimizer',\n",
    "                           [{'type': 'adam',\n",
    "                             'lr': hp.loguniform('adam_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'sgd',\n",
    "                             'lr': hp.loguniform('sgd_lr', np.log(0.00001), np.log(0.01))}]),\n",
    "    'batch_size': hp.quniform('batch_size', 32, 128, 32),\n",
    "}\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "\n",
    "        # 매개변수\n",
    "        input_dropout = self.params['input_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "\n",
    "        # 표준화\n",
    "#         self.scaler = StandardScaler()\n",
    "#         tr_x = self.scaler.fit_transform(tr_x)\n",
    "#         va_x = self.scaler.transform(va_x)\n",
    "\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # 입력계층\n",
    "        self.model.add(Dropout(input_dropout, input_shape=(tr_x.shape[1],)))\n",
    "\n",
    "        # 은닉계층\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 출력 계층\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # 최적화(옵티마이저)\n",
    "        if optimizer_type == 'sgd':\n",
    "            optimizer = SGD(lr=optimizer_lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, decay=0.)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 목적함수, 평가지표 등의 설정\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # 에폭 수, 조기 종료\n",
    "        # 에폭을 너무 크게 하면 작은 학습률일 때 끝나지 않을 수 있으므로 주의\n",
    "        nb_epoch = 200\n",
    "        patience = 20\n",
    "        early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "        # 학습의 실행\n",
    "        history = self.model.fit(tr_x, tr_y,\n",
    "                                 epochs=nb_epoch,\n",
    "                                 batch_size=batch_size, verbose=1,\n",
    "                                 validation_data=(va_x, va_y),\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 예측\n",
    "        # x = self.scaler.transform(x)\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred = y_pred.flatten()\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 매개변수 튜닝의 실행\n",
    "# -----------------------------------\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    # 매개변수 셋을 지정했을 때, 최소화해야 할 함수를 지정\n",
    "    # 모델의 매개변수 탐색에서는 모델에 매개변수를 지정하여 학습예측한 경우의 점수로 함\n",
    "    model = MLP(params)\n",
    "    model.fit(train_x, train_y, valid_x, valid_y)\n",
    "    valid_pred = model.predict(valid_x)\n",
    "    score = log_loss(valid_y, valid_pred)\n",
    "    print(f'params: {params}, logloss: {score:.4f}')\n",
    "\n",
    "    # 정보를 기록\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperopt에 의한 매개변수 탐색의 실행\n",
    "max_evals = 10\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "# 기록한 정보에서 매개변수와 점수를 출력\n",
    "# trials에서도 정보를 취득할 수 있지만 매개변수를 취득하기 어려움\n",
    "history = sorted(history, key=lambda tpl: tpl[1])\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87073c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.read_csv(os.path.join(file_path, train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_df_2.columns:\n",
    "    print(f'col:{col}, uniuqe: {train_df_2[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae24c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
