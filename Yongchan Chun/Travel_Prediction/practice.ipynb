{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c393d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cc955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/yongchanchun/Desktop/MacBook_Pro_Desktop/TAVE/머신러닝/DACON/여행_상품_신청_여부/data'\n",
    "submission_path = 'sample_submission.csv'\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5601f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(file_path, train_path), index_col = 'id')\n",
    "test_df = pd.read_csv(os.path.join(file_path, test_path), index_col = 'id')\n",
    "sub_file = pd.read_csv(os.path.join(file_path, submission_path), index_col = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0614a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1955 entries, 1 to 1955\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       1861 non-null   float64\n",
      " 1   TypeofContact             1945 non-null   object \n",
      " 2   CityTier                  1955 non-null   int64  \n",
      " 3   DurationOfPitch           1853 non-null   float64\n",
      " 4   Occupation                1955 non-null   object \n",
      " 5   Gender                    1955 non-null   object \n",
      " 6   NumberOfPersonVisiting    1955 non-null   int64  \n",
      " 7   NumberOfFollowups         1942 non-null   float64\n",
      " 8   ProductPitched            1955 non-null   object \n",
      " 9   PreferredPropertyStar     1945 non-null   float64\n",
      " 10  MaritalStatus             1955 non-null   object \n",
      " 11  NumberOfTrips             1898 non-null   float64\n",
      " 12  Passport                  1955 non-null   int64  \n",
      " 13  PitchSatisfactionScore    1955 non-null   int64  \n",
      " 14  OwnCar                    1955 non-null   int64  \n",
      " 15  NumberOfChildrenVisiting  1928 non-null   float64\n",
      " 16  Designation               1955 non-null   object \n",
      " 17  MonthlyIncome             1855 non-null   float64\n",
      " 18  ProdTaken                 1955 non-null   int64  \n",
      "dtypes: float64(7), int64(6), object(6)\n",
      "memory usage: 305.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f548fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>20384.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19599.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21274.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19907.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>20723.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Super Deluxe</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AVP</td>\n",
       "      <td>31595.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21651.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>22218.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>17853.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age    TypeofContact  CityTier  DurationOfPitch      Occupation  \\\n",
       "id                                                                       \n",
       "1     28.0  Company Invited         1             10.0  Small Business   \n",
       "2     34.0     Self Enquiry         3              NaN  Small Business   \n",
       "3     45.0  Company Invited         1              NaN        Salaried   \n",
       "4     29.0  Company Invited         1              7.0  Small Business   \n",
       "5     42.0     Self Enquiry         3              6.0        Salaried   \n",
       "...    ...              ...       ...              ...             ...   \n",
       "1951  28.0     Self Enquiry         1             10.0  Small Business   \n",
       "1952  41.0     Self Enquiry         3              8.0        Salaried   \n",
       "1953  38.0  Company Invited         3             28.0  Small Business   \n",
       "1954  28.0     Self Enquiry         3             30.0  Small Business   \n",
       "1955  22.0  Company Invited         1              9.0        Salaried   \n",
       "\n",
       "      Gender  NumberOfPersonVisiting  NumberOfFollowups ProductPitched  \\\n",
       "id                                                                       \n",
       "1       Male                       3                4.0          Basic   \n",
       "2     Female                       2                4.0         Deluxe   \n",
       "3       Male                       2                3.0         Deluxe   \n",
       "4       Male                       3                5.0          Basic   \n",
       "5       Male                       2                3.0         Deluxe   \n",
       "...      ...                     ...                ...            ...   \n",
       "1951    Male                       3                5.0          Basic   \n",
       "1952  Female                       3                3.0   Super Deluxe   \n",
       "1953  Female                       3                4.0          Basic   \n",
       "1954  Female                       3                5.0         Deluxe   \n",
       "1955    Male                       2                4.0          Basic   \n",
       "\n",
       "      PreferredPropertyStar MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                   \n",
       "1                       3.0       Married            3.0         0   \n",
       "2                       4.0        Single            1.0         1   \n",
       "3                       4.0       Married            2.0         0   \n",
       "4                       4.0       Married            3.0         0   \n",
       "5                       3.0      Divorced            2.0         0   \n",
       "...                     ...           ...            ...       ...   \n",
       "1951                    3.0        Single            2.0         0   \n",
       "1952                    5.0      Divorced            1.0         0   \n",
       "1953                    3.0      Divorced            7.0         0   \n",
       "1954                    3.0       Married            3.0         0   \n",
       "1955                    3.0      Divorced            1.0         1   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting Designation  \\\n",
       "id                                                                           \n",
       "1                          1       0                       1.0   Executive   \n",
       "2                          5       1                       0.0     Manager   \n",
       "3                          4       1                       0.0     Manager   \n",
       "4                          4       0                       1.0   Executive   \n",
       "5                          3       1                       0.0     Manager   \n",
       "...                      ...     ...                       ...         ...   \n",
       "1951                       1       1                       2.0   Executive   \n",
       "1952                       5       1                       1.0         AVP   \n",
       "1953                       2       1                       2.0   Executive   \n",
       "1954                       1       1                       2.0     Manager   \n",
       "1955                       3       0                       0.0   Executive   \n",
       "\n",
       "      MonthlyIncome  ProdTaken  \n",
       "id                              \n",
       "1           20384.0          0  \n",
       "2           19599.0          1  \n",
       "3               NaN          0  \n",
       "4           21274.0          1  \n",
       "5           19907.0          0  \n",
       "...             ...        ...  \n",
       "1951        20723.0          0  \n",
       "1952        31595.0          0  \n",
       "1953        21651.0          0  \n",
       "1954        22218.0          0  \n",
       "1955        17853.0          1  \n",
       "\n",
       "[1955 rows x 19 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd96ee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'TypeofContact', 'CityTier', 'DurationOfPitch', 'Occupation',\n",
       "       'Gender', 'NumberOfPersonVisiting', 'NumberOfFollowups',\n",
       "       'ProductPitched', 'PreferredPropertyStar', 'MaritalStatus',\n",
       "       'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar',\n",
       "       'NumberOfChildrenVisiting', 'Designation', 'MonthlyIncome',\n",
       "       'ProdTaken'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17b80592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94, 10, 0, 102, 0, 0, 0, 13, 0, 10, 0, 57, 0, 0, 0, 27, 0, 100, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_na = []\n",
    "for col in train_df.columns:\n",
    "    num_na.append(train_df[train_df[col].isna()].shape[0])\n",
    "num_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf993864",
   "metadata": {},
   "source": [
    "## 결측치를 제거하지 않은 상태로 GBDT를 적용해보자\n",
    "1. lightgbm 사용\n",
    "2. xgboost 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e62152",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col])\n",
    "        train_df_all_label_encoded[col] = le.transform(train_df[col])\n",
    "    else:\n",
    "        train_df_all_label_encoded[col] = train_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34ffdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(test_df[col])\n",
    "        test_df_all_label_encoded[col] = le.transform(test_df[col])\n",
    "    else:\n",
    "        test_df_all_label_encoded[col] = test_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9a87a21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19668.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>21334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2933 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  TypeofContact  CityTier  DurationOfPitch  Occupation  Gender  \\\n",
       "id                                                                         \n",
       "1     32.0              0         3              NaN           3       2   \n",
       "2     46.0              1         2             11.0           3       2   \n",
       "3     37.0              1         3             22.0           3       2   \n",
       "4     43.0              1         1             36.0           3       2   \n",
       "5     25.0              1         3              7.0           1       1   \n",
       "...    ...            ...       ...              ...         ...     ...   \n",
       "2929  54.0              1         1              6.0           3       1   \n",
       "2930  33.0              1         1              9.0           3       0   \n",
       "2931  33.0              0         1             31.0           2       2   \n",
       "2932  26.0              1         1              9.0           3       2   \n",
       "2933  31.0              1         1              9.0           2       2   \n",
       "\n",
       "      NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "id                                                                \n",
       "1                          2                5.0               1   \n",
       "2                          3                NaN               1   \n",
       "3                          3                4.0               1   \n",
       "4                          3                6.0               1   \n",
       "5                          4                4.0               0   \n",
       "...                      ...                ...             ...   \n",
       "2929                       2                3.0               4   \n",
       "2930                       4                2.0               1   \n",
       "2931                       4                4.0               1   \n",
       "2932                       4                2.0               0   \n",
       "2933                       3                5.0               1   \n",
       "\n",
       "      PreferredPropertyStar  MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                    \n",
       "1                       3.0              1            1.0         0   \n",
       "2                       4.0              1            1.0         1   \n",
       "3                       3.0              1            5.0         0   \n",
       "4                       3.0              3            6.0         0   \n",
       "5                       4.0              3            3.0         1   \n",
       "...                     ...            ...            ...       ...   \n",
       "2929                    3.0              2            7.0         0   \n",
       "2930                    3.0              3            2.0         0   \n",
       "2931                    3.0              0            3.0         0   \n",
       "2932                    5.0              3            2.0         0   \n",
       "2933                    3.0              0            3.0         0   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting  Designation  \\\n",
       "id                                                                            \n",
       "1                          2       0                       1.0            2   \n",
       "2                          5       0                       1.0            2   \n",
       "3                          5       1                       0.0            2   \n",
       "4                          3       1                       2.0            2   \n",
       "5                          4       1                       3.0            1   \n",
       "...                      ...     ...                       ...          ...   \n",
       "2929                       4       1                       1.0            0   \n",
       "2930                       3       0                       1.0            2   \n",
       "2931                       4       1                       1.0            2   \n",
       "2932                       2       1                       3.0            1   \n",
       "2933                       4       1                       1.0            2   \n",
       "\n",
       "      MonthlyIncome  \n",
       "id                   \n",
       "1           19668.0  \n",
       "2           20021.0  \n",
       "3           21334.0  \n",
       "4           22950.0  \n",
       "5           21880.0  \n",
       "...             ...  \n",
       "2929        32328.0  \n",
       "2930        23733.0  \n",
       "2931        23987.0  \n",
       "2932        22102.0  \n",
       "2933        22830.0  \n",
       "\n",
       "[2933 rows x 18 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_all_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "544a6e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2933 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ProdTaken\n",
       "id             \n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "5             0\n",
       "...         ...\n",
       "2929          0\n",
       "2930          0\n",
       "2931          0\n",
       "2932          0\n",
       "2933          0\n",
       "\n",
       "[2933 rows x 1 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0c5db",
   "metadata": {},
   "source": [
    "### 1. lightgbm 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78484774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(n_estimators=1000, random_state=777)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(n_estimators=1000, random_state=777)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(n_estimators=1000, random_state=777)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_x = train_df_all_label_encoded.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df_all_label_encoded['ProdTaken']\n",
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3dd4a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df_all_label_encoded)\n",
    "file_name = 'lightgbm_with_null_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30ca61",
   "metadata": {},
   "source": [
    "### 2. xgboost 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be0a07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "14aee38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.15026\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.13811\teval-error:0.16368                                      \n",
      "[2]\ttrain-error:0.13875\teval-error:0.15601                                      \n",
      "[3]\ttrain-error:0.13491\teval-error:0.15345                                      \n",
      "[4]\ttrain-error:0.12468\teval-error:0.15601                                      \n",
      "[5]\ttrain-error:0.12596\teval-error:0.15345                                      \n",
      "[6]\ttrain-error:0.12660\teval-error:0.15601                                      \n",
      "[7]\ttrain-error:0.12340\teval-error:0.16880                                      \n",
      "[8]\ttrain-error:0.11893\teval-error:0.15090                                      \n",
      "[9]\ttrain-error:0.11637\teval-error:0.14834                                      \n",
      "[10]\ttrain-error:0.11765\teval-error:0.15090                                     \n",
      "[11]\ttrain-error:0.11509\teval-error:0.14834                                     \n",
      "[12]\ttrain-error:0.11189\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.11381\teval-error:0.14834                                     \n",
      "[14]\ttrain-error:0.11317\teval-error:0.14322                                     \n",
      "[15]\ttrain-error:0.10997\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.11125\teval-error:0.14322                                     \n",
      "[17]\ttrain-error:0.11253\teval-error:0.14578                                     \n",
      "[18]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[19]\ttrain-error:0.10870\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.10742\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.10486\teval-error:0.13555                                     \n",
      "[22]\ttrain-error:0.10294\teval-error:0.13299                                     \n",
      "[23]\ttrain-error:0.10486\teval-error:0.13811                                     \n",
      "[24]\ttrain-error:0.09974\teval-error:0.13811                                     \n",
      "[25]\ttrain-error:0.09591\teval-error:0.13555                                     \n",
      "[26]\ttrain-error:0.09399\teval-error:0.13043                                     \n",
      "[27]\ttrain-error:0.08823\teval-error:0.13043                                     \n",
      "[28]\ttrain-error:0.08696\teval-error:0.13043                                     \n",
      "[29]\ttrain-error:0.08760\teval-error:0.12788                                     \n",
      "[30]\ttrain-error:0.08696\teval-error:0.13043                                     \n",
      "[31]\ttrain-error:0.08760\teval-error:0.13299                                     \n",
      "[32]\ttrain-error:0.08504\teval-error:0.13043                                     \n",
      "[33]\ttrain-error:0.08184\teval-error:0.13043                                     \n",
      "[34]\ttrain-error:0.08120\teval-error:0.13299                                     \n",
      "[35]\ttrain-error:0.07992\teval-error:0.13299                                     \n",
      "[36]\ttrain-error:0.07673\teval-error:0.13043                                     \n",
      "[37]\ttrain-error:0.07289\teval-error:0.13043                                     \n",
      "[38]\ttrain-error:0.07289\teval-error:0.12788                                     \n",
      "[39]\ttrain-error:0.06969\teval-error:0.13043                                     \n",
      "[40]\ttrain-error:0.07097\teval-error:0.13043                                     \n",
      "[41]\ttrain-error:0.07161\teval-error:0.13555                                     \n",
      "[42]\ttrain-error:0.06969\teval-error:0.13555                                     \n",
      "[43]\ttrain-error:0.06714\teval-error:0.13811                                     \n",
      "[44]\ttrain-error:0.06714\teval-error:0.13043                                     \n",
      "[45]\ttrain-error:0.06458\teval-error:0.13555                                     \n",
      "[46]\ttrain-error:0.06650\teval-error:0.13299                                     \n",
      "[47]\ttrain-error:0.06266\teval-error:0.13043                                     \n",
      "[48]\ttrain-error:0.06330\teval-error:0.13299                                     \n",
      "[49]\ttrain-error:0.05818\teval-error:0.13555                                     \n",
      "[50]\ttrain-error:0.05946\teval-error:0.13299                                     \n",
      "[51]\ttrain-error:0.05754\teval-error:0.13299                                     \n",
      "[52]\ttrain-error:0.05882\teval-error:0.13043                                     \n",
      "[53]\ttrain-error:0.05946\teval-error:0.13043                                     \n",
      "[54]\ttrain-error:0.05690\teval-error:0.13043                                     \n",
      "[55]\ttrain-error:0.05243\teval-error:0.13043                                     \n",
      "[56]\ttrain-error:0.05115\teval-error:0.13299                                     \n",
      "[57]\ttrain-error:0.04987\teval-error:0.13299                                     \n",
      "[58]\ttrain-error:0.04923\teval-error:0.13555                                     \n",
      "[59]\ttrain-error:0.04859\teval-error:0.13043                                     \n",
      "[60]\ttrain-error:0.04732\teval-error:0.13043                                     \n",
      "[61]\ttrain-error:0.04795\teval-error:0.13555                                     \n",
      "[62]\ttrain-error:0.04732\teval-error:0.14067                                     \n",
      "[63]\ttrain-error:0.04732\teval-error:0.14067                                     \n",
      "[64]\ttrain-error:0.04732\teval-error:0.13299                                     \n",
      "[65]\ttrain-error:0.04795\teval-error:0.12788                                     \n",
      "[66]\ttrain-error:0.04668\teval-error:0.13299                                     \n",
      "[67]\ttrain-error:0.04540\teval-error:0.13811                                     \n",
      "[68]\ttrain-error:0.04540\teval-error:0.13299                                     \n",
      "[69]\ttrain-error:0.04412\teval-error:0.13555                                     \n",
      "[70]\ttrain-error:0.04412\teval-error:0.13043                                     \n",
      "[71]\ttrain-error:0.04412\teval-error:0.12788                                     \n",
      "[72]\ttrain-error:0.04348\teval-error:0.12788                                     \n",
      "[73]\ttrain-error:0.04284\teval-error:0.12788                                     \n",
      "[74]\ttrain-error:0.04220\teval-error:0.13299                                     \n",
      "[75]\ttrain-error:0.04156\teval-error:0.13043                                     \n",
      "[76]\ttrain-error:0.04156\teval-error:0.13043                                     \n",
      "[77]\ttrain-error:0.03964\teval-error:0.13043                                     \n",
      "[78]\ttrain-error:0.03964\teval-error:0.12276                                     \n",
      "[79]\ttrain-error:0.03964\teval-error:0.12532                                     \n",
      "[80]\ttrain-error:0.03836\teval-error:0.12276                                     \n",
      "[81]\ttrain-error:0.03900\teval-error:0.12021                                     \n",
      "[82]\ttrain-error:0.03772\teval-error:0.12532                                     \n",
      "[83]\ttrain-error:0.03900\teval-error:0.12276                                     \n",
      "[84]\ttrain-error:0.03772\teval-error:0.12276                                     \n",
      "[85]\ttrain-error:0.03836\teval-error:0.12788                                     \n",
      "[86]\ttrain-error:0.03708\teval-error:0.12276                                     \n",
      "[87]\ttrain-error:0.03708\teval-error:0.12788                                     \n",
      "[88]\ttrain-error:0.03708\teval-error:0.12276                                     \n",
      "[89]\ttrain-error:0.03644\teval-error:0.12276                                     \n",
      "[90]\ttrain-error:0.03581\teval-error:0.12532                                     \n",
      "[91]\ttrain-error:0.03708\teval-error:0.12532                                     \n",
      "[92]\ttrain-error:0.03581\teval-error:0.12532                                     \n",
      "[93]\ttrain-error:0.03517\teval-error:0.12532                                     \n",
      "[94]\ttrain-error:0.03517\teval-error:0.12788                                     \n",
      "[95]\ttrain-error:0.03453\teval-error:0.12788                                     \n",
      "[96]\ttrain-error:0.03453\teval-error:0.12532                                     \n",
      "[97]\ttrain-error:0.03325\teval-error:0.12788                                     \n",
      "[98]\ttrain-error:0.03389\teval-error:0.12532                                     \n",
      "[99]\ttrain-error:0.03325\teval-error:0.12532                                     \n",
      "[100]\ttrain-error:0.03325\teval-error:0.12532                                    \n",
      "[101]\ttrain-error:0.03325\teval-error:0.12532                                    \n",
      "[102]\ttrain-error:0.03133\teval-error:0.12788                                    \n",
      "[103]\ttrain-error:0.03005\teval-error:0.12276                                    \n",
      "[104]\ttrain-error:0.02941\teval-error:0.12532                                    \n",
      "[105]\ttrain-error:0.02749\teval-error:0.12532                                    \n",
      "[106]\ttrain-error:0.03005\teval-error:0.12276                                    \n",
      "[107]\ttrain-error:0.02877\teval-error:0.12021                                    \n",
      "[108]\ttrain-error:0.02685\teval-error:0.12532                                    \n",
      "[109]\ttrain-error:0.02685\teval-error:0.12276                                    \n",
      "[110]\ttrain-error:0.02621\teval-error:0.12532                                    \n",
      "[111]\ttrain-error:0.02685\teval-error:0.12276                                    \n",
      "[112]\ttrain-error:0.02494\teval-error:0.12788                                    \n",
      "[113]\ttrain-error:0.02494\teval-error:0.12532                                    \n",
      "[114]\ttrain-error:0.02366\teval-error:0.12276                                    \n",
      "[115]\ttrain-error:0.02430\teval-error:0.12276                                    \n",
      "[116]\ttrain-error:0.02430\teval-error:0.12276                                    \n",
      "[117]\ttrain-error:0.02302\teval-error:0.12021                                    \n",
      "[118]\ttrain-error:0.02302\teval-error:0.12021                                    \n",
      "[119]\ttrain-error:0.02174\teval-error:0.12021                                    \n",
      "[120]\ttrain-error:0.02110\teval-error:0.12021                                    \n",
      "[121]\ttrain-error:0.01982\teval-error:0.12021                                    \n",
      "[122]\ttrain-error:0.02110\teval-error:0.12276                                    \n",
      "[123]\ttrain-error:0.02110\teval-error:0.12276                                    \n",
      "[124]\ttrain-error:0.02046\teval-error:0.12276                                    \n",
      "[125]\ttrain-error:0.02046\teval-error:0.12276                                    \n",
      "[126]\ttrain-error:0.01918\teval-error:0.12276                                    \n",
      "[127]\ttrain-error:0.01854\teval-error:0.12276                                    \n",
      "[128]\ttrain-error:0.01854\teval-error:0.12021                                    \n",
      "[129]\ttrain-error:0.01726\teval-error:0.12788                                    \n",
      "[130]\ttrain-error:0.01726\teval-error:0.12788                                    \n",
      "[131]\ttrain-error:0.01662\teval-error:0.12788                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 4.7007755374969665e-08, 'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.2303522377913218, 'lambda': 0.00023255399132147476, 'max_depth': 8, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8721\n",
      "[0]\ttrain-error:0.14706\teval-error:0.15601                                      \n",
      "[1]\ttrain-error:0.14642\teval-error:0.16624                                      \n",
      "[2]\ttrain-error:0.13811\teval-error:0.16624                                      \n",
      "[3]\ttrain-error:0.14386\teval-error:0.17136                                      \n",
      "[4]\ttrain-error:0.14003\teval-error:0.16880                                      \n",
      "[5]\ttrain-error:0.14067\teval-error:0.16624                                      \n",
      "[6]\ttrain-error:0.14770\teval-error:0.17391                                      \n",
      "[7]\ttrain-error:0.14706\teval-error:0.16624                                      \n",
      "[8]\ttrain-error:0.13811\teval-error:0.15857                                      \n",
      "[9]\ttrain-error:0.13555\teval-error:0.15601                                      \n",
      "[10]\ttrain-error:0.13299\teval-error:0.15601                                     \n",
      "[11]\ttrain-error:0.13491\teval-error:0.16112                                     \n",
      "[12]\ttrain-error:0.12979\teval-error:0.15090                                     \n",
      "[13]\ttrain-error:0.13043\teval-error:0.14834                                     \n",
      "[14]\ttrain-error:0.12979\teval-error:0.15090                                     \n",
      "[15]\ttrain-error:0.12979\teval-error:0.15090                                     \n",
      "[16]\ttrain-error:0.12788\teval-error:0.15345                                     \n",
      "[17]\ttrain-error:0.12596\teval-error:0.15601                                     \n",
      "[18]\ttrain-error:0.12596\teval-error:0.15601                                     \n",
      "[19]\ttrain-error:0.12660\teval-error:0.15090                                     \n",
      "[20]\ttrain-error:0.12596\teval-error:0.14834                                     \n",
      "[21]\ttrain-error:0.12532\teval-error:0.15345                                     \n",
      "[22]\ttrain-error:0.12468\teval-error:0.14322                                     \n",
      "[23]\ttrain-error:0.12404\teval-error:0.14322                                     \n",
      "[24]\ttrain-error:0.12404\teval-error:0.14067                                     \n",
      "[25]\ttrain-error:0.11765\teval-error:0.14322                                     \n",
      "[26]\ttrain-error:0.11893\teval-error:0.14322                                     \n",
      "[27]\ttrain-error:0.11573\teval-error:0.14067                                     \n",
      "[28]\ttrain-error:0.11509\teval-error:0.14067                                     \n",
      "[29]\ttrain-error:0.11317\teval-error:0.14067                                     \n",
      "[30]\ttrain-error:0.11125\teval-error:0.14322                                     \n",
      "[31]\ttrain-error:0.11061\teval-error:0.14322                                     \n",
      "[32]\ttrain-error:0.10997\teval-error:0.14578                                     \n",
      "[33]\ttrain-error:0.10806\teval-error:0.14834                                     \n",
      "[34]\ttrain-error:0.10486\teval-error:0.14322                                     \n",
      "[35]\ttrain-error:0.10358\teval-error:0.14834                                     \n",
      "[36]\ttrain-error:0.10294\teval-error:0.14067                                     \n",
      "[37]\ttrain-error:0.10358\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.10166\teval-error:0.13811                                     \n",
      "[39]\ttrain-error:0.09910\teval-error:0.14067                                     \n",
      "[40]\ttrain-error:0.09974\teval-error:0.13811                                     \n",
      "[41]\ttrain-error:0.09783\teval-error:0.13811                                     \n",
      "[42]\ttrain-error:0.09719\teval-error:0.14067                                     \n",
      "[43]\ttrain-error:0.09591\teval-error:0.13811                                     \n",
      "[44]\ttrain-error:0.09527\teval-error:0.14067                                     \n",
      "[45]\ttrain-error:0.09527\teval-error:0.13811                                     \n",
      "[46]\ttrain-error:0.09143\teval-error:0.13555                                     \n",
      "[47]\ttrain-error:0.09207\teval-error:0.13555                                     \n",
      "[48]\ttrain-error:0.09015\teval-error:0.13299                                     \n",
      "[49]\ttrain-error:0.08951\teval-error:0.13299                                     \n",
      "[50]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[51]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[52]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[53]\ttrain-error:0.08184\teval-error:0.13555                                     \n",
      "[54]\ttrain-error:0.08184\teval-error:0.13555                                     \n",
      "[55]\ttrain-error:0.08184\teval-error:0.13555                                     \n",
      "[56]\ttrain-error:0.08120\teval-error:0.13811                                     \n",
      "[57]\ttrain-error:0.07737\teval-error:0.13811                                     \n",
      "[58]\ttrain-error:0.07609\teval-error:0.14067                                     \n",
      "[59]\ttrain-error:0.07353\teval-error:0.13555                                     \n",
      "[60]\ttrain-error:0.07353\teval-error:0.13555                                     \n",
      "[61]\ttrain-error:0.07289\teval-error:0.13555                                     \n",
      "[62]\ttrain-error:0.06969\teval-error:0.13811                                     \n",
      "[63]\ttrain-error:0.06969\teval-error:0.13555                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64]\ttrain-error:0.06969\teval-error:0.13555                                     \n",
      "[65]\ttrain-error:0.06841\teval-error:0.13555                                     \n",
      "[66]\ttrain-error:0.06905\teval-error:0.13299                                     \n",
      "[67]\ttrain-error:0.06714\teval-error:0.13299                                     \n",
      "[68]\ttrain-error:0.06522\teval-error:0.13555                                     \n",
      "[69]\ttrain-error:0.06394\teval-error:0.13555                                     \n",
      "[70]\ttrain-error:0.06266\teval-error:0.13555                                     \n",
      "[71]\ttrain-error:0.06074\teval-error:0.13299                                     \n",
      "[72]\ttrain-error:0.06074\teval-error:0.13555                                     \n",
      "[73]\ttrain-error:0.06010\teval-error:0.13555                                     \n",
      "[74]\ttrain-error:0.05946\teval-error:0.13299                                     \n",
      "[75]\ttrain-error:0.05818\teval-error:0.13299                                     \n",
      "[76]\ttrain-error:0.05754\teval-error:0.13043                                     \n",
      "[77]\ttrain-error:0.05882\teval-error:0.13299                                     \n",
      "[78]\ttrain-error:0.05754\teval-error:0.12788                                     \n",
      "[79]\ttrain-error:0.05882\teval-error:0.13299                                     \n",
      "[80]\ttrain-error:0.05754\teval-error:0.13043                                     \n",
      "[81]\ttrain-error:0.05754\teval-error:0.13299                                     \n",
      "[82]\ttrain-error:0.05627\teval-error:0.13299                                     \n",
      "[83]\ttrain-error:0.05690\teval-error:0.13299                                     \n",
      "[84]\ttrain-error:0.05563\teval-error:0.13043                                     \n",
      "[85]\ttrain-error:0.05243\teval-error:0.13555                                     \n",
      "[86]\ttrain-error:0.05307\teval-error:0.13555                                     \n",
      "[87]\ttrain-error:0.05371\teval-error:0.13555                                     \n",
      "[88]\ttrain-error:0.05371\teval-error:0.13299                                     \n",
      "[89]\ttrain-error:0.05243\teval-error:0.13299                                     \n",
      "[90]\ttrain-error:0.05243\teval-error:0.13299                                     \n",
      "[91]\ttrain-error:0.05115\teval-error:0.13043                                     \n",
      "[92]\ttrain-error:0.05115\teval-error:0.13043                                     \n",
      "[93]\ttrain-error:0.05051\teval-error:0.13043                                     \n",
      "[94]\ttrain-error:0.04987\teval-error:0.13043                                     \n",
      "[95]\ttrain-error:0.04987\teval-error:0.13299                                     \n",
      "[96]\ttrain-error:0.04795\teval-error:0.13299                                     \n",
      "[97]\ttrain-error:0.04732\teval-error:0.13555                                     \n",
      "[98]\ttrain-error:0.04668\teval-error:0.13555                                     \n",
      "[99]\ttrain-error:0.04604\teval-error:0.13299                                     \n",
      "[100]\ttrain-error:0.04476\teval-error:0.13555                                    \n",
      "[101]\ttrain-error:0.04412\teval-error:0.13299                                    \n",
      "[102]\ttrain-error:0.04476\teval-error:0.13299                                    \n",
      "[103]\ttrain-error:0.04412\teval-error:0.13299                                    \n",
      "[104]\ttrain-error:0.04284\teval-error:0.13299                                    \n",
      "[105]\ttrain-error:0.04156\teval-error:0.13299                                    \n",
      "[106]\ttrain-error:0.04220\teval-error:0.13299                                    \n",
      "[107]\ttrain-error:0.04348\teval-error:0.13299                                    \n",
      "[108]\ttrain-error:0.04284\teval-error:0.13555                                    \n",
      "[109]\ttrain-error:0.04412\teval-error:0.13299                                    \n",
      "[110]\ttrain-error:0.04412\teval-error:0.13555                                    \n",
      "[111]\ttrain-error:0.04220\teval-error:0.13555                                    \n",
      "[112]\ttrain-error:0.04220\teval-error:0.13555                                    \n",
      "[113]\ttrain-error:0.04220\teval-error:0.13299                                    \n",
      "[114]\ttrain-error:0.04284\teval-error:0.13299                                    \n",
      "[115]\ttrain-error:0.04220\teval-error:0.13299                                    \n",
      "[116]\ttrain-error:0.04156\teval-error:0.13299                                    \n",
      "[117]\ttrain-error:0.04220\teval-error:0.13299                                    \n",
      "[118]\ttrain-error:0.04156\teval-error:0.13043                                    \n",
      "[119]\ttrain-error:0.04156\teval-error:0.12788                                    \n",
      "[120]\ttrain-error:0.04092\teval-error:0.12788                                    \n",
      "[121]\ttrain-error:0.04028\teval-error:0.12788                                    \n",
      "[122]\ttrain-error:0.04028\teval-error:0.13043                                    \n",
      "[123]\ttrain-error:0.04028\teval-error:0.12788                                    \n",
      "[124]\ttrain-error:0.03900\teval-error:0.12788                                    \n",
      "[125]\ttrain-error:0.03964\teval-error:0.12788                                    \n",
      "[126]\ttrain-error:0.03836\teval-error:0.12788                                    \n",
      "[127]\ttrain-error:0.03644\teval-error:0.12788                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.6551353081917938, 'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.0033249580737345877, 'lambda': 0.008217152087772439, 'max_depth': 9, 'min_child_weight': 5.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8721\n",
      "[0]\ttrain-error:0.12852\teval-error:0.19182                                      \n",
      "[1]\ttrain-error:0.12021\teval-error:0.15857                                      \n",
      "[2]\ttrain-error:0.10934\teval-error:0.15345                                      \n",
      "[3]\ttrain-error:0.10038\teval-error:0.14067                                      \n",
      "[4]\ttrain-error:0.09783\teval-error:0.13555                                      \n",
      "[5]\ttrain-error:0.09591\teval-error:0.14067                                      \n",
      "[6]\ttrain-error:0.08951\teval-error:0.14578                                      \n",
      "[7]\ttrain-error:0.09079\teval-error:0.13299                                      \n",
      "[8]\ttrain-error:0.08184\teval-error:0.13555                                      \n",
      "[9]\ttrain-error:0.08184\teval-error:0.13811                                      \n",
      "[10]\ttrain-error:0.07801\teval-error:0.13043                                     \n",
      "[11]\ttrain-error:0.07481\teval-error:0.12532                                     \n",
      "[12]\ttrain-error:0.07097\teval-error:0.12788                                     \n",
      "[13]\ttrain-error:0.07097\teval-error:0.12276                                     \n",
      "[14]\ttrain-error:0.06586\teval-error:0.12532                                     \n",
      "[15]\ttrain-error:0.06841\teval-error:0.12021                                     \n",
      "[16]\ttrain-error:0.06266\teval-error:0.12021                                     \n",
      "[17]\ttrain-error:0.06010\teval-error:0.12021                                     \n",
      "[18]\ttrain-error:0.06074\teval-error:0.12532                                     \n",
      "[19]\ttrain-error:0.05627\teval-error:0.12788                                     \n",
      "[20]\ttrain-error:0.05499\teval-error:0.12788                                     \n",
      "[21]\ttrain-error:0.05435\teval-error:0.12021                                     \n",
      "[22]\ttrain-error:0.05243\teval-error:0.11765                                     \n",
      "[23]\ttrain-error:0.04923\teval-error:0.12021                                     \n",
      "[24]\ttrain-error:0.04476\teval-error:0.12276                                     \n",
      "[25]\ttrain-error:0.04476\teval-error:0.12276                                     \n",
      "[26]\ttrain-error:0.04540\teval-error:0.12532                                     \n",
      "[27]\ttrain-error:0.04284\teval-error:0.13043                                     \n",
      "[28]\ttrain-error:0.04156\teval-error:0.12788                                     \n",
      "[29]\ttrain-error:0.04220\teval-error:0.12788                                     \n",
      "[30]\ttrain-error:0.03836\teval-error:0.13043                                     \n",
      "[31]\ttrain-error:0.03644\teval-error:0.12788                                     \n",
      "[32]\ttrain-error:0.03261\teval-error:0.13043                                     \n",
      "[33]\ttrain-error:0.03005\teval-error:0.13555                                     \n",
      "[34]\ttrain-error:0.02813\teval-error:0.13811                                     \n",
      "[35]\ttrain-error:0.03069\teval-error:0.13811                                     \n",
      "[36]\ttrain-error:0.02685\teval-error:0.13811                                     \n",
      "[37]\ttrain-error:0.02749\teval-error:0.13811                                     \n",
      "[38]\ttrain-error:0.02558\teval-error:0.13299                                     \n",
      "[39]\ttrain-error:0.02558\teval-error:0.13299                                     \n",
      "[40]\ttrain-error:0.02494\teval-error:0.13299                                     \n",
      "[41]\ttrain-error:0.02494\teval-error:0.13555                                     \n",
      "[42]\ttrain-error:0.02238\teval-error:0.13555                                     \n",
      "[43]\ttrain-error:0.02174\teval-error:0.13299                                     \n",
      "[44]\ttrain-error:0.02302\teval-error:0.13043                                     \n",
      "[45]\ttrain-error:0.02238\teval-error:0.12532                                     \n",
      "[46]\ttrain-error:0.02110\teval-error:0.12276                                     \n",
      "[47]\ttrain-error:0.01918\teval-error:0.12276                                     \n",
      "[48]\ttrain-error:0.01790\teval-error:0.12788                                     \n",
      "[49]\ttrain-error:0.01854\teval-error:0.12021                                     \n",
      "[50]\ttrain-error:0.01854\teval-error:0.12788                                     \n",
      "[51]\ttrain-error:0.01726\teval-error:0.12788                                     \n",
      "[52]\ttrain-error:0.01662\teval-error:0.13043                                     \n",
      "[53]\ttrain-error:0.01598\teval-error:0.12788                                     \n",
      "[54]\ttrain-error:0.01471\teval-error:0.12021                                     \n",
      "[55]\ttrain-error:0.01534\teval-error:0.13043                                     \n",
      "[56]\ttrain-error:0.01471\teval-error:0.13043                                     \n",
      "[57]\ttrain-error:0.01407\teval-error:0.13043                                     \n",
      "[58]\ttrain-error:0.01407\teval-error:0.13043                                     \n",
      "[59]\ttrain-error:0.01279\teval-error:0.13043                                     \n",
      "[60]\ttrain-error:0.01215\teval-error:0.13043                                     \n",
      "[61]\ttrain-error:0.01151\teval-error:0.13299                                     \n",
      "[62]\ttrain-error:0.00959\teval-error:0.13299                                     \n",
      "[63]\ttrain-error:0.00959\teval-error:0.13043                                     \n",
      "[64]\ttrain-error:0.00895\teval-error:0.13299                                     \n",
      "[65]\ttrain-error:0.00895\teval-error:0.13043                                     \n",
      "[66]\ttrain-error:0.00703\teval-error:0.13043                                     \n",
      "[67]\ttrain-error:0.00511\teval-error:0.12788                                     \n",
      "[68]\ttrain-error:0.00639\teval-error:0.12788                                     \n",
      "[69]\ttrain-error:0.00511\teval-error:0.12788                                     \n",
      "[70]\ttrain-error:0.00511\teval-error:0.13043                                     \n",
      "[71]\ttrain-error:0.00511\teval-error:0.13043                                     \n",
      "[72]\ttrain-error:0.00448\teval-error:0.13043                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 1.474622118588522e-07, 'booster': 'gbtree', 'colsample_bytree': 0.9, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 3.104254110160204e-05, 'lambda': 0.0011629964580983944, 'max_depth': 9, 'min_child_weight': 2.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8696\n",
      "[0]\ttrain-error:0.16624\teval-error:0.16880                                      \n",
      "[1]\ttrain-error:0.16624\teval-error:0.16880                                      \n",
      "[2]\ttrain-error:0.15793\teval-error:0.16624                                      \n",
      "[3]\ttrain-error:0.16049\teval-error:0.17136                                      \n",
      "[4]\ttrain-error:0.16049\teval-error:0.17136                                      \n",
      "[5]\ttrain-error:0.15537\teval-error:0.16880                                      \n",
      "[6]\ttrain-error:0.15793\teval-error:0.16880                                      \n",
      "[7]\ttrain-error:0.16176\teval-error:0.17903                                      \n",
      "[8]\ttrain-error:0.15729\teval-error:0.17136                                      \n",
      "[9]\ttrain-error:0.15537\teval-error:0.17136                                      \n",
      "[10]\ttrain-error:0.15217\teval-error:0.17136                                     \n",
      "[11]\ttrain-error:0.15217\teval-error:0.17136                                     \n",
      "[12]\ttrain-error:0.15217\teval-error:0.16880                                     \n",
      "[13]\ttrain-error:0.14962\teval-error:0.16624                                     \n",
      "[14]\ttrain-error:0.14770\teval-error:0.16368                                     \n",
      "[15]\ttrain-error:0.14642\teval-error:0.15090                                     \n",
      "[16]\ttrain-error:0.14834\teval-error:0.15857                                     \n",
      "[17]\ttrain-error:0.14706\teval-error:0.16112                                     \n",
      "[18]\ttrain-error:0.14578\teval-error:0.16368                                     \n",
      "[19]\ttrain-error:0.14386\teval-error:0.15090                                     \n",
      "[20]\ttrain-error:0.14322\teval-error:0.15090                                     \n",
      "[21]\ttrain-error:0.14130\teval-error:0.14834                                     \n",
      "[22]\ttrain-error:0.14194\teval-error:0.14578                                     \n",
      "[23]\ttrain-error:0.14067\teval-error:0.14834                                     \n",
      "[24]\ttrain-error:0.14258\teval-error:0.15090                                     \n",
      "[25]\ttrain-error:0.14130\teval-error:0.14834                                     \n",
      "[26]\ttrain-error:0.13875\teval-error:0.14067                                     \n",
      "[27]\ttrain-error:0.13683\teval-error:0.13811                                     \n",
      "[28]\ttrain-error:0.13875\teval-error:0.14067                                     \n",
      "[29]\ttrain-error:0.13875\teval-error:0.14067                                     \n",
      "[30]\ttrain-error:0.13747\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.13875\teval-error:0.13555                                     \n",
      "[32]\ttrain-error:0.13811\teval-error:0.13811                                     \n",
      "[33]\ttrain-error:0.13747\teval-error:0.13811                                     \n",
      "[34]\ttrain-error:0.13747\teval-error:0.13555                                     \n",
      "[35]\ttrain-error:0.13555\teval-error:0.13555                                     \n",
      "[36]\ttrain-error:0.13555\teval-error:0.14067                                     \n",
      "[37]\ttrain-error:0.13299\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.13491\teval-error:0.14067                                     \n",
      "[39]\ttrain-error:0.13235\teval-error:0.14067                                     \n",
      "[40]\ttrain-error:0.13171\teval-error:0.14067                                     \n",
      "[41]\ttrain-error:0.13107\teval-error:0.14067                                     \n",
      "[42]\ttrain-error:0.13043\teval-error:0.14067                                     \n",
      "[43]\ttrain-error:0.12916\teval-error:0.14067                                     \n",
      "[44]\ttrain-error:0.12788\teval-error:0.13811                                     \n",
      "[45]\ttrain-error:0.12852\teval-error:0.14067                                     \n",
      "[46]\ttrain-error:0.12788\teval-error:0.14067                                     \n",
      "[47]\ttrain-error:0.12979\teval-error:0.13811                                     \n",
      "[48]\ttrain-error:0.12916\teval-error:0.13555                                     \n",
      "[49]\ttrain-error:0.12916\teval-error:0.13811                                     \n",
      "[50]\ttrain-error:0.12596\teval-error:0.13555                                     \n",
      "[51]\ttrain-error:0.12404\teval-error:0.13811                                     \n",
      "[52]\ttrain-error:0.12340\teval-error:0.13555                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53]\ttrain-error:0.12084\teval-error:0.13555                                     \n",
      "[54]\ttrain-error:0.11957\teval-error:0.13299                                     \n",
      "[55]\ttrain-error:0.11893\teval-error:0.13811                                     \n",
      "[56]\ttrain-error:0.11829\teval-error:0.13555                                     \n",
      "[57]\ttrain-error:0.11701\teval-error:0.13811                                     \n",
      "[58]\ttrain-error:0.11573\teval-error:0.13299                                     \n",
      "[59]\ttrain-error:0.11573\teval-error:0.13299                                     \n",
      "[60]\ttrain-error:0.11509\teval-error:0.13299                                     \n",
      "[61]\ttrain-error:0.11253\teval-error:0.13299                                     \n",
      "[62]\ttrain-error:0.11189\teval-error:0.13299                                     \n",
      "[63]\ttrain-error:0.11189\teval-error:0.12788                                     \n",
      "[64]\ttrain-error:0.11125\teval-error:0.12788                                     \n",
      "[65]\ttrain-error:0.10997\teval-error:0.12788                                     \n",
      "[66]\ttrain-error:0.11061\teval-error:0.13043                                     \n",
      "[67]\ttrain-error:0.11125\teval-error:0.12788                                     \n",
      "[68]\ttrain-error:0.10997\teval-error:0.13043                                     \n",
      "[69]\ttrain-error:0.10997\teval-error:0.13299                                     \n",
      "[70]\ttrain-error:0.10997\teval-error:0.13299                                     \n",
      "[71]\ttrain-error:0.10997\teval-error:0.13555                                     \n",
      "[72]\ttrain-error:0.10934\teval-error:0.13811                                     \n",
      "[73]\ttrain-error:0.10934\teval-error:0.13555                                     \n",
      "[74]\ttrain-error:0.10614\teval-error:0.13555                                     \n",
      "[75]\ttrain-error:0.10614\teval-error:0.13299                                     \n",
      "[76]\ttrain-error:0.10614\teval-error:0.13811                                     \n",
      "[77]\ttrain-error:0.10678\teval-error:0.14067                                     \n",
      "[78]\ttrain-error:0.10614\teval-error:0.13811                                     \n",
      "[79]\ttrain-error:0.10550\teval-error:0.13555                                     \n",
      "[80]\ttrain-error:0.10486\teval-error:0.13811                                     \n",
      "[81]\ttrain-error:0.10486\teval-error:0.13811                                     \n",
      "[82]\ttrain-error:0.10422\teval-error:0.13811                                     \n",
      "[83]\ttrain-error:0.10230\teval-error:0.14834                                     \n",
      "[84]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[85]\ttrain-error:0.10230\teval-error:0.14578                                     \n",
      "[86]\ttrain-error:0.10166\teval-error:0.14578                                     \n",
      "[87]\ttrain-error:0.10358\teval-error:0.14834                                     \n",
      "[88]\ttrain-error:0.10294\teval-error:0.14322                                     \n",
      "[89]\ttrain-error:0.10230\teval-error:0.14578                                     \n",
      "[90]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[91]\ttrain-error:0.10294\teval-error:0.14322                                     \n",
      "[92]\ttrain-error:0.10166\teval-error:0.14322                                     \n",
      "[93]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[94]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[95]\ttrain-error:0.10166\teval-error:0.14322                                     \n",
      "[96]\ttrain-error:0.10102\teval-error:0.14322                                     \n",
      "[97]\ttrain-error:0.10038\teval-error:0.14322                                     \n",
      "[98]\ttrain-error:0.10038\teval-error:0.14067                                     \n",
      "[99]\ttrain-error:0.09783\teval-error:0.14322                                     \n",
      "[100]\ttrain-error:0.09655\teval-error:0.14322                                    \n",
      "[101]\ttrain-error:0.09655\teval-error:0.14322                                    \n",
      "[102]\ttrain-error:0.09719\teval-error:0.14322                                    \n",
      "[103]\ttrain-error:0.09591\teval-error:0.14322                                    \n",
      "[104]\ttrain-error:0.09655\teval-error:0.14322                                    \n",
      "[105]\ttrain-error:0.09655\teval-error:0.14322                                    \n",
      "[106]\ttrain-error:0.09463\teval-error:0.14322                                    \n",
      "[107]\ttrain-error:0.09399\teval-error:0.14322                                    \n",
      "[108]\ttrain-error:0.09399\teval-error:0.14578                                    \n",
      "[109]\ttrain-error:0.09399\teval-error:0.14834                                    \n",
      "[110]\ttrain-error:0.09463\teval-error:0.14834                                    \n",
      "[111]\ttrain-error:0.09399\teval-error:0.14322                                    \n",
      "[112]\ttrain-error:0.09591\teval-error:0.14322                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.6094152451734796, 'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.056659872651347414, 'lambda': 4.383031097817279e-06, 'max_depth': 3, 'min_child_weight': 3.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8542\n",
      "[0]\ttrain-error:0.14706\teval-error:0.15601                                      \n",
      "[1]\ttrain-error:0.14258\teval-error:0.17136                                      \n",
      "[2]\ttrain-error:0.13939\teval-error:0.16112                                      \n",
      "[3]\ttrain-error:0.13875\teval-error:0.15857                                      \n",
      "[4]\ttrain-error:0.13747\teval-error:0.16112                                      \n",
      "[5]\ttrain-error:0.13427\teval-error:0.15857                                      \n",
      "[6]\ttrain-error:0.12916\teval-error:0.14834                                      \n",
      "[7]\ttrain-error:0.12852\teval-error:0.14578                                      \n",
      "[8]\ttrain-error:0.12788\teval-error:0.14578                                      \n",
      "[9]\ttrain-error:0.12660\teval-error:0.14834                                      \n",
      "[10]\ttrain-error:0.12724\teval-error:0.14578                                     \n",
      "[11]\ttrain-error:0.12788\teval-error:0.15090                                     \n",
      "[12]\ttrain-error:0.12468\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.12276\teval-error:0.14834                                     \n",
      "[14]\ttrain-error:0.12212\teval-error:0.15090                                     \n",
      "[15]\ttrain-error:0.12021\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.11829\teval-error:0.14322                                     \n",
      "[17]\ttrain-error:0.11893\teval-error:0.14322                                     \n",
      "[18]\ttrain-error:0.11701\teval-error:0.14067                                     \n",
      "[19]\ttrain-error:0.11445\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.11253\teval-error:0.13811                                     \n",
      "[21]\ttrain-error:0.11189\teval-error:0.14067                                     \n",
      "[22]\ttrain-error:0.11189\teval-error:0.13811                                     \n",
      "[23]\ttrain-error:0.11125\teval-error:0.14067                                     \n",
      "[24]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[25]\ttrain-error:0.10486\teval-error:0.13811                                     \n",
      "[26]\ttrain-error:0.10806\teval-error:0.13555                                     \n",
      "[27]\ttrain-error:0.10550\teval-error:0.13811                                     \n",
      "[28]\ttrain-error:0.10166\teval-error:0.13555                                     \n",
      "[29]\ttrain-error:0.09974\teval-error:0.13811                                     \n",
      "[30]\ttrain-error:0.10102\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.09655\teval-error:0.13299                                     \n",
      "[32]\ttrain-error:0.09527\teval-error:0.13299                                     \n",
      "[33]\ttrain-error:0.09271\teval-error:0.13043                                     \n",
      "[34]\ttrain-error:0.09271\teval-error:0.13555                                     \n",
      "[35]\ttrain-error:0.09143\teval-error:0.12788                                     \n",
      "[36]\ttrain-error:0.08951\teval-error:0.13043                                     \n",
      "[37]\ttrain-error:0.08760\teval-error:0.12788                                     \n",
      "[38]\ttrain-error:0.08696\teval-error:0.12532                                     \n",
      "[39]\ttrain-error:0.08312\teval-error:0.12532                                     \n",
      "[40]\ttrain-error:0.08312\teval-error:0.12788                                     \n",
      "[41]\ttrain-error:0.08120\teval-error:0.13043                                     \n",
      "[42]\ttrain-error:0.07737\teval-error:0.13299                                     \n",
      "[43]\ttrain-error:0.08056\teval-error:0.13555                                     \n",
      "[44]\ttrain-error:0.07928\teval-error:0.13299                                     \n",
      "[45]\ttrain-error:0.07864\teval-error:0.13043                                     \n",
      "[46]\ttrain-error:0.07737\teval-error:0.13043                                     \n",
      "[47]\ttrain-error:0.07609\teval-error:0.12788                                     \n",
      "[48]\ttrain-error:0.07353\teval-error:0.12788                                     \n",
      "[49]\ttrain-error:0.07161\teval-error:0.12532                                     \n",
      "[50]\ttrain-error:0.07033\teval-error:0.12532                                     \n",
      "[51]\ttrain-error:0.07097\teval-error:0.12788                                     \n",
      "[52]\ttrain-error:0.07289\teval-error:0.12788                                     \n",
      "[53]\ttrain-error:0.07161\teval-error:0.12788                                     \n",
      "[54]\ttrain-error:0.07097\teval-error:0.12788                                     \n",
      "[55]\ttrain-error:0.07033\teval-error:0.12276                                     \n",
      "[56]\ttrain-error:0.06969\teval-error:0.12276                                     \n",
      "[57]\ttrain-error:0.06905\teval-error:0.12276                                     \n",
      "[58]\ttrain-error:0.06778\teval-error:0.12276                                     \n",
      "[59]\ttrain-error:0.06522\teval-error:0.12021                                     \n",
      "[60]\ttrain-error:0.06650\teval-error:0.12021                                     \n",
      "[61]\ttrain-error:0.06266\teval-error:0.12276                                     \n",
      "[62]\ttrain-error:0.06074\teval-error:0.12276                                     \n",
      "[63]\ttrain-error:0.06010\teval-error:0.12276                                     \n",
      "[64]\ttrain-error:0.05818\teval-error:0.12532                                     \n",
      "[65]\ttrain-error:0.05754\teval-error:0.12532                                     \n",
      "[66]\ttrain-error:0.05754\teval-error:0.12276                                     \n",
      "[67]\ttrain-error:0.05627\teval-error:0.12532                                     \n",
      "[68]\ttrain-error:0.05627\teval-error:0.12276                                     \n",
      "[69]\ttrain-error:0.05371\teval-error:0.12788                                     \n",
      "[70]\ttrain-error:0.05307\teval-error:0.13043                                     \n",
      "[71]\ttrain-error:0.05179\teval-error:0.13043                                     \n",
      "[72]\ttrain-error:0.05371\teval-error:0.12788                                     \n",
      "[73]\ttrain-error:0.05243\teval-error:0.12788                                     \n",
      "[74]\ttrain-error:0.04859\teval-error:0.12788                                     \n",
      "[75]\ttrain-error:0.04795\teval-error:0.13043                                     \n",
      "[76]\ttrain-error:0.04604\teval-error:0.13043                                     \n",
      "[77]\ttrain-error:0.04604\teval-error:0.13043                                     \n",
      "[78]\ttrain-error:0.04604\teval-error:0.12788                                     \n",
      "[79]\ttrain-error:0.04476\teval-error:0.13043                                     \n",
      "[80]\ttrain-error:0.04476\teval-error:0.13043                                     \n",
      "[81]\ttrain-error:0.04412\teval-error:0.12532                                     \n",
      "[82]\ttrain-error:0.04220\teval-error:0.12532                                     \n",
      "[83]\ttrain-error:0.04156\teval-error:0.12532                                     \n",
      "[84]\ttrain-error:0.04220\teval-error:0.12788                                     \n",
      "[85]\ttrain-error:0.04156\teval-error:0.12788                                     \n",
      "[86]\ttrain-error:0.04156\teval-error:0.12532                                     \n",
      "[87]\ttrain-error:0.03964\teval-error:0.12021                                     \n",
      "[88]\ttrain-error:0.03900\teval-error:0.12276                                     \n",
      "[89]\ttrain-error:0.03900\teval-error:0.12276                                     \n",
      "[90]\ttrain-error:0.03836\teval-error:0.12276                                     \n",
      "[91]\ttrain-error:0.03772\teval-error:0.12532                                     \n",
      "[92]\ttrain-error:0.03836\teval-error:0.12532                                     \n",
      "[93]\ttrain-error:0.03900\teval-error:0.12532                                     \n",
      "[94]\ttrain-error:0.03836\teval-error:0.12532                                     \n",
      "[95]\ttrain-error:0.03772\teval-error:0.12532                                     \n",
      "[96]\ttrain-error:0.03708\teval-error:0.12276                                     \n",
      "[97]\ttrain-error:0.03644\teval-error:0.12276                                     \n",
      "[98]\ttrain-error:0.03581\teval-error:0.12276                                     \n",
      "[99]\ttrain-error:0.03581\teval-error:0.13043                                     \n",
      "[100]\ttrain-error:0.03453\teval-error:0.12788                                    \n",
      "[101]\ttrain-error:0.03453\teval-error:0.13555                                    \n",
      "[102]\ttrain-error:0.03389\teval-error:0.13555                                    \n",
      "[103]\ttrain-error:0.03325\teval-error:0.13555                                    \n",
      "[104]\ttrain-error:0.03261\teval-error:0.13043                                    \n",
      "[105]\ttrain-error:0.03261\teval-error:0.13555                                    \n",
      "[106]\ttrain-error:0.03133\teval-error:0.13299                                    \n",
      "[107]\ttrain-error:0.03005\teval-error:0.13299                                    \n",
      "[108]\ttrain-error:0.03069\teval-error:0.13299                                    \n",
      "[109]\ttrain-error:0.03005\teval-error:0.13299                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.001377010769780299, 'booster': 'gbtree', 'colsample_bytree': 0.9, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.05228636043070423, 'lambda': 0.8889323013471523, 'max_depth': 7, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8670\n",
      "[0]\ttrain-error:0.14386\teval-error:0.15601                                      \n",
      "[1]\ttrain-error:0.14130\teval-error:0.15601                                      \n",
      "[2]\ttrain-error:0.13555\teval-error:0.14834                                      \n",
      "[3]\ttrain-error:0.13683\teval-error:0.14322                                      \n",
      "[4]\ttrain-error:0.13171\teval-error:0.14834                                      \n",
      "[5]\ttrain-error:0.12916\teval-error:0.14834                                      \n",
      "[6]\ttrain-error:0.12532\teval-error:0.14578                                      \n",
      "[7]\ttrain-error:0.12852\teval-error:0.15090                                      \n",
      "[8]\ttrain-error:0.12148\teval-error:0.15090                                      \n",
      "[9]\ttrain-error:0.11445\teval-error:0.15345                                      \n",
      "[10]\ttrain-error:0.11509\teval-error:0.15601                                     \n",
      "[11]\ttrain-error:0.11573\teval-error:0.14322                                     \n",
      "[12]\ttrain-error:0.11189\teval-error:0.14322                                     \n",
      "[13]\ttrain-error:0.11381\teval-error:0.15090                                     \n",
      "[14]\ttrain-error:0.11381\teval-error:0.14834                                     \n",
      "[15]\ttrain-error:0.11381\teval-error:0.14578                                     \n",
      "[16]\ttrain-error:0.10934\teval-error:0.14834                                     \n",
      "[17]\ttrain-error:0.10934\teval-error:0.14578                                     \n",
      "[18]\ttrain-error:0.10806\teval-error:0.14578                                     \n",
      "[19]\ttrain-error:0.10806\teval-error:0.14578                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\ttrain-error:0.10806\teval-error:0.14578                                     \n",
      "[21]\ttrain-error:0.10806\teval-error:0.14578                                     \n",
      "[22]\ttrain-error:0.10678\teval-error:0.14322                                     \n",
      "[23]\ttrain-error:0.10486\teval-error:0.14067                                     \n",
      "[24]\ttrain-error:0.10358\teval-error:0.13811                                     \n",
      "[25]\ttrain-error:0.10230\teval-error:0.13811                                     \n",
      "[26]\ttrain-error:0.10294\teval-error:0.13555                                     \n",
      "[27]\ttrain-error:0.10166\teval-error:0.14067                                     \n",
      "[28]\ttrain-error:0.09719\teval-error:0.13811                                     \n",
      "[29]\ttrain-error:0.09783\teval-error:0.14322                                     \n",
      "[30]\ttrain-error:0.09527\teval-error:0.14067                                     \n",
      "[31]\ttrain-error:0.09271\teval-error:0.14322                                     \n",
      "[32]\ttrain-error:0.09399\teval-error:0.14578                                     \n",
      "[33]\ttrain-error:0.09463\teval-error:0.14322                                     \n",
      "[34]\ttrain-error:0.09271\teval-error:0.13811                                     \n",
      "[35]\ttrain-error:0.08887\teval-error:0.14322                                     \n",
      "[36]\ttrain-error:0.08632\teval-error:0.13811                                     \n",
      "[37]\ttrain-error:0.08376\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.08440\teval-error:0.14067                                     \n",
      "[39]\ttrain-error:0.08056\teval-error:0.14067                                     \n",
      "[40]\ttrain-error:0.08056\teval-error:0.14067                                     \n",
      "[41]\ttrain-error:0.07992\teval-error:0.14322                                     \n",
      "[42]\ttrain-error:0.07801\teval-error:0.14322                                     \n",
      "[43]\ttrain-error:0.07801\teval-error:0.14322                                     \n",
      "[44]\ttrain-error:0.07545\teval-error:0.14578                                     \n",
      "[45]\ttrain-error:0.07417\teval-error:0.14322                                     \n",
      "[46]\ttrain-error:0.07353\teval-error:0.14067                                     \n",
      "[47]\ttrain-error:0.07417\teval-error:0.14322                                     \n",
      "[48]\ttrain-error:0.07353\teval-error:0.14578                                     \n",
      "[49]\ttrain-error:0.07033\teval-error:0.14578                                     \n",
      "[50]\ttrain-error:0.06905\teval-error:0.14578                                     \n",
      "[51]\ttrain-error:0.06778\teval-error:0.14067                                     \n",
      "[52]\ttrain-error:0.06714\teval-error:0.14067                                     \n",
      "[53]\ttrain-error:0.06778\teval-error:0.14067                                     \n",
      "[54]\ttrain-error:0.06522\teval-error:0.14322                                     \n",
      "[55]\ttrain-error:0.06266\teval-error:0.14322                                     \n",
      "[56]\ttrain-error:0.06266\teval-error:0.14578                                     \n",
      "[57]\ttrain-error:0.05946\teval-error:0.14322                                     \n",
      "[58]\ttrain-error:0.06266\teval-error:0.14322                                     \n",
      "[59]\ttrain-error:0.06330\teval-error:0.14578                                     \n",
      "[60]\ttrain-error:0.06202\teval-error:0.14067                                     \n",
      "[61]\ttrain-error:0.06010\teval-error:0.13555                                     \n",
      "[62]\ttrain-error:0.05946\teval-error:0.14067                                     \n",
      "[63]\ttrain-error:0.05946\teval-error:0.14322                                     \n",
      "[64]\ttrain-error:0.05627\teval-error:0.14578                                     \n",
      "[65]\ttrain-error:0.05690\teval-error:0.14834                                     \n",
      "[66]\ttrain-error:0.05627\teval-error:0.14322                                     \n",
      "[67]\ttrain-error:0.05435\teval-error:0.14322                                     \n",
      "[68]\ttrain-error:0.05371\teval-error:0.14578                                     \n",
      "[69]\ttrain-error:0.05243\teval-error:0.14578                                     \n",
      "[70]\ttrain-error:0.05307\teval-error:0.14067                                     \n",
      "[71]\ttrain-error:0.05307\teval-error:0.13811                                     \n",
      "[72]\ttrain-error:0.05051\teval-error:0.14322                                     \n",
      "[73]\ttrain-error:0.05179\teval-error:0.13811                                     \n",
      "[74]\ttrain-error:0.04987\teval-error:0.13299                                     \n",
      "[75]\ttrain-error:0.04923\teval-error:0.13555                                     \n",
      "[76]\ttrain-error:0.04923\teval-error:0.13811                                     \n",
      "[77]\ttrain-error:0.04795\teval-error:0.13555                                     \n",
      "[78]\ttrain-error:0.04540\teval-error:0.13555                                     \n",
      "[79]\ttrain-error:0.04540\teval-error:0.13811                                     \n",
      "[80]\ttrain-error:0.04540\teval-error:0.14067                                     \n",
      "[81]\ttrain-error:0.04412\teval-error:0.14067                                     \n",
      "[82]\ttrain-error:0.04348\teval-error:0.13811                                     \n",
      "[83]\ttrain-error:0.04284\teval-error:0.13811                                     \n",
      "[84]\ttrain-error:0.04284\teval-error:0.14067                                     \n",
      "[85]\ttrain-error:0.04092\teval-error:0.14322                                     \n",
      "[86]\ttrain-error:0.04028\teval-error:0.13811                                     \n",
      "[87]\ttrain-error:0.04028\teval-error:0.13811                                     \n",
      "[88]\ttrain-error:0.03900\teval-error:0.13811                                     \n",
      "[89]\ttrain-error:0.03836\teval-error:0.13555                                     \n",
      "[90]\ttrain-error:0.03836\teval-error:0.13811                                     \n",
      "[91]\ttrain-error:0.03708\teval-error:0.13811                                     \n",
      "[92]\ttrain-error:0.03581\teval-error:0.13811                                     \n",
      "[93]\ttrain-error:0.03517\teval-error:0.13811                                     \n",
      "[94]\ttrain-error:0.03261\teval-error:0.14067                                     \n",
      "[95]\ttrain-error:0.03453\teval-error:0.14067                                     \n",
      "[96]\ttrain-error:0.03453\teval-error:0.14322                                     \n",
      "[97]\ttrain-error:0.03197\teval-error:0.14067                                     \n",
      "[98]\ttrain-error:0.03325\teval-error:0.13811                                     \n",
      "[99]\ttrain-error:0.03261\teval-error:0.13811                                     \n",
      "[100]\ttrain-error:0.03197\teval-error:0.13811                                    \n",
      "[101]\ttrain-error:0.03197\teval-error:0.14322                                    \n",
      "[102]\ttrain-error:0.03197\teval-error:0.14067                                    \n",
      "[103]\ttrain-error:0.03197\teval-error:0.13811                                    \n",
      "[104]\ttrain-error:0.03261\teval-error:0.13811                                    \n",
      "[105]\ttrain-error:0.03133\teval-error:0.14322                                    \n",
      "[106]\ttrain-error:0.03133\teval-error:0.14067                                    \n",
      "[107]\ttrain-error:0.02941\teval-error:0.14322                                    \n",
      "[108]\ttrain-error:0.03005\teval-error:0.14578                                    \n",
      "[109]\ttrain-error:0.02941\teval-error:0.14578                                    \n",
      "[110]\ttrain-error:0.02877\teval-error:0.14578                                    \n",
      "[111]\ttrain-error:0.02941\teval-error:0.14578                                    \n",
      "[112]\ttrain-error:0.02685\teval-error:0.14322                                    \n",
      "[113]\ttrain-error:0.02558\teval-error:0.14322                                    \n",
      "[114]\ttrain-error:0.02621\teval-error:0.14322                                    \n",
      "[115]\ttrain-error:0.02685\teval-error:0.14578                                    \n",
      "[116]\ttrain-error:0.02621\teval-error:0.14578                                    \n",
      "[117]\ttrain-error:0.02621\teval-error:0.14834                                    \n",
      "[118]\ttrain-error:0.02621\teval-error:0.14834                                    \n",
      "[119]\ttrain-error:0.02494\teval-error:0.14834                                    \n",
      "[120]\ttrain-error:0.02430\teval-error:0.14578                                    \n",
      "[121]\ttrain-error:0.02302\teval-error:0.14578                                    \n",
      "[122]\ttrain-error:0.02238\teval-error:0.14067                                    \n",
      "[123]\ttrain-error:0.02046\teval-error:0.14322                                    \n",
      "[124]\ttrain-error:0.02110\teval-error:0.14578                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 2.507046567027063e-06, 'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.0004775620462151609, 'lambda': 3.314519737278568e-05, 'max_depth': 5, 'min_child_weight': 2.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8542\n",
      "[0]\ttrain-error:0.14642\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.15921\teval-error:0.17391                                      \n",
      "[2]\ttrain-error:0.14706\teval-error:0.15857                                      \n",
      "[3]\ttrain-error:0.15409\teval-error:0.17647                                      \n",
      "[4]\ttrain-error:0.16304\teval-error:0.17136                                      \n",
      "[5]\ttrain-error:0.16368\teval-error:0.16624                                      \n",
      "[6]\ttrain-error:0.16049\teval-error:0.16880                                      \n",
      "[7]\ttrain-error:0.15985\teval-error:0.16880                                      \n",
      "[8]\ttrain-error:0.15473\teval-error:0.16880                                      \n",
      "[9]\ttrain-error:0.15217\teval-error:0.16368                                      \n",
      "[10]\ttrain-error:0.14834\teval-error:0.16368                                     \n",
      "[11]\ttrain-error:0.14898\teval-error:0.17136                                     \n",
      "[12]\ttrain-error:0.14578\teval-error:0.16112                                     \n",
      "[13]\ttrain-error:0.14642\teval-error:0.16368                                     \n",
      "[14]\ttrain-error:0.14258\teval-error:0.15601                                     \n",
      "[15]\ttrain-error:0.14322\teval-error:0.16112                                     \n",
      "[16]\ttrain-error:0.14386\teval-error:0.16368                                     \n",
      "[17]\ttrain-error:0.14706\teval-error:0.16112                                     \n",
      "[18]\ttrain-error:0.14322\teval-error:0.15601                                     \n",
      "[19]\ttrain-error:0.14130\teval-error:0.15090                                     \n",
      "[20]\ttrain-error:0.14322\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.14130\teval-error:0.14578                                     \n",
      "[22]\ttrain-error:0.13875\teval-error:0.15090                                     \n",
      "[23]\ttrain-error:0.14067\teval-error:0.14834                                     \n",
      "[24]\ttrain-error:0.13875\teval-error:0.14834                                     \n",
      "[25]\ttrain-error:0.13619\teval-error:0.15090                                     \n",
      "[26]\ttrain-error:0.13363\teval-error:0.14834                                     \n",
      "[27]\ttrain-error:0.13171\teval-error:0.14834                                     \n",
      "[28]\ttrain-error:0.12979\teval-error:0.14322                                     \n",
      "[29]\ttrain-error:0.13107\teval-error:0.14578                                     \n",
      "[30]\ttrain-error:0.12979\teval-error:0.13555                                     \n",
      "[31]\ttrain-error:0.13043\teval-error:0.13555                                     \n",
      "[32]\ttrain-error:0.12852\teval-error:0.14067                                     \n",
      "[33]\ttrain-error:0.12788\teval-error:0.13811                                     \n",
      "[34]\ttrain-error:0.12916\teval-error:0.14322                                     \n",
      "[35]\ttrain-error:0.12724\teval-error:0.14322                                     \n",
      "[36]\ttrain-error:0.12660\teval-error:0.14067                                     \n",
      "[37]\ttrain-error:0.12404\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.12532\teval-error:0.14578                                     \n",
      "[39]\ttrain-error:0.12468\teval-error:0.14578                                     \n",
      "[40]\ttrain-error:0.12021\teval-error:0.14067                                     \n",
      "[41]\ttrain-error:0.11957\teval-error:0.14322                                     \n",
      "[42]\ttrain-error:0.12021\teval-error:0.14067                                     \n",
      "[43]\ttrain-error:0.11957\teval-error:0.14067                                     \n",
      "[44]\ttrain-error:0.12021\teval-error:0.14067                                     \n",
      "[45]\ttrain-error:0.11957\teval-error:0.14067                                     \n",
      "[46]\ttrain-error:0.12021\teval-error:0.14067                                     \n",
      "[47]\ttrain-error:0.11509\teval-error:0.13555                                     \n",
      "[48]\ttrain-error:0.11381\teval-error:0.13811                                     \n",
      "[49]\ttrain-error:0.11317\teval-error:0.14322                                     \n",
      "[50]\ttrain-error:0.11381\teval-error:0.13299                                     \n",
      "[51]\ttrain-error:0.10997\teval-error:0.13811                                     \n",
      "[52]\ttrain-error:0.10997\teval-error:0.13811                                     \n",
      "[53]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[54]\ttrain-error:0.10934\teval-error:0.14067                                     \n",
      "[55]\ttrain-error:0.10806\teval-error:0.13811                                     \n",
      "[56]\ttrain-error:0.10678\teval-error:0.13811                                     \n",
      "[57]\ttrain-error:0.10550\teval-error:0.13299                                     \n",
      "[58]\ttrain-error:0.10550\teval-error:0.13299                                     \n",
      "[59]\ttrain-error:0.10422\teval-error:0.13555                                     \n",
      "[60]\ttrain-error:0.10358\teval-error:0.13555                                     \n",
      "[61]\ttrain-error:0.10102\teval-error:0.13555                                     \n",
      "[62]\ttrain-error:0.10230\teval-error:0.14067                                     \n",
      "[63]\ttrain-error:0.10102\teval-error:0.13555                                     \n",
      "[64]\ttrain-error:0.09783\teval-error:0.13555                                     \n",
      "[65]\ttrain-error:0.09655\teval-error:0.13555                                     \n",
      "[66]\ttrain-error:0.09655\teval-error:0.13555                                     \n",
      "[67]\ttrain-error:0.09655\teval-error:0.13811                                     \n",
      "[68]\ttrain-error:0.09527\teval-error:0.13811                                     \n",
      "[69]\ttrain-error:0.09655\teval-error:0.14067                                     \n",
      "[70]\ttrain-error:0.09655\teval-error:0.13555                                     \n",
      "[71]\ttrain-error:0.09591\teval-error:0.13811                                     \n",
      "[72]\ttrain-error:0.09335\teval-error:0.13811                                     \n",
      "[73]\ttrain-error:0.09335\teval-error:0.13811                                     \n",
      "[74]\ttrain-error:0.09207\teval-error:0.14067                                     \n",
      "[75]\ttrain-error:0.09143\teval-error:0.14067                                     \n",
      "[76]\ttrain-error:0.09015\teval-error:0.14067                                     \n",
      "[77]\ttrain-error:0.09271\teval-error:0.13555                                     \n",
      "[78]\ttrain-error:0.09015\teval-error:0.13555                                     \n",
      "[79]\ttrain-error:0.09079\teval-error:0.13555                                     \n",
      "[80]\ttrain-error:0.09079\teval-error:0.13555                                     \n",
      "[81]\ttrain-error:0.08887\teval-error:0.13555                                     \n",
      "[82]\ttrain-error:0.08951\teval-error:0.13555                                     \n",
      "[83]\ttrain-error:0.08951\teval-error:0.13555                                     \n",
      "[84]\ttrain-error:0.08951\teval-error:0.13811                                     \n",
      "[85]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[86]\ttrain-error:0.08696\teval-error:0.13555                                     \n",
      "[87]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[88]\ttrain-error:0.08568\teval-error:0.13555                                     \n",
      "[89]\ttrain-error:0.08376\teval-error:0.13555                                     \n",
      "[90]\ttrain-error:0.08248\teval-error:0.13555                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91]\ttrain-error:0.08184\teval-error:0.13555                                     \n",
      "[92]\ttrain-error:0.08120\teval-error:0.13555                                     \n",
      "[93]\ttrain-error:0.08120\teval-error:0.13555                                     \n",
      "[94]\ttrain-error:0.08184\teval-error:0.13811                                     \n",
      "[95]\ttrain-error:0.07992\teval-error:0.14067                                     \n",
      "[96]\ttrain-error:0.07801\teval-error:0.14322                                     \n",
      "[97]\ttrain-error:0.07801\teval-error:0.14322                                     \n",
      "[98]\ttrain-error:0.07737\teval-error:0.14067                                     \n",
      "[99]\ttrain-error:0.07737\teval-error:0.14067                                     \n",
      "[100]\ttrain-error:0.07673\teval-error:0.14067                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 7.238237902933563e-05, 'booster': 'gbtree', 'colsample_bytree': 0.6000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 5.886431621337949e-07, 'lambda': 6.146174239324659, 'max_depth': 8, 'min_child_weight': 5.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8593\n",
      "[0]\ttrain-error:0.14642\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.14450\teval-error:0.16112                                      \n",
      "[2]\ttrain-error:0.14003\teval-error:0.15345                                      \n",
      "[3]\ttrain-error:0.14642\teval-error:0.15090                                      \n",
      "[4]\ttrain-error:0.14578\teval-error:0.15601                                      \n",
      "[5]\ttrain-error:0.14450\teval-error:0.16880                                      \n",
      "[6]\ttrain-error:0.13811\teval-error:0.15857                                      \n",
      "[7]\ttrain-error:0.14003\teval-error:0.16624                                      \n",
      "[8]\ttrain-error:0.13875\teval-error:0.16368                                      \n",
      "[9]\ttrain-error:0.13747\teval-error:0.15857                                      \n",
      "[10]\ttrain-error:0.13555\teval-error:0.15857                                     \n",
      "[11]\ttrain-error:0.13235\teval-error:0.15090                                     \n",
      "[12]\ttrain-error:0.13107\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.13235\teval-error:0.14834                                     \n",
      "[14]\ttrain-error:0.13491\teval-error:0.14578                                     \n",
      "[15]\ttrain-error:0.13299\teval-error:0.14834                                     \n",
      "[16]\ttrain-error:0.13299\teval-error:0.14578                                     \n",
      "[17]\ttrain-error:0.13299\teval-error:0.15090                                     \n",
      "[18]\ttrain-error:0.13107\teval-error:0.14322                                     \n",
      "[19]\ttrain-error:0.12979\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.12532\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.12660\teval-error:0.14834                                     \n",
      "[22]\ttrain-error:0.12596\teval-error:0.14578                                     \n",
      "[23]\ttrain-error:0.12340\teval-error:0.14578                                     \n",
      "[24]\ttrain-error:0.12340\teval-error:0.14578                                     \n",
      "[25]\ttrain-error:0.12404\teval-error:0.14834                                     \n",
      "[26]\ttrain-error:0.12404\teval-error:0.14834                                     \n",
      "[27]\ttrain-error:0.12532\teval-error:0.14834                                     \n",
      "[28]\ttrain-error:0.12084\teval-error:0.14834                                     \n",
      "[29]\ttrain-error:0.12021\teval-error:0.14834                                     \n",
      "[30]\ttrain-error:0.11893\teval-error:0.14834                                     \n",
      "[31]\ttrain-error:0.11509\teval-error:0.14578                                     \n",
      "[32]\ttrain-error:0.11381\teval-error:0.14067                                     \n",
      "[33]\ttrain-error:0.11445\teval-error:0.14067                                     \n",
      "[34]\ttrain-error:0.11381\teval-error:0.14067                                     \n",
      "[35]\ttrain-error:0.11125\teval-error:0.14322                                     \n",
      "[36]\ttrain-error:0.11189\teval-error:0.14322                                     \n",
      "[37]\ttrain-error:0.10997\teval-error:0.14322                                     \n",
      "[38]\ttrain-error:0.10806\teval-error:0.14322                                     \n",
      "[39]\ttrain-error:0.10742\teval-error:0.14322                                     \n",
      "[40]\ttrain-error:0.10614\teval-error:0.14067                                     \n",
      "[41]\ttrain-error:0.10486\teval-error:0.14067                                     \n",
      "[42]\ttrain-error:0.10166\teval-error:0.14322                                     \n",
      "[43]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[44]\ttrain-error:0.10358\teval-error:0.14067                                     \n",
      "[45]\ttrain-error:0.10166\teval-error:0.14067                                     \n",
      "[46]\ttrain-error:0.10166\teval-error:0.14067                                     \n",
      "[47]\ttrain-error:0.10166\teval-error:0.13811                                     \n",
      "[48]\ttrain-error:0.09846\teval-error:0.14322                                     \n",
      "[49]\ttrain-error:0.09783\teval-error:0.14067                                     \n",
      "[50]\ttrain-error:0.09591\teval-error:0.14322                                     \n",
      "[51]\ttrain-error:0.09591\teval-error:0.14322                                     \n",
      "[52]\ttrain-error:0.09527\teval-error:0.14067                                     \n",
      "[53]\ttrain-error:0.09463\teval-error:0.13811                                     \n",
      "[54]\ttrain-error:0.09335\teval-error:0.14067                                     \n",
      "[55]\ttrain-error:0.09271\teval-error:0.14067                                     \n",
      "[56]\ttrain-error:0.09143\teval-error:0.13811                                     \n",
      "[57]\ttrain-error:0.09335\teval-error:0.14067                                     \n",
      "[58]\ttrain-error:0.09143\teval-error:0.14322                                     \n",
      "[59]\ttrain-error:0.08951\teval-error:0.14067                                     \n",
      "[60]\ttrain-error:0.08887\teval-error:0.14322                                     \n",
      "[61]\ttrain-error:0.08760\teval-error:0.14322                                     \n",
      "[62]\ttrain-error:0.08760\teval-error:0.14322                                     \n",
      "[63]\ttrain-error:0.08696\teval-error:0.14578                                     \n",
      "[64]\ttrain-error:0.08376\teval-error:0.14578                                     \n",
      "[65]\ttrain-error:0.08440\teval-error:0.14578                                     \n",
      "[66]\ttrain-error:0.08312\teval-error:0.14578                                     \n",
      "[67]\ttrain-error:0.08248\teval-error:0.14578                                     \n",
      "[68]\ttrain-error:0.08056\teval-error:0.14578                                     \n",
      "[69]\ttrain-error:0.07928\teval-error:0.14578                                     \n",
      "[70]\ttrain-error:0.07928\teval-error:0.14834                                     \n",
      "[71]\ttrain-error:0.07928\teval-error:0.14834                                     \n",
      "[72]\ttrain-error:0.07992\teval-error:0.14834                                     \n",
      "[73]\ttrain-error:0.07864\teval-error:0.14834                                     \n",
      "[74]\ttrain-error:0.07864\teval-error:0.14834                                     \n",
      "[75]\ttrain-error:0.07864\teval-error:0.14578                                     \n",
      "[76]\ttrain-error:0.07801\teval-error:0.14578                                     \n",
      "[77]\ttrain-error:0.07673\teval-error:0.14834                                     \n",
      "[78]\ttrain-error:0.07737\teval-error:0.14834                                     \n",
      "[79]\ttrain-error:0.07801\teval-error:0.14834                                     \n",
      "[80]\ttrain-error:0.07417\teval-error:0.14834                                     \n",
      "[81]\ttrain-error:0.07353\teval-error:0.14834                                     \n",
      "[82]\ttrain-error:0.07353\teval-error:0.14834                                     \n",
      "[83]\ttrain-error:0.07289\teval-error:0.14834                                     \n",
      "[84]\ttrain-error:0.07289\teval-error:0.15090                                     \n",
      "[85]\ttrain-error:0.07033\teval-error:0.15090                                     \n",
      "[86]\ttrain-error:0.06905\teval-error:0.15090                                     \n",
      "[87]\ttrain-error:0.06969\teval-error:0.15090                                     \n",
      "[88]\ttrain-error:0.06841\teval-error:0.15090                                     \n",
      "[89]\ttrain-error:0.06714\teval-error:0.15090                                     \n",
      "[90]\ttrain-error:0.06650\teval-error:0.15090                                     \n",
      "[91]\ttrain-error:0.06586\teval-error:0.15090                                     \n",
      "[92]\ttrain-error:0.06714\teval-error:0.15090                                     \n",
      "[93]\ttrain-error:0.06330\teval-error:0.15090                                     \n",
      "[94]\ttrain-error:0.06330\teval-error:0.14834                                     \n",
      "[95]\ttrain-error:0.06266\teval-error:0.14834                                     \n",
      "[96]\ttrain-error:0.06330\teval-error:0.14834                                     \n",
      "[97]\ttrain-error:0.06138\teval-error:0.14834                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.009202605971773943, 'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 1.534104713196397e-05, 'lambda': 6.044604387780255, 'max_depth': 8, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8517\n",
      "[0]\ttrain-error:0.14706\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.14130\teval-error:0.15090                                      \n",
      "[2]\ttrain-error:0.14258\teval-error:0.15090                                      \n",
      "[3]\ttrain-error:0.14067\teval-error:0.15345                                      \n",
      "[4]\ttrain-error:0.13939\teval-error:0.16880                                      \n",
      "[5]\ttrain-error:0.14194\teval-error:0.16368                                      \n",
      "[6]\ttrain-error:0.14706\teval-error:0.17136                                      \n",
      "[7]\ttrain-error:0.14450\teval-error:0.16624                                      \n",
      "[8]\ttrain-error:0.13683\teval-error:0.16368                                      \n",
      "[9]\ttrain-error:0.13427\teval-error:0.16368                                      \n",
      "[10]\ttrain-error:0.13555\teval-error:0.15857                                     \n",
      "[11]\ttrain-error:0.13555\teval-error:0.15601                                     \n",
      "[12]\ttrain-error:0.12979\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.13043\teval-error:0.15090                                     \n",
      "[14]\ttrain-error:0.12788\teval-error:0.14067                                     \n",
      "[15]\ttrain-error:0.13043\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.12852\teval-error:0.14578                                     \n",
      "[17]\ttrain-error:0.12852\teval-error:0.14834                                     \n",
      "[18]\ttrain-error:0.12404\teval-error:0.14578                                     \n",
      "[19]\ttrain-error:0.12596\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.12276\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.12276\teval-error:0.14322                                     \n",
      "[22]\ttrain-error:0.12404\teval-error:0.14322                                     \n",
      "[23]\ttrain-error:0.12404\teval-error:0.14067                                     \n",
      "[24]\ttrain-error:0.12212\teval-error:0.13811                                     \n",
      "[25]\ttrain-error:0.12276\teval-error:0.14322                                     \n",
      "[26]\ttrain-error:0.11893\teval-error:0.14067                                     \n",
      "[27]\ttrain-error:0.11893\teval-error:0.13555                                     \n",
      "[28]\ttrain-error:0.11957\teval-error:0.13555                                     \n",
      "[29]\ttrain-error:0.11893\teval-error:0.13555                                     \n",
      "[30]\ttrain-error:0.11765\teval-error:0.13555                                     \n",
      "[31]\ttrain-error:0.11637\teval-error:0.13811                                     \n",
      "[32]\ttrain-error:0.11509\teval-error:0.13555                                     \n",
      "[33]\ttrain-error:0.11509\teval-error:0.13555                                     \n",
      "[34]\ttrain-error:0.11317\teval-error:0.13811                                     \n",
      "[35]\ttrain-error:0.11445\teval-error:0.13811                                     \n",
      "[36]\ttrain-error:0.11381\teval-error:0.13811                                     \n",
      "[37]\ttrain-error:0.11317\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.11317\teval-error:0.13811                                     \n",
      "[39]\ttrain-error:0.11253\teval-error:0.13811                                     \n",
      "[40]\ttrain-error:0.11253\teval-error:0.13811                                     \n",
      "[41]\ttrain-error:0.10934\teval-error:0.13811                                     \n",
      "[42]\ttrain-error:0.10934\teval-error:0.14067                                     \n",
      "[43]\ttrain-error:0.10870\teval-error:0.14322                                     \n",
      "[44]\ttrain-error:0.10806\teval-error:0.13811                                     \n",
      "[45]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[46]\ttrain-error:0.10678\teval-error:0.14067                                     \n",
      "[47]\ttrain-error:0.10678\teval-error:0.14067                                     \n",
      "[48]\ttrain-error:0.10614\teval-error:0.14322                                     \n",
      "[49]\ttrain-error:0.10230\teval-error:0.14322                                     \n",
      "[50]\ttrain-error:0.10038\teval-error:0.14322                                     \n",
      "[51]\ttrain-error:0.09910\teval-error:0.14322                                     \n",
      "[52]\ttrain-error:0.09846\teval-error:0.14322                                     \n",
      "[53]\ttrain-error:0.09399\teval-error:0.14322                                     \n",
      "[54]\ttrain-error:0.09527\teval-error:0.14578                                     \n",
      "[55]\ttrain-error:0.09527\teval-error:0.14578                                     \n",
      "[56]\ttrain-error:0.09463\teval-error:0.14578                                     \n",
      "[57]\ttrain-error:0.09463\teval-error:0.14578                                     \n",
      "[58]\ttrain-error:0.09463\teval-error:0.14578                                     \n",
      "[59]\ttrain-error:0.09463\teval-error:0.14578                                     \n",
      "[60]\ttrain-error:0.09399\teval-error:0.14578                                     \n",
      "[61]\ttrain-error:0.09271\teval-error:0.13811                                     \n",
      "[62]\ttrain-error:0.09079\teval-error:0.13555                                     \n",
      "[63]\ttrain-error:0.09015\teval-error:0.14578                                     \n",
      "[64]\ttrain-error:0.08887\teval-error:0.14578                                     \n",
      "[65]\ttrain-error:0.08887\teval-error:0.14578                                     \n",
      "[66]\ttrain-error:0.08760\teval-error:0.14578                                     \n",
      "[67]\ttrain-error:0.08568\teval-error:0.14322                                     \n",
      "[68]\ttrain-error:0.08568\teval-error:0.14322                                     \n",
      "[69]\ttrain-error:0.08504\teval-error:0.14578                                     \n",
      "[70]\ttrain-error:0.08376\teval-error:0.14578                                     \n",
      "[71]\ttrain-error:0.08248\teval-error:0.14322                                     \n",
      "[72]\ttrain-error:0.08504\teval-error:0.14322                                     \n",
      "[73]\ttrain-error:0.08312\teval-error:0.14322                                     \n",
      "[74]\ttrain-error:0.08056\teval-error:0.14067                                     \n",
      "[75]\ttrain-error:0.07928\teval-error:0.14322                                     \n",
      "[76]\ttrain-error:0.07928\teval-error:0.13555                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.0033082210523680835, 'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 3.9098708471955056e-08, 'lambda': 0.0008804676132208737, 'max_depth': 4, 'min_child_weight': 2.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.14706\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.14514\teval-error:0.15345                                      \n",
      "[2]\ttrain-error:0.14450\teval-error:0.15345                                      \n",
      "[3]\ttrain-error:0.14450\teval-error:0.15345                                      \n",
      "[4]\ttrain-error:0.14450\teval-error:0.15345                                      \n",
      "[5]\ttrain-error:0.14130\teval-error:0.15601                                      \n",
      "[6]\ttrain-error:0.13747\teval-error:0.16112                                      \n",
      "[7]\ttrain-error:0.14450\teval-error:0.15601                                      \n",
      "[8]\ttrain-error:0.13427\teval-error:0.15857                                      \n",
      "[9]\ttrain-error:0.13427\teval-error:0.15345                                      \n",
      "[10]\ttrain-error:0.13107\teval-error:0.15090                                     \n",
      "[11]\ttrain-error:0.12979\teval-error:0.15345                                     \n",
      "[12]\ttrain-error:0.13107\teval-error:0.14578                                     \n",
      "[13]\ttrain-error:0.12852\teval-error:0.14578                                     \n",
      "[14]\ttrain-error:0.12979\teval-error:0.14834                                     \n",
      "[15]\ttrain-error:0.12852\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.12468\teval-error:0.14322                                     \n",
      "[17]\ttrain-error:0.12468\teval-error:0.14578                                     \n",
      "[18]\ttrain-error:0.12596\teval-error:0.14067                                     \n",
      "[19]\ttrain-error:0.12532\teval-error:0.14067                                     \n",
      "[20]\ttrain-error:0.12404\teval-error:0.13555                                     \n",
      "[21]\ttrain-error:0.12212\teval-error:0.14067                                     \n",
      "[22]\ttrain-error:0.12148\teval-error:0.14067                                     \n",
      "[23]\ttrain-error:0.12084\teval-error:0.15090                                     \n",
      "[24]\ttrain-error:0.12084\teval-error:0.14834                                     \n",
      "[25]\ttrain-error:0.12021\teval-error:0.14322                                     \n",
      "[26]\ttrain-error:0.12276\teval-error:0.14834                                     \n",
      "[27]\ttrain-error:0.11957\teval-error:0.14322                                     \n",
      "[28]\ttrain-error:0.11829\teval-error:0.13555                                     \n",
      "[29]\ttrain-error:0.11509\teval-error:0.14067                                     \n",
      "[30]\ttrain-error:0.11701\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.11637\teval-error:0.14067                                     \n",
      "[32]\ttrain-error:0.11381\teval-error:0.14067                                     \n",
      "[33]\ttrain-error:0.11573\teval-error:0.13043                                     \n",
      "[34]\ttrain-error:0.11445\teval-error:0.13555                                     \n",
      "[35]\ttrain-error:0.11445\teval-error:0.13299                                     \n",
      "[36]\ttrain-error:0.11381\teval-error:0.13299                                     \n",
      "[37]\ttrain-error:0.11317\teval-error:0.13555                                     \n",
      "[38]\ttrain-error:0.10997\teval-error:0.13299                                     \n",
      "[39]\ttrain-error:0.11125\teval-error:0.13555                                     \n",
      "[40]\ttrain-error:0.11061\teval-error:0.13043                                     \n",
      "[41]\ttrain-error:0.10934\teval-error:0.13043                                     \n",
      "[42]\ttrain-error:0.11061\teval-error:0.13043                                     \n",
      "[43]\ttrain-error:0.10934\teval-error:0.12788                                     \n",
      "[44]\ttrain-error:0.10934\teval-error:0.12788                                     \n",
      "[45]\ttrain-error:0.10870\teval-error:0.13043                                     \n",
      "[46]\ttrain-error:0.10870\teval-error:0.13043                                     \n",
      "[47]\ttrain-error:0.10678\teval-error:0.13043                                     \n",
      "[48]\ttrain-error:0.10550\teval-error:0.13043                                     \n",
      "[49]\ttrain-error:0.10358\teval-error:0.13299                                     \n",
      "[50]\ttrain-error:0.10230\teval-error:0.13555                                     \n",
      "[51]\ttrain-error:0.10166\teval-error:0.13811                                     \n",
      "[52]\ttrain-error:0.10102\teval-error:0.13043                                     \n",
      "[53]\ttrain-error:0.10038\teval-error:0.13299                                     \n",
      "[54]\ttrain-error:0.10038\teval-error:0.13555                                     \n",
      "[55]\ttrain-error:0.10038\teval-error:0.13555                                     \n",
      "[56]\ttrain-error:0.09783\teval-error:0.13555                                     \n",
      "[57]\ttrain-error:0.09910\teval-error:0.13555                                     \n",
      "[58]\ttrain-error:0.09655\teval-error:0.13555                                     \n",
      "[59]\ttrain-error:0.09527\teval-error:0.13811                                     \n",
      "[60]\ttrain-error:0.09463\teval-error:0.13811                                     \n",
      "[61]\ttrain-error:0.09463\teval-error:0.14067                                     \n",
      "[62]\ttrain-error:0.09143\teval-error:0.14322                                     \n",
      "[63]\ttrain-error:0.08951\teval-error:0.14322                                     \n",
      "[64]\ttrain-error:0.08823\teval-error:0.14067                                     \n",
      "[65]\ttrain-error:0.08760\teval-error:0.14067                                     \n",
      "[66]\ttrain-error:0.08696\teval-error:0.13811                                     \n",
      "[67]\ttrain-error:0.08760\teval-error:0.13555                                     \n",
      "[68]\ttrain-error:0.08760\teval-error:0.13811                                     \n",
      "[69]\ttrain-error:0.08887\teval-error:0.13811                                     \n",
      "[70]\ttrain-error:0.08696\teval-error:0.13811                                     \n",
      "[71]\ttrain-error:0.08504\teval-error:0.14322                                     \n",
      "[72]\ttrain-error:0.08440\teval-error:0.14067                                     \n",
      "[73]\ttrain-error:0.08568\teval-error:0.13811                                     \n",
      "[74]\ttrain-error:0.08568\teval-error:0.14067                                     \n",
      "[75]\ttrain-error:0.08504\teval-error:0.14067                                     \n",
      "[76]\ttrain-error:0.08440\teval-error:0.14067                                     \n",
      "[77]\ttrain-error:0.08440\teval-error:0.14322                                     \n",
      "[78]\ttrain-error:0.08056\teval-error:0.14322                                     \n",
      "[79]\ttrain-error:0.08312\teval-error:0.14067                                     \n",
      "[80]\ttrain-error:0.08440\teval-error:0.14067                                     \n",
      "[81]\ttrain-error:0.08056\teval-error:0.14322                                     \n",
      "[82]\ttrain-error:0.07928\teval-error:0.14322                                     \n",
      "[83]\ttrain-error:0.07928\teval-error:0.14322                                     \n",
      "[84]\ttrain-error:0.07801\teval-error:0.14322                                     \n",
      "[85]\ttrain-error:0.07673\teval-error:0.14322                                     \n",
      "[86]\ttrain-error:0.07801\teval-error:0.14578                                     \n",
      "[87]\ttrain-error:0.07801\teval-error:0.15090                                     \n",
      "[88]\ttrain-error:0.07609\teval-error:0.15090                                     \n",
      "[89]\ttrain-error:0.07673\teval-error:0.15345                                     \n",
      "[90]\ttrain-error:0.07609\teval-error:0.15090                                     \n",
      "[91]\ttrain-error:0.07545\teval-error:0.14834                                     \n",
      "[92]\ttrain-error:0.07289\teval-error:0.14834                                     \n",
      "[93]\ttrain-error:0.07225\teval-error:0.14322                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.0007771965590141622, 'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 2.402921211411591e-05, 'lambda': 0.011644344275499728, 'max_depth': 4, 'min_child_weight': 3.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8568\n",
      "[0]\ttrain-error:0.14898\teval-error:0.16880                                      \n",
      "[1]\ttrain-error:0.14003\teval-error:0.16880                                      \n",
      "[2]\ttrain-error:0.13619\teval-error:0.16368                                      \n",
      "[3]\ttrain-error:0.13811\teval-error:0.16624                                      \n",
      "[4]\ttrain-error:0.13363\teval-error:0.16368                                      \n",
      "[5]\ttrain-error:0.13171\teval-error:0.15857                                      \n",
      "[6]\ttrain-error:0.13171\teval-error:0.17647                                      \n",
      "[7]\ttrain-error:0.13491\teval-error:0.16624                                      \n",
      "[8]\ttrain-error:0.12916\teval-error:0.15857                                      \n",
      "[9]\ttrain-error:0.12340\teval-error:0.14578                                      \n",
      "[10]\ttrain-error:0.12084\teval-error:0.14578                                     \n",
      "[11]\ttrain-error:0.12148\teval-error:0.14578                                     \n",
      "[12]\ttrain-error:0.12084\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.12084\teval-error:0.15090                                     \n",
      "[14]\ttrain-error:0.11829\teval-error:0.13811                                     \n",
      "[15]\ttrain-error:0.12148\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.12021\teval-error:0.14067                                     \n",
      "[17]\ttrain-error:0.11893\teval-error:0.14067                                     \n",
      "[18]\ttrain-error:0.11509\teval-error:0.13811                                     \n",
      "[19]\ttrain-error:0.11317\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.11381\teval-error:0.13811                                     \n",
      "[21]\ttrain-error:0.11125\teval-error:0.14322                                     \n",
      "[22]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[23]\ttrain-error:0.10806\teval-error:0.14067                                     \n",
      "[24]\ttrain-error:0.10742\teval-error:0.14067                                     \n",
      "[25]\ttrain-error:0.10614\teval-error:0.13811                                     \n",
      "[26]\ttrain-error:0.10614\teval-error:0.14067                                     \n",
      "[27]\ttrain-error:0.10102\teval-error:0.13811                                     \n",
      "[28]\ttrain-error:0.09846\teval-error:0.13555                                     \n",
      "[29]\ttrain-error:0.09719\teval-error:0.13555                                     \n",
      "[30]\ttrain-error:0.09783\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.09463\teval-error:0.13811                                     \n",
      "[32]\ttrain-error:0.09079\teval-error:0.13555                                     \n",
      "[33]\ttrain-error:0.09143\teval-error:0.13811                                     \n",
      "[34]\ttrain-error:0.08951\teval-error:0.13811                                     \n",
      "[35]\ttrain-error:0.08760\teval-error:0.13299                                     \n",
      "[36]\ttrain-error:0.08504\teval-error:0.13555                                     \n",
      "[37]\ttrain-error:0.08248\teval-error:0.13811                                     \n",
      "[38]\ttrain-error:0.08248\teval-error:0.13811                                     \n",
      "[39]\ttrain-error:0.08248\teval-error:0.13299                                     \n",
      "[40]\ttrain-error:0.08056\teval-error:0.13811                                     \n",
      "[41]\ttrain-error:0.07864\teval-error:0.13555                                     \n",
      "[42]\ttrain-error:0.07928\teval-error:0.13555                                     \n",
      "[43]\ttrain-error:0.07673\teval-error:0.13555                                     \n",
      "[44]\ttrain-error:0.07673\teval-error:0.13043                                     \n",
      "[45]\ttrain-error:0.07481\teval-error:0.13299                                     \n",
      "[46]\ttrain-error:0.07353\teval-error:0.13555                                     \n",
      "[47]\ttrain-error:0.07417\teval-error:0.13299                                     \n",
      "[48]\ttrain-error:0.07353\teval-error:0.13555                                     \n",
      "[49]\ttrain-error:0.07225\teval-error:0.13299                                     \n",
      "[50]\ttrain-error:0.07033\teval-error:0.13555                                     \n",
      "[51]\ttrain-error:0.06969\teval-error:0.13555                                     \n",
      "[52]\ttrain-error:0.06778\teval-error:0.14067                                     \n",
      "[53]\ttrain-error:0.06650\teval-error:0.13299                                     \n",
      "[54]\ttrain-error:0.06586\teval-error:0.13555                                     \n",
      "[55]\ttrain-error:0.06586\teval-error:0.13555                                     \n",
      "[56]\ttrain-error:0.06586\teval-error:0.13555                                     \n",
      "[57]\ttrain-error:0.06458\teval-error:0.13811                                     \n",
      "[58]\ttrain-error:0.06330\teval-error:0.13555                                     \n",
      "[59]\ttrain-error:0.06074\teval-error:0.13299                                     \n",
      "[60]\ttrain-error:0.06074\teval-error:0.13811                                     \n",
      "[61]\ttrain-error:0.05946\teval-error:0.13555                                     \n",
      "[62]\ttrain-error:0.05946\teval-error:0.13555                                     \n",
      "[63]\ttrain-error:0.05754\teval-error:0.13555                                     \n",
      "[64]\ttrain-error:0.05818\teval-error:0.14067                                     \n",
      "[65]\ttrain-error:0.05818\teval-error:0.14067                                     \n",
      "[66]\ttrain-error:0.05563\teval-error:0.13555                                     \n",
      "[67]\ttrain-error:0.05627\teval-error:0.13299                                     \n",
      "[68]\ttrain-error:0.05435\teval-error:0.13555                                     \n",
      "[69]\ttrain-error:0.05307\teval-error:0.13043                                     \n",
      "[70]\ttrain-error:0.05307\teval-error:0.13299                                     \n",
      "[71]\ttrain-error:0.05307\teval-error:0.13555                                     \n",
      "[72]\ttrain-error:0.05243\teval-error:0.13299                                     \n",
      "[73]\ttrain-error:0.05243\teval-error:0.13811                                     \n",
      "[74]\ttrain-error:0.05051\teval-error:0.13299                                     \n",
      "[75]\ttrain-error:0.05051\teval-error:0.14067                                     \n",
      "[76]\ttrain-error:0.04923\teval-error:0.14067                                     \n",
      "[77]\ttrain-error:0.04859\teval-error:0.13555                                     \n",
      "[78]\ttrain-error:0.04732\teval-error:0.13555                                     \n",
      "[79]\ttrain-error:0.04668\teval-error:0.14067                                     \n",
      "[80]\ttrain-error:0.04668\teval-error:0.13555                                     \n",
      "[81]\ttrain-error:0.04604\teval-error:0.13555                                     \n",
      "[82]\ttrain-error:0.04476\teval-error:0.13811                                     \n",
      "[83]\ttrain-error:0.04604\teval-error:0.13811                                     \n",
      "[84]\ttrain-error:0.04476\teval-error:0.13811                                     \n",
      "[85]\ttrain-error:0.04284\teval-error:0.13555                                     \n",
      "[86]\ttrain-error:0.04156\teval-error:0.13555                                     \n",
      "[87]\ttrain-error:0.03836\teval-error:0.13555                                     \n",
      "[88]\ttrain-error:0.04092\teval-error:0.13555                                     \n",
      "[89]\ttrain-error:0.03900\teval-error:0.14067                                     \n",
      "[90]\ttrain-error:0.03964\teval-error:0.13811                                     \n",
      "[91]\ttrain-error:0.03772\teval-error:0.13811                                     \n",
      "[92]\ttrain-error:0.03644\teval-error:0.14067                                     \n",
      "[93]\ttrain-error:0.03644\teval-error:0.14067                                     \n",
      "[94]\ttrain-error:0.03708\teval-error:0.13811                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.0005545453499935225, 'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.03315537992515972, 'lambda': 0.000850817444635001, 'max_depth': 7, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8619\n",
      "[0]\ttrain-error:0.14706\teval-error:0.15601                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-error:0.14898\teval-error:0.16624                                      \n",
      "[2]\ttrain-error:0.14386\teval-error:0.15857                                      \n",
      "[3]\ttrain-error:0.14067\teval-error:0.15857                                      \n",
      "[4]\ttrain-error:0.14578\teval-error:0.15601                                      \n",
      "[5]\ttrain-error:0.13747\teval-error:0.15601                                      \n",
      "[6]\ttrain-error:0.13811\teval-error:0.14578                                      \n",
      "[7]\ttrain-error:0.13427\teval-error:0.14578                                      \n",
      "[8]\ttrain-error:0.13491\teval-error:0.14578                                      \n",
      "[9]\ttrain-error:0.13107\teval-error:0.14578                                      \n",
      "[10]\ttrain-error:0.13555\teval-error:0.14578                                     \n",
      "[11]\ttrain-error:0.13171\teval-error:0.14578                                     \n",
      "[12]\ttrain-error:0.12916\teval-error:0.14322                                     \n",
      "[13]\ttrain-error:0.12724\teval-error:0.13811                                     \n",
      "[14]\ttrain-error:0.12660\teval-error:0.14322                                     \n",
      "[15]\ttrain-error:0.12596\teval-error:0.13555                                     \n",
      "[16]\ttrain-error:0.12212\teval-error:0.13299                                     \n",
      "[17]\ttrain-error:0.12212\teval-error:0.13299                                     \n",
      "[18]\ttrain-error:0.11573\teval-error:0.13555                                     \n",
      "[19]\ttrain-error:0.12021\teval-error:0.13299                                     \n",
      "[20]\ttrain-error:0.12021\teval-error:0.13043                                     \n",
      "[21]\ttrain-error:0.11573\teval-error:0.13555                                     \n",
      "[22]\ttrain-error:0.11445\teval-error:0.13043                                     \n",
      "[23]\ttrain-error:0.11445\teval-error:0.12788                                     \n",
      "[24]\ttrain-error:0.10997\teval-error:0.13043                                     \n",
      "[25]\ttrain-error:0.10678\teval-error:0.13043                                     \n",
      "[26]\ttrain-error:0.10997\teval-error:0.12788                                     \n",
      "[27]\ttrain-error:0.10678\teval-error:0.13043                                     \n",
      "[28]\ttrain-error:0.10358\teval-error:0.13299                                     \n",
      "[29]\ttrain-error:0.10230\teval-error:0.12532                                     \n",
      "[30]\ttrain-error:0.10294\teval-error:0.12532                                     \n",
      "[31]\ttrain-error:0.09910\teval-error:0.12788                                     \n",
      "[32]\ttrain-error:0.09015\teval-error:0.12532                                     \n",
      "[33]\ttrain-error:0.09143\teval-error:0.12788                                     \n",
      "[34]\ttrain-error:0.08951\teval-error:0.12532                                     \n",
      "[35]\ttrain-error:0.08696\teval-error:0.13043                                     \n",
      "[36]\ttrain-error:0.08696\teval-error:0.12532                                     \n",
      "[37]\ttrain-error:0.08632\teval-error:0.13043                                     \n",
      "[38]\ttrain-error:0.08568\teval-error:0.13043                                     \n",
      "[39]\ttrain-error:0.08440\teval-error:0.13811                                     \n",
      "[40]\ttrain-error:0.08568\teval-error:0.14067                                     \n",
      "[41]\ttrain-error:0.08312\teval-error:0.13811                                     \n",
      "[42]\ttrain-error:0.08376\teval-error:0.14322                                     \n",
      "[43]\ttrain-error:0.07992\teval-error:0.14322                                     \n",
      "[44]\ttrain-error:0.08120\teval-error:0.13811                                     \n",
      "[45]\ttrain-error:0.08056\teval-error:0.13043                                     \n",
      "[46]\ttrain-error:0.07737\teval-error:0.13043                                     \n",
      "[47]\ttrain-error:0.07545\teval-error:0.13043                                     \n",
      "[48]\ttrain-error:0.07481\teval-error:0.13299                                     \n",
      "[49]\ttrain-error:0.07353\teval-error:0.13299                                     \n",
      "[50]\ttrain-error:0.07225\teval-error:0.13811                                     \n",
      "[51]\ttrain-error:0.07033\teval-error:0.13555                                     \n",
      "[52]\ttrain-error:0.07097\teval-error:0.14322                                     \n",
      "[53]\ttrain-error:0.06905\teval-error:0.14067                                     \n",
      "[54]\ttrain-error:0.06841\teval-error:0.13811                                     \n",
      "[55]\ttrain-error:0.06586\teval-error:0.14067                                     \n",
      "[56]\ttrain-error:0.06586\teval-error:0.14067                                     \n",
      "[57]\ttrain-error:0.06394\teval-error:0.14067                                     \n",
      "[58]\ttrain-error:0.06330\teval-error:0.14067                                     \n",
      "[59]\ttrain-error:0.06330\teval-error:0.13811                                     \n",
      "[60]\ttrain-error:0.06266\teval-error:0.14067                                     \n",
      "[61]\ttrain-error:0.06074\teval-error:0.14322                                     \n",
      "[62]\ttrain-error:0.06138\teval-error:0.14322                                     \n",
      "[63]\ttrain-error:0.05818\teval-error:0.14322                                     \n",
      "[64]\ttrain-error:0.05818\teval-error:0.14578                                     \n",
      "[65]\ttrain-error:0.05818\teval-error:0.14834                                     \n",
      "[66]\ttrain-error:0.05563\teval-error:0.14834                                     \n",
      "[67]\ttrain-error:0.05563\teval-error:0.14322                                     \n",
      "[68]\ttrain-error:0.05435\teval-error:0.14322                                     \n",
      "[69]\ttrain-error:0.05243\teval-error:0.14322                                     \n",
      "[70]\ttrain-error:0.05179\teval-error:0.14578                                     \n",
      "[71]\ttrain-error:0.05051\teval-error:0.14322                                     \n",
      "[72]\ttrain-error:0.05051\teval-error:0.14322                                     \n",
      "[73]\ttrain-error:0.04859\teval-error:0.14067                                     \n",
      "[74]\ttrain-error:0.04604\teval-error:0.14322                                     \n",
      "[75]\ttrain-error:0.04668\teval-error:0.14578                                     \n",
      "[76]\ttrain-error:0.04348\teval-error:0.14322                                     \n",
      "[77]\ttrain-error:0.04540\teval-error:0.14578                                     \n",
      "[78]\ttrain-error:0.04476\teval-error:0.14322                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 3.881510399323762e-07, 'booster': 'gbtree', 'colsample_bytree': 0.9, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.002982727156236776, 'lambda': 0.00027518418426570034, 'max_depth': 8, 'min_child_weight': 5.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8517\n",
      "[0]\ttrain-error:0.13811\teval-error:0.15090                                      \n",
      "[1]\ttrain-error:0.13683\teval-error:0.14834                                      \n",
      "[2]\ttrain-error:0.13299\teval-error:0.15090                                      \n",
      "[3]\ttrain-error:0.14194\teval-error:0.15857                                      \n",
      "[4]\ttrain-error:0.13619\teval-error:0.14834                                      \n",
      "[5]\ttrain-error:0.13363\teval-error:0.15090                                      \n",
      "[6]\ttrain-error:0.13299\teval-error:0.15345                                      \n",
      "[7]\ttrain-error:0.13427\teval-error:0.16112                                      \n",
      "[8]\ttrain-error:0.13171\teval-error:0.15345                                      \n",
      "[9]\ttrain-error:0.12532\teval-error:0.15857                                      \n",
      "[10]\ttrain-error:0.12212\teval-error:0.15090                                     \n",
      "[11]\ttrain-error:0.12276\teval-error:0.14578                                     \n",
      "[12]\ttrain-error:0.11957\teval-error:0.13811                                     \n",
      "[13]\ttrain-error:0.11765\teval-error:0.14067                                     \n",
      "[14]\ttrain-error:0.11765\teval-error:0.13811                                     \n",
      "[15]\ttrain-error:0.11637\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.11381\teval-error:0.14322                                     \n",
      "[17]\ttrain-error:0.11381\teval-error:0.14067                                     \n",
      "[18]\ttrain-error:0.11253\teval-error:0.14067                                     \n",
      "[19]\ttrain-error:0.11253\teval-error:0.14322                                     \n",
      "[20]\ttrain-error:0.11189\teval-error:0.13555                                     \n",
      "[21]\ttrain-error:0.11189\teval-error:0.14067                                     \n",
      "[22]\ttrain-error:0.11061\teval-error:0.14322                                     \n",
      "[23]\ttrain-error:0.10870\teval-error:0.13811                                     \n",
      "[24]\ttrain-error:0.10678\teval-error:0.13811                                     \n",
      "[25]\ttrain-error:0.10550\teval-error:0.13811                                     \n",
      "[26]\ttrain-error:0.10550\teval-error:0.13811                                     \n",
      "[27]\ttrain-error:0.10550\teval-error:0.13811                                     \n",
      "[28]\ttrain-error:0.10358\teval-error:0.14067                                     \n",
      "[29]\ttrain-error:0.10166\teval-error:0.13811                                     \n",
      "[30]\ttrain-error:0.10102\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.09910\teval-error:0.13555                                     \n",
      "[32]\ttrain-error:0.09783\teval-error:0.13555                                     \n",
      "[33]\ttrain-error:0.09591\teval-error:0.13555                                     \n",
      "[34]\ttrain-error:0.09527\teval-error:0.13555                                     \n",
      "[35]\ttrain-error:0.09719\teval-error:0.13811                                     \n",
      "[36]\ttrain-error:0.09207\teval-error:0.13299                                     \n",
      "[37]\ttrain-error:0.09271\teval-error:0.13299                                     \n",
      "[38]\ttrain-error:0.09207\teval-error:0.13299                                     \n",
      "[39]\ttrain-error:0.08887\teval-error:0.13299                                     \n",
      "[40]\ttrain-error:0.08504\teval-error:0.13299                                     \n",
      "[41]\ttrain-error:0.08376\teval-error:0.13299                                     \n",
      "[42]\ttrain-error:0.08120\teval-error:0.13555                                     \n",
      "[43]\ttrain-error:0.08120\teval-error:0.13299                                     \n",
      "[44]\ttrain-error:0.07992\teval-error:0.13299                                     \n",
      "[45]\ttrain-error:0.07992\teval-error:0.13555                                     \n",
      "[46]\ttrain-error:0.07928\teval-error:0.13555                                     \n",
      "[47]\ttrain-error:0.07609\teval-error:0.13555                                     \n",
      "[48]\ttrain-error:0.07481\teval-error:0.13299                                     \n",
      "[49]\ttrain-error:0.07417\teval-error:0.13299                                     \n",
      "[50]\ttrain-error:0.07353\teval-error:0.13299                                     \n",
      "[51]\ttrain-error:0.07033\teval-error:0.13299                                     \n",
      "[52]\ttrain-error:0.06841\teval-error:0.13555                                     \n",
      "[53]\ttrain-error:0.06586\teval-error:0.13299                                     \n",
      "[54]\ttrain-error:0.06330\teval-error:0.13299                                     \n",
      "[55]\ttrain-error:0.06330\teval-error:0.13299                                     \n",
      "[56]\ttrain-error:0.06202\teval-error:0.12788                                     \n",
      "[57]\ttrain-error:0.06074\teval-error:0.13043                                     \n",
      "[58]\ttrain-error:0.05946\teval-error:0.13299                                     \n",
      "[59]\ttrain-error:0.05818\teval-error:0.13043                                     \n",
      "[60]\ttrain-error:0.05690\teval-error:0.12788                                     \n",
      "[61]\ttrain-error:0.05435\teval-error:0.13043                                     \n",
      "[62]\ttrain-error:0.05499\teval-error:0.13043                                     \n",
      "[63]\ttrain-error:0.05307\teval-error:0.13043                                     \n",
      "[64]\ttrain-error:0.05307\teval-error:0.13043                                     \n",
      "[65]\ttrain-error:0.05307\teval-error:0.13043                                     \n",
      "[66]\ttrain-error:0.04923\teval-error:0.13043                                     \n",
      "[67]\ttrain-error:0.04732\teval-error:0.12788                                     \n",
      "[68]\ttrain-error:0.04668\teval-error:0.12788                                     \n",
      "[69]\ttrain-error:0.04540\teval-error:0.12788                                     \n",
      "[70]\ttrain-error:0.04540\teval-error:0.12788                                     \n",
      "[71]\ttrain-error:0.04348\teval-error:0.12788                                     \n",
      "[72]\ttrain-error:0.04476\teval-error:0.12788                                     \n",
      "[73]\ttrain-error:0.04476\teval-error:0.12788                                     \n",
      "[74]\ttrain-error:0.04156\teval-error:0.12788                                     \n",
      "[75]\ttrain-error:0.04156\teval-error:0.12788                                     \n",
      "[76]\ttrain-error:0.04220\teval-error:0.12788                                     \n",
      "[77]\ttrain-error:0.04156\teval-error:0.12788                                     \n",
      "[78]\ttrain-error:0.04092\teval-error:0.13043                                     \n",
      "[79]\ttrain-error:0.04028\teval-error:0.13043                                     \n",
      "[80]\ttrain-error:0.03964\teval-error:0.13043                                     \n",
      "[81]\ttrain-error:0.03964\teval-error:0.12788                                     \n",
      "[82]\ttrain-error:0.03836\teval-error:0.12532                                     \n",
      "[83]\ttrain-error:0.03772\teval-error:0.12532                                     \n",
      "[84]\ttrain-error:0.03644\teval-error:0.12532                                     \n",
      "[85]\ttrain-error:0.03517\teval-error:0.12532                                     \n",
      "[86]\ttrain-error:0.03517\teval-error:0.12532                                     \n",
      "[87]\ttrain-error:0.03325\teval-error:0.12788                                     \n",
      "[88]\ttrain-error:0.03517\teval-error:0.12788                                     \n",
      "[89]\ttrain-error:0.03453\teval-error:0.13043                                     \n",
      "[90]\ttrain-error:0.03325\teval-error:0.13299                                     \n",
      "[91]\ttrain-error:0.03197\teval-error:0.13299                                     \n",
      "[92]\ttrain-error:0.03197\teval-error:0.13299                                     \n",
      "[93]\ttrain-error:0.03197\teval-error:0.13299                                     \n",
      "[94]\ttrain-error:0.03069\teval-error:0.13299                                     \n",
      "[95]\ttrain-error:0.03069\teval-error:0.13299                                     \n",
      "[96]\ttrain-error:0.03069\teval-error:0.13299                                     \n",
      "[97]\ttrain-error:0.03005\teval-error:0.13299                                     \n",
      "[98]\ttrain-error:0.03005\teval-error:0.13043                                     \n",
      "[99]\ttrain-error:0.02941\teval-error:0.13299                                     \n",
      "[100]\ttrain-error:0.02941\teval-error:0.13299                                    \n",
      "[101]\ttrain-error:0.02941\teval-error:0.13299                                    \n",
      "[102]\ttrain-error:0.02941\teval-error:0.13043                                    \n",
      "[103]\ttrain-error:0.02941\teval-error:0.13043                                    \n",
      "[104]\ttrain-error:0.02877\teval-error:0.13299                                    \n",
      "[105]\ttrain-error:0.02749\teval-error:0.13043                                    \n",
      "[106]\ttrain-error:0.02749\teval-error:0.12788                                    \n",
      "[107]\ttrain-error:0.02685\teval-error:0.13043                                    \n",
      "[108]\ttrain-error:0.02685\teval-error:0.13043                                    \n",
      "[109]\ttrain-error:0.02621\teval-error:0.12788                                    \n",
      "[110]\ttrain-error:0.02558\teval-error:0.13043                                    \n",
      "[111]\ttrain-error:0.02558\teval-error:0.12788                                    \n",
      "[112]\ttrain-error:0.02558\teval-error:0.13299                                    \n",
      "[113]\ttrain-error:0.02494\teval-error:0.13299                                    \n",
      "[114]\ttrain-error:0.02494\teval-error:0.13299                                    \n",
      "[115]\ttrain-error:0.02558\teval-error:0.13299                                    \n",
      "[116]\ttrain-error:0.02558\teval-error:0.13555                                    \n",
      "[117]\ttrain-error:0.02558\teval-error:0.13555                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118]\ttrain-error:0.02558\teval-error:0.13555                                    \n",
      "[119]\ttrain-error:0.02494\teval-error:0.13299                                    \n",
      "[120]\ttrain-error:0.02494\teval-error:0.13299                                    \n",
      "[121]\ttrain-error:0.02430\teval-error:0.13555                                    \n",
      "[122]\ttrain-error:0.02430\teval-error:0.13555                                    \n",
      "[123]\ttrain-error:0.02302\teval-error:0.13555                                    \n",
      "[124]\ttrain-error:0.02302\teval-error:0.13555                                    \n",
      "[125]\ttrain-error:0.02174\teval-error:0.13555                                    \n",
      "[126]\ttrain-error:0.01982\teval-error:0.13555                                    \n",
      "[127]\ttrain-error:0.01918\teval-error:0.13555                                    \n",
      "[128]\ttrain-error:0.01982\teval-error:0.13555                                    \n",
      "[129]\ttrain-error:0.01854\teval-error:0.13555                                    \n",
      "[130]\ttrain-error:0.01918\teval-error:0.13555                                    \n",
      "[131]\ttrain-error:0.01918\teval-error:0.13811                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 0.0012971811915216706, 'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.0009879757301228382, 'lambda': 6.270140782903342, 'max_depth': 7, 'min_child_weight': 1.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8619\n",
      "[0]\ttrain-error:0.14130\teval-error:0.14322                                      \n",
      "[1]\ttrain-error:0.13939\teval-error:0.14578                                      \n",
      "[2]\ttrain-error:0.14322\teval-error:0.14578                                      \n",
      "[3]\ttrain-error:0.14322\teval-error:0.14834                                      \n",
      "[4]\ttrain-error:0.14514\teval-error:0.15090                                      \n",
      "[5]\ttrain-error:0.14067\teval-error:0.14578                                      \n",
      "[6]\ttrain-error:0.14130\teval-error:0.14834                                      \n",
      "[7]\ttrain-error:0.13939\teval-error:0.14834                                      \n",
      "[8]\ttrain-error:0.13747\teval-error:0.14834                                      \n",
      "[9]\ttrain-error:0.13555\teval-error:0.15090                                      \n",
      "[10]\ttrain-error:0.13555\teval-error:0.15090                                     \n",
      "[11]\ttrain-error:0.13555\teval-error:0.14834                                     \n",
      "[12]\ttrain-error:0.13299\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.13171\teval-error:0.14067                                     \n",
      "[14]\ttrain-error:0.13299\teval-error:0.14067                                     \n",
      "[15]\ttrain-error:0.13299\teval-error:0.14322                                     \n",
      "[16]\ttrain-error:0.13043\teval-error:0.14067                                     \n",
      "[17]\ttrain-error:0.13171\teval-error:0.14067                                     \n",
      "[18]\ttrain-error:0.12916\teval-error:0.14067                                     \n",
      "[19]\ttrain-error:0.12596\teval-error:0.13811                                     \n",
      "[20]\ttrain-error:0.12724\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.12788\teval-error:0.14322                                     \n",
      "[22]\ttrain-error:0.12916\teval-error:0.13811                                     \n",
      "[23]\ttrain-error:0.12724\teval-error:0.13811                                     \n",
      "[24]\ttrain-error:0.12724\teval-error:0.14067                                     \n",
      "[25]\ttrain-error:0.12788\teval-error:0.13555                                     \n",
      "[26]\ttrain-error:0.12468\teval-error:0.14067                                     \n",
      "[27]\ttrain-error:0.12468\teval-error:0.13555                                     \n",
      "[28]\ttrain-error:0.12340\teval-error:0.13299                                     \n",
      "[29]\ttrain-error:0.12532\teval-error:0.13299                                     \n",
      "[30]\ttrain-error:0.12340\teval-error:0.13811                                     \n",
      "[31]\ttrain-error:0.12404\teval-error:0.13555                                     \n",
      "[32]\ttrain-error:0.12021\teval-error:0.13299                                     \n",
      "[33]\ttrain-error:0.12148\teval-error:0.13555                                     \n",
      "[34]\ttrain-error:0.12084\teval-error:0.13299                                     \n",
      "[35]\ttrain-error:0.12021\teval-error:0.13555                                     \n",
      "[36]\ttrain-error:0.11957\teval-error:0.13555                                     \n",
      "[37]\ttrain-error:0.11701\teval-error:0.13555                                     \n",
      "[38]\ttrain-error:0.11765\teval-error:0.13811                                     \n",
      "[39]\ttrain-error:0.11829\teval-error:0.14578                                     \n",
      "[40]\ttrain-error:0.11701\teval-error:0.14322                                     \n",
      "[41]\ttrain-error:0.11509\teval-error:0.14322                                     \n",
      "[42]\ttrain-error:0.11509\teval-error:0.14578                                     \n",
      "[43]\ttrain-error:0.11317\teval-error:0.14322                                     \n",
      "[44]\ttrain-error:0.11381\teval-error:0.14578                                     \n",
      "[45]\ttrain-error:0.11189\teval-error:0.14578                                     \n",
      "[46]\ttrain-error:0.11189\teval-error:0.14322                                     \n",
      "[47]\ttrain-error:0.11253\teval-error:0.13811                                     \n",
      "[48]\ttrain-error:0.11189\teval-error:0.14067                                     \n",
      "[49]\ttrain-error:0.11061\teval-error:0.14322                                     \n",
      "[50]\ttrain-error:0.10934\teval-error:0.13811                                     \n",
      "[51]\ttrain-error:0.10934\teval-error:0.14322                                     \n",
      "[52]\ttrain-error:0.10806\teval-error:0.13811                                     \n",
      "[53]\ttrain-error:0.10934\teval-error:0.13555                                     \n",
      "[54]\ttrain-error:0.10742\teval-error:0.13811                                     \n",
      "[55]\ttrain-error:0.10550\teval-error:0.13811                                     \n",
      "[56]\ttrain-error:0.10486\teval-error:0.14067                                     \n",
      "[57]\ttrain-error:0.10550\teval-error:0.14067                                     \n",
      "[58]\ttrain-error:0.10486\teval-error:0.13811                                     \n",
      "[59]\ttrain-error:0.10486\teval-error:0.13555                                     \n",
      "[60]\ttrain-error:0.10358\teval-error:0.13555                                     \n",
      "[61]\ttrain-error:0.10294\teval-error:0.14067                                     \n",
      "[62]\ttrain-error:0.10486\teval-error:0.14067                                     \n",
      "[63]\ttrain-error:0.10358\teval-error:0.14067                                     \n",
      "[64]\ttrain-error:0.10166\teval-error:0.14067                                     \n",
      "[65]\ttrain-error:0.10230\teval-error:0.13811                                     \n",
      "[66]\ttrain-error:0.10102\teval-error:0.14322                                     \n",
      "[67]\ttrain-error:0.10102\teval-error:0.14578                                     \n",
      "[68]\ttrain-error:0.09719\teval-error:0.14578                                     \n",
      "[69]\ttrain-error:0.09719\teval-error:0.14578                                     \n",
      "[70]\ttrain-error:0.09655\teval-error:0.14578                                     \n",
      "[71]\ttrain-error:0.09527\teval-error:0.14578                                     \n",
      "[72]\ttrain-error:0.09335\teval-error:0.14578                                     \n",
      "[73]\ttrain-error:0.09271\teval-error:0.14578                                     \n",
      "[74]\ttrain-error:0.09143\teval-error:0.14322                                     \n",
      "[75]\ttrain-error:0.08951\teval-error:0.14067                                     \n",
      "[76]\ttrain-error:0.08823\teval-error:0.14067                                     \n",
      "[77]\ttrain-error:0.08760\teval-error:0.14322                                     \n",
      "[78]\ttrain-error:0.08887\teval-error:0.14322                                     \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 2.360977214783455e-05, 'booster': 'gbtree', 'colsample_bytree': 0.8500000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.3620641082107287, 'lambda': 1.9377542128016094, 'max_depth': 4, 'min_child_weight': 3.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8568\n",
      "[0]\ttrain-error:0.14450\teval-error:0.16112                                      \n",
      "[1]\ttrain-error:0.14386\teval-error:0.16368                                      \n",
      "[2]\ttrain-error:0.13683\teval-error:0.14834                                      \n",
      "[3]\ttrain-error:0.13427\teval-error:0.14578                                      \n",
      "[4]\ttrain-error:0.13299\teval-error:0.14578                                      \n",
      "[5]\ttrain-error:0.13235\teval-error:0.14578                                      \n",
      "[6]\ttrain-error:0.13043\teval-error:0.15345                                      \n",
      "[7]\ttrain-error:0.13171\teval-error:0.16112                                      \n",
      "[8]\ttrain-error:0.12852\teval-error:0.15601                                      \n",
      "[9]\ttrain-error:0.12596\teval-error:0.15090                                      \n",
      "[10]\ttrain-error:0.12212\teval-error:0.14834                                     \n",
      "[11]\ttrain-error:0.11957\teval-error:0.15345                                     \n",
      "[12]\ttrain-error:0.12212\teval-error:0.14834                                     \n",
      "[13]\ttrain-error:0.12276\teval-error:0.15090                                     \n",
      "[14]\ttrain-error:0.11893\teval-error:0.14322                                     \n",
      "[15]\ttrain-error:0.11573\teval-error:0.14578                                     \n",
      "[16]\ttrain-error:0.11509\teval-error:0.14322                                     \n",
      "[17]\ttrain-error:0.11445\teval-error:0.14578                                     \n",
      "[18]\ttrain-error:0.11381\teval-error:0.14578                                     \n",
      "[19]\ttrain-error:0.11125\teval-error:0.13555                                     \n",
      "[20]\ttrain-error:0.11125\teval-error:0.14322                                     \n",
      "[21]\ttrain-error:0.11189\teval-error:0.14578                                     \n",
      "[22]\ttrain-error:0.11253\teval-error:0.14578                                     \n",
      "[23]\ttrain-error:0.11189\teval-error:0.14322                                     \n",
      "[24]\ttrain-error:0.10997\teval-error:0.14322                                     \n",
      "[25]\ttrain-error:0.10870\teval-error:0.14578                                     \n",
      "[26]\ttrain-error:0.10678\teval-error:0.14578                                     \n",
      "[27]\ttrain-error:0.10678\teval-error:0.14322                                     \n",
      "[28]\ttrain-error:0.10550\teval-error:0.14322                                     \n",
      "[29]\ttrain-error:0.10550\teval-error:0.14322                                     \n",
      "[30]\ttrain-error:0.10486\teval-error:0.14322                                     \n",
      "[31]\ttrain-error:0.10230\teval-error:0.14067                                     \n",
      "[32]\ttrain-error:0.09974\teval-error:0.14322                                     \n",
      "[33]\ttrain-error:0.09783\teval-error:0.14578                                     \n",
      "[34]\ttrain-error:0.09783\teval-error:0.14067                                     \n",
      "[35]\ttrain-error:0.09655\teval-error:0.14578                                     \n",
      "[36]\ttrain-error:0.09335\teval-error:0.13811                                     \n",
      "[37]\ttrain-error:0.09335\teval-error:0.14067                                     \n",
      "[38]\ttrain-error:0.09271\teval-error:0.13555                                     \n",
      "[39]\ttrain-error:0.09271\teval-error:0.13811                                     \n",
      "[40]\ttrain-error:0.09015\teval-error:0.13555                                     \n",
      "[41]\ttrain-error:0.08951\teval-error:0.13811                                     \n",
      "[42]\ttrain-error:0.08823\teval-error:0.13811                                     \n",
      "[43]\ttrain-error:0.08696\teval-error:0.13811                                     \n",
      "[44]\ttrain-error:0.08568\teval-error:0.14067                                     \n",
      "[45]\ttrain-error:0.08632\teval-error:0.13299                                     \n",
      "[46]\ttrain-error:0.08504\teval-error:0.13299                                     \n",
      "[47]\ttrain-error:0.08568\teval-error:0.13299                                     \n",
      "[48]\ttrain-error:0.08696\teval-error:0.13555                                     \n",
      "[49]\ttrain-error:0.08184\teval-error:0.13811                                     \n",
      "[50]\ttrain-error:0.08056\teval-error:0.13811                                     \n",
      "[51]\ttrain-error:0.08120\teval-error:0.13555                                     \n",
      "[52]\ttrain-error:0.07992\teval-error:0.13299                                     \n",
      "[53]\ttrain-error:0.08120\teval-error:0.13043                                     \n",
      "[54]\ttrain-error:0.07992\teval-error:0.13043                                     \n",
      "[55]\ttrain-error:0.07992\teval-error:0.13043                                     \n",
      "[56]\ttrain-error:0.07864\teval-error:0.13043                                     \n",
      "[57]\ttrain-error:0.07481\teval-error:0.13043                                     \n",
      "[58]\ttrain-error:0.07353\teval-error:0.13043                                     \n",
      "[59]\ttrain-error:0.07289\teval-error:0.13299                                     \n",
      "[60]\ttrain-error:0.07097\teval-error:0.13299                                     \n",
      "[61]\ttrain-error:0.06969\teval-error:0.13043                                     \n",
      "[62]\ttrain-error:0.06778\teval-error:0.13299                                     \n",
      "[63]\ttrain-error:0.06650\teval-error:0.13811                                     \n",
      "[64]\ttrain-error:0.06778\teval-error:0.13811                                     \n",
      "[65]\ttrain-error:0.06905\teval-error:0.14067                                     \n",
      "[66]\ttrain-error:0.06586\teval-error:0.13811                                     \n",
      "[67]\ttrain-error:0.06714\teval-error:0.14322                                     \n",
      "[68]\ttrain-error:0.06394\teval-error:0.14322                                     \n",
      "[69]\ttrain-error:0.06330\teval-error:0.14067                                     \n",
      "[70]\ttrain-error:0.06202\teval-error:0.13811                                     \n",
      "[71]\ttrain-error:0.06202\teval-error:0.13811                                     \n",
      "[72]\ttrain-error:0.06266\teval-error:0.13555                                     \n",
      "[73]\ttrain-error:0.06074\teval-error:0.13555                                     \n",
      "[74]\ttrain-error:0.06010\teval-error:0.13811                                     \n",
      "[75]\ttrain-error:0.06074\teval-error:0.14322                                     \n",
      "[76]\ttrain-error:0.05946\teval-error:0.13043                                     \n",
      "[77]\ttrain-error:0.05818\teval-error:0.13043                                     \n",
      "[78]\ttrain-error:0.05882\teval-error:0.13043                                     \n",
      "[79]\ttrain-error:0.05882\teval-error:0.13555                                     \n",
      "[80]\ttrain-error:0.05690\teval-error:0.13299                                     \n",
      "[81]\ttrain-error:0.05627\teval-error:0.13555                                     \n",
      "[82]\ttrain-error:0.05627\teval-error:0.12788                                     \n",
      "[83]\ttrain-error:0.05754\teval-error:0.13811                                     \n",
      "[84]\ttrain-error:0.05627\teval-error:0.13555                                     \n",
      "[85]\ttrain-error:0.05690\teval-error:0.13811                                     \n",
      "[86]\ttrain-error:0.05563\teval-error:0.14067                                     \n",
      "[87]\ttrain-error:0.05627\teval-error:0.14067                                     \n",
      "[88]\ttrain-error:0.05563\teval-error:0.13811                                     \n",
      "[89]\ttrain-error:0.05563\teval-error:0.13811                                     \n",
      "[90]\ttrain-error:0.05499\teval-error:0.14067                                     \n",
      "[91]\ttrain-error:0.05371\teval-error:0.14067                                     \n",
      "[92]\ttrain-error:0.05243\teval-error:0.14067                                     \n",
      "[93]\ttrain-error:0.05115\teval-error:0.14322                                     \n",
      "[94]\ttrain-error:0.05051\teval-error:0.13299                                     \n",
      "[95]\ttrain-error:0.05115\teval-error:0.13811                                     \n",
      "[96]\ttrain-error:0.05115\teval-error:0.14067                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97]\ttrain-error:0.04923\teval-error:0.13299                                     \n",
      "[98]\ttrain-error:0.04859\teval-error:0.13555                                     \n",
      "[99]\ttrain-error:0.04795\teval-error:0.13043                                     \n",
      "[100]\ttrain-error:0.04668\teval-error:0.13555                                    \n",
      "[101]\ttrain-error:0.04668\teval-error:0.13555                                    \n",
      "[102]\ttrain-error:0.04732\teval-error:0.13555                                    \n",
      "[103]\ttrain-error:0.04668\teval-error:0.13811                                    \n",
      "[104]\ttrain-error:0.04412\teval-error:0.13811                                    \n",
      "[105]\ttrain-error:0.04412\teval-error:0.13811                                    \n",
      "[106]\ttrain-error:0.04412\teval-error:0.14067                                    \n",
      "[107]\ttrain-error:0.04156\teval-error:0.13811                                    \n",
      "[108]\ttrain-error:0.03964\teval-error:0.13811                                    \n",
      "[109]\ttrain-error:0.04028\teval-error:0.13811                                    \n",
      "[110]\ttrain-error:0.04220\teval-error:0.13555                                    \n",
      "[111]\ttrain-error:0.04284\teval-error:0.14067                                    \n",
      "[112]\ttrain-error:0.04092\teval-error:0.13811                                    \n",
      "[113]\ttrain-error:0.04156\teval-error:0.13811                                    \n",
      "[114]\ttrain-error:0.04156\teval-error:0.13299                                    \n",
      "[115]\ttrain-error:0.04156\teval-error:0.13811                                    \n",
      "[116]\ttrain-error:0.04092\teval-error:0.13811                                    \n",
      "[117]\ttrain-error:0.04028\teval-error:0.13555                                    \n",
      "[118]\ttrain-error:0.04028\teval-error:0.13555                                    \n",
      "[119]\ttrain-error:0.04028\teval-error:0.13555                                    \n",
      "[120]\ttrain-error:0.03964\teval-error:0.13555                                    \n",
      "[121]\ttrain-error:0.03772\teval-error:0.13555                                    \n",
      "[122]\ttrain-error:0.03772\teval-error:0.13811                                    \n",
      "[123]\ttrain-error:0.03708\teval-error:0.13811                                    \n",
      "[124]\ttrain-error:0.03836\teval-error:0.13555                                    \n",
      "[125]\ttrain-error:0.03772\teval-error:0.13555                                    \n",
      "[126]\ttrain-error:0.03581\teval-error:0.13555                                    \n",
      "[127]\ttrain-error:0.03261\teval-error:0.13299                                    \n",
      "[128]\ttrain-error:0.03261\teval-error:0.13299                                    \n",
      "[129]\ttrain-error:0.03453\teval-error:0.13299                                    \n",
      "[130]\ttrain-error:0.03389\teval-error:0.13299                                    \n",
      "[131]\ttrain-error:0.03325\teval-error:0.13555                                    \n",
      "[132]\ttrain-error:0.03133\teval-error:0.13555                                    \n",
      "<class 'numpy.ndarray'>                                                         \n",
      "<class 'pandas.core.series.Series'>                                             \n",
      "params: {'alpha': 6.019219240407955e-05, 'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 1.704829307081581e-07, 'lambda': 2.5825253342900034e-06, 'max_depth': 5, 'min_child_weight': 3.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, AS: 0.8645\n",
      "100%|█████████| 15/15 [00:18<00:00,  1.26s/trial, best loss: 0.8516624040920716]\n",
      "best params:{'alpha': 4.7007755374969665e-08, 'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.2303522377913218, 'lambda': 0.00023255399132147476, 'max_depth': 8, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}, score:0.8721\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# 특징과 목적변수를 xgboost의 데이터 구조로 변환\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_df_all_label_encoded)\n",
    "\n",
    "\n",
    "# 매개변수의 탐색범위\n",
    "param_space = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'error',\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 9, 1),    \n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "     # 여유가 있으면 alpha, lambda도 조정\n",
    "    'alpha' : hp.loguniform('alpha', np.log(1e-8), np.log(1.0)),\n",
    "    'lambda' : hp.loguniform('lambda', np.log(1e-6), np.log(10.0)),\n",
    "    'random_state': 71\n",
    "\n",
    "}\n",
    "\n",
    "num_round=500\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "# define classifier\n",
    "def classifier(value_list):\n",
    "    result = []\n",
    "    for value in value_list:\n",
    "        if value > 0.5:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    result = np.array(result)\n",
    "    return result\n",
    "    \n",
    "def score(params):\n",
    "    # max_depth의 형을 정수형으로 수정\n",
    "    params['max_depth'] = int(params['max_depth']) \n",
    "    model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=50)\n",
    "    # model.fit(tr_x, tr_y)\n",
    "    va_pred = model.predict(dvalid)\n",
    "    print(type(va_pred), type(va_y))\n",
    "    va_pred = classifier(va_pred)\n",
    "    score = accuracy_score(va_y, va_pred)\n",
    "    print(f'params: {params}, AS: {score:.4f}')\n",
    "\n",
    "    # 정보를 기록\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperopt에 의한 매개변수 탐색 실행\n",
    "max_evals = 15\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "# 기록한 정보에서 매개변수와 점수를 출력\n",
    "# (trials에서도 정보를 취득할 수 있지만 매개변수의 취득이 다소 어려움)\n",
    "history = sorted(history, key=lambda tpl: tpl[1], reverse=True)\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3667ee2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.15026\teval-error:0.16112\n",
      "[1]\ttrain-error:0.13811\teval-error:0.16368\n",
      "[2]\ttrain-error:0.13875\teval-error:0.15601\n",
      "[3]\ttrain-error:0.13491\teval-error:0.15345\n",
      "[4]\ttrain-error:0.12468\teval-error:0.15601\n",
      "[5]\ttrain-error:0.12596\teval-error:0.15345\n",
      "[6]\ttrain-error:0.12660\teval-error:0.15601\n",
      "[7]\ttrain-error:0.12340\teval-error:0.16880\n",
      "[8]\ttrain-error:0.11893\teval-error:0.15090\n",
      "[9]\ttrain-error:0.11637\teval-error:0.14834\n",
      "[10]\ttrain-error:0.11765\teval-error:0.15090\n",
      "[11]\ttrain-error:0.11509\teval-error:0.14834\n",
      "[12]\ttrain-error:0.11189\teval-error:0.14834\n",
      "[13]\ttrain-error:0.11381\teval-error:0.14834\n",
      "[14]\ttrain-error:0.11317\teval-error:0.14322\n",
      "[15]\ttrain-error:0.10997\teval-error:0.14322\n",
      "[16]\ttrain-error:0.11125\teval-error:0.14322\n",
      "[17]\ttrain-error:0.11253\teval-error:0.14578\n",
      "[18]\ttrain-error:0.10806\teval-error:0.14067\n",
      "[19]\ttrain-error:0.10870\teval-error:0.14322\n",
      "[20]\ttrain-error:0.10742\teval-error:0.14322\n",
      "[21]\ttrain-error:0.10486\teval-error:0.13555\n",
      "[22]\ttrain-error:0.10294\teval-error:0.13299\n",
      "[23]\ttrain-error:0.10486\teval-error:0.13811\n",
      "[24]\ttrain-error:0.09974\teval-error:0.13811\n",
      "[25]\ttrain-error:0.09591\teval-error:0.13555\n",
      "[26]\ttrain-error:0.09399\teval-error:0.13043\n",
      "[27]\ttrain-error:0.08823\teval-error:0.13043\n",
      "[28]\ttrain-error:0.08696\teval-error:0.13043\n",
      "[29]\ttrain-error:0.08760\teval-error:0.12788\n",
      "[30]\ttrain-error:0.08696\teval-error:0.13043\n",
      "[31]\ttrain-error:0.08760\teval-error:0.13299\n",
      "[32]\ttrain-error:0.08504\teval-error:0.13043\n",
      "[33]\ttrain-error:0.08184\teval-error:0.13043\n",
      "[34]\ttrain-error:0.08120\teval-error:0.13299\n",
      "[35]\ttrain-error:0.07992\teval-error:0.13299\n",
      "[36]\ttrain-error:0.07673\teval-error:0.13043\n",
      "[37]\ttrain-error:0.07289\teval-error:0.13043\n",
      "[38]\ttrain-error:0.07289\teval-error:0.12788\n",
      "[39]\ttrain-error:0.06969\teval-error:0.13043\n",
      "[40]\ttrain-error:0.07097\teval-error:0.13043\n",
      "[41]\ttrain-error:0.07161\teval-error:0.13555\n",
      "[42]\ttrain-error:0.06969\teval-error:0.13555\n",
      "[43]\ttrain-error:0.06714\teval-error:0.13811\n",
      "[44]\ttrain-error:0.06714\teval-error:0.13043\n",
      "[45]\ttrain-error:0.06458\teval-error:0.13555\n",
      "[46]\ttrain-error:0.06650\teval-error:0.13299\n",
      "[47]\ttrain-error:0.06266\teval-error:0.13043\n",
      "[48]\ttrain-error:0.06330\teval-error:0.13299\n",
      "[49]\ttrain-error:0.05818\teval-error:0.13555\n",
      "[50]\ttrain-error:0.05946\teval-error:0.13299\n",
      "[51]\ttrain-error:0.05754\teval-error:0.13299\n",
      "[52]\ttrain-error:0.05882\teval-error:0.13043\n",
      "[53]\ttrain-error:0.05946\teval-error:0.13043\n",
      "[54]\ttrain-error:0.05690\teval-error:0.13043\n",
      "[55]\ttrain-error:0.05243\teval-error:0.13043\n",
      "[56]\ttrain-error:0.05115\teval-error:0.13299\n",
      "[57]\ttrain-error:0.04987\teval-error:0.13299\n",
      "[58]\ttrain-error:0.04923\teval-error:0.13555\n",
      "[59]\ttrain-error:0.04859\teval-error:0.13043\n",
      "[60]\ttrain-error:0.04732\teval-error:0.13043\n",
      "[61]\ttrain-error:0.04795\teval-error:0.13555\n",
      "[62]\ttrain-error:0.04732\teval-error:0.14067\n",
      "[63]\ttrain-error:0.04732\teval-error:0.14067\n",
      "[64]\ttrain-error:0.04732\teval-error:0.13299\n",
      "[65]\ttrain-error:0.04795\teval-error:0.12788\n",
      "[66]\ttrain-error:0.04668\teval-error:0.13299\n",
      "[67]\ttrain-error:0.04540\teval-error:0.13811\n",
      "[68]\ttrain-error:0.04540\teval-error:0.13299\n",
      "[69]\ttrain-error:0.04412\teval-error:0.13555\n",
      "[70]\ttrain-error:0.04412\teval-error:0.13043\n",
      "[71]\ttrain-error:0.04412\teval-error:0.12788\n",
      "[72]\ttrain-error:0.04348\teval-error:0.12788\n",
      "[73]\ttrain-error:0.04284\teval-error:0.12788\n",
      "[74]\ttrain-error:0.04220\teval-error:0.13299\n",
      "[75]\ttrain-error:0.04156\teval-error:0.13043\n",
      "[76]\ttrain-error:0.04156\teval-error:0.13043\n",
      "[77]\ttrain-error:0.03964\teval-error:0.13043\n",
      "[78]\ttrain-error:0.03964\teval-error:0.12276\n",
      "[79]\ttrain-error:0.03964\teval-error:0.12532\n",
      "[80]\ttrain-error:0.03836\teval-error:0.12276\n",
      "[81]\ttrain-error:0.03900\teval-error:0.12021\n",
      "[82]\ttrain-error:0.03772\teval-error:0.12532\n",
      "[83]\ttrain-error:0.03900\teval-error:0.12276\n",
      "[84]\ttrain-error:0.03772\teval-error:0.12276\n",
      "[85]\ttrain-error:0.03836\teval-error:0.12788\n",
      "[86]\ttrain-error:0.03708\teval-error:0.12276\n",
      "[87]\ttrain-error:0.03708\teval-error:0.12788\n",
      "[88]\ttrain-error:0.03708\teval-error:0.12276\n",
      "[89]\ttrain-error:0.03644\teval-error:0.12276\n",
      "[90]\ttrain-error:0.03581\teval-error:0.12532\n",
      "[91]\ttrain-error:0.03708\teval-error:0.12532\n",
      "[92]\ttrain-error:0.03581\teval-error:0.12532\n",
      "[93]\ttrain-error:0.03517\teval-error:0.12532\n",
      "[94]\ttrain-error:0.03517\teval-error:0.12788\n",
      "[95]\ttrain-error:0.03453\teval-error:0.12788\n",
      "[96]\ttrain-error:0.03453\teval-error:0.12532\n",
      "[97]\ttrain-error:0.03325\teval-error:0.12788\n",
      "[98]\ttrain-error:0.03389\teval-error:0.12532\n",
      "[99]\ttrain-error:0.03325\teval-error:0.12532\n",
      "[100]\ttrain-error:0.03325\teval-error:0.12532\n",
      "[101]\ttrain-error:0.03325\teval-error:0.12532\n",
      "[102]\ttrain-error:0.03133\teval-error:0.12788\n",
      "[103]\ttrain-error:0.03005\teval-error:0.12276\n",
      "[104]\ttrain-error:0.02941\teval-error:0.12532\n",
      "[105]\ttrain-error:0.02749\teval-error:0.12532\n",
      "[106]\ttrain-error:0.03005\teval-error:0.12276\n",
      "[107]\ttrain-error:0.02877\teval-error:0.12021\n",
      "[108]\ttrain-error:0.02685\teval-error:0.12532\n",
      "[109]\ttrain-error:0.02685\teval-error:0.12276\n",
      "[110]\ttrain-error:0.02621\teval-error:0.12532\n",
      "[111]\ttrain-error:0.02685\teval-error:0.12276\n",
      "[112]\ttrain-error:0.02494\teval-error:0.12788\n",
      "[113]\ttrain-error:0.02494\teval-error:0.12532\n",
      "[114]\ttrain-error:0.02366\teval-error:0.12276\n",
      "[115]\ttrain-error:0.02430\teval-error:0.12276\n",
      "[116]\ttrain-error:0.02430\teval-error:0.12276\n",
      "[117]\ttrain-error:0.02302\teval-error:0.12021\n",
      "[118]\ttrain-error:0.02302\teval-error:0.12021\n",
      "[119]\ttrain-error:0.02174\teval-error:0.12021\n",
      "[120]\ttrain-error:0.02110\teval-error:0.12021\n",
      "[121]\ttrain-error:0.01982\teval-error:0.12021\n",
      "[122]\ttrain-error:0.02110\teval-error:0.12276\n",
      "[123]\ttrain-error:0.02110\teval-error:0.12276\n",
      "[124]\ttrain-error:0.02046\teval-error:0.12276\n",
      "[125]\ttrain-error:0.02046\teval-error:0.12276\n",
      "[126]\ttrain-error:0.01918\teval-error:0.12276\n",
      "[127]\ttrain-error:0.01854\teval-error:0.12276\n",
      "[128]\ttrain-error:0.01854\teval-error:0.12021\n",
      "[129]\ttrain-error:0.01726\teval-error:0.12788\n",
      "[130]\ttrain-error:0.01726\teval-error:0.12788\n",
      "[131]\ttrain-error:0.01662\teval-error:0.12788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {'alpha': 4.7007755374969665e-08, 'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.1, 'eval_metric': 'error', 'gamma': 0.2303522377913218, 'lambda': 0.00023255399132147476, 'max_depth': 8, 'min_child_weight': 4.0, 'objective': 'binary:logistic', 'random_state': 71, 'subsample': 0.8}\n",
    "\n",
    "num_round = 500\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round,\n",
    "                  evals=watchlist,\n",
    "                  early_stopping_rounds=50)\n",
    "\n",
    "# 최적의 결정 트리의 개수로 예측\n",
    "pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "pred = classifier(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a99f09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = pred\n",
    "file_name = 'xgboost_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad0a74",
   "metadata": {},
   "source": [
    "## 결측치를 제거한 상태로 GBDT를 적용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3e98a",
   "metadata": {},
   "source": [
    "## 결측치 하나씩 확인해보기\n",
    "1. 결측치가 들어있는 열을 직접 확인해보기\n",
    "2. 열마다 결측치를 처리할 적절한 방법 찾아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed843ce",
   "metadata": {},
   "source": [
    "### 1. 결측치가 들어있는 열을 직접 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14593443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17853.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age    TypeofContact  DurationOfPitch  NumberOfFollowups  \\\n",
       "id                                                                \n",
       "1     28.0  Company Invited             10.0                4.0   \n",
       "2     34.0     Self Enquiry              NaN                4.0   \n",
       "3     45.0  Company Invited              NaN                3.0   \n",
       "4     29.0  Company Invited              7.0                5.0   \n",
       "5     42.0     Self Enquiry              6.0                3.0   \n",
       "...    ...              ...              ...                ...   \n",
       "1951  28.0     Self Enquiry             10.0                5.0   \n",
       "1952  41.0     Self Enquiry              8.0                3.0   \n",
       "1953  38.0  Company Invited             28.0                4.0   \n",
       "1954  28.0     Self Enquiry             30.0                5.0   \n",
       "1955  22.0  Company Invited              9.0                4.0   \n",
       "\n",
       "      PreferredPropertyStar  NumberOfTrips  NumberOfChildrenVisiting  \\\n",
       "id                                                                     \n",
       "1                       3.0            3.0                       1.0   \n",
       "2                       4.0            1.0                       0.0   \n",
       "3                       4.0            2.0                       0.0   \n",
       "4                       4.0            3.0                       1.0   \n",
       "5                       3.0            2.0                       0.0   \n",
       "...                     ...            ...                       ...   \n",
       "1951                    3.0            2.0                       2.0   \n",
       "1952                    5.0            1.0                       1.0   \n",
       "1953                    3.0            7.0                       2.0   \n",
       "1954                    3.0            3.0                       2.0   \n",
       "1955                    3.0            1.0                       0.0   \n",
       "\n",
       "      MonthlyIncome  \n",
       "id                   \n",
       "1           20384.0  \n",
       "2           19599.0  \n",
       "3               NaN  \n",
       "4           21274.0  \n",
       "5           19907.0  \n",
       "...             ...  \n",
       "1951        20723.0  \n",
       "1952        31595.0  \n",
       "1953        21651.0  \n",
       "1954        22218.0  \n",
       "1955        17853.0  \n",
       "\n",
       "[1955 rows x 8 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = train_df.columns\n",
    "na_col = []\n",
    "no_na_col = []\n",
    "for i, num in enumerate(num_na):\n",
    "    if num != 0:\n",
    "        na_col.append(col_names[i])\n",
    "    else:\n",
    "        no_na_col.append(col_names[i])\n",
    "\n",
    "train_df[na_col]        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "683608f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1955 entries, 1 to 1955\n",
      "Data columns (total 8 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       1861 non-null   float64\n",
      " 1   TypeofContact             1945 non-null   object \n",
      " 2   DurationOfPitch           1853 non-null   float64\n",
      " 3   NumberOfFollowups         1942 non-null   float64\n",
      " 4   PreferredPropertyStar     1945 non-null   float64\n",
      " 5   NumberOfTrips             1898 non-null   float64\n",
      " 6   NumberOfChildrenVisiting  1928 non-null   float64\n",
      " 7   MonthlyIncome             1855 non-null   float64\n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 137.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df[na_col].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99c31338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i, col in enumerate(na_col, 1):\n",
    "    row = int(np.sqrt(len(na_col)))\n",
    "    plt.subplot(int(np.sqrt(len(na_col))), int(len(na_col)/row), i)\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "19b8909e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1861.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1853.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1942.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1945.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1898.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1928.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1855.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.462117</td>\n",
       "      <td>0.715601</td>\n",
       "      <td>1.641432</td>\n",
       "      <td>15.524015</td>\n",
       "      <td>2.338107</td>\n",
       "      <td>1.588747</td>\n",
       "      <td>2.922762</td>\n",
       "      <td>3.718332</td>\n",
       "      <td>1.172890</td>\n",
       "      <td>3.568638</td>\n",
       "      <td>1.275192</td>\n",
       "      <td>3.255532</td>\n",
       "      <td>0.291049</td>\n",
       "      <td>3.067519</td>\n",
       "      <td>0.619437</td>\n",
       "      <td>1.213174</td>\n",
       "      <td>1.731458</td>\n",
       "      <td>23624.108895</td>\n",
       "      <td>0.195908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.189948</td>\n",
       "      <td>0.462445</td>\n",
       "      <td>0.908744</td>\n",
       "      <td>8.150057</td>\n",
       "      <td>0.627545</td>\n",
       "      <td>0.547326</td>\n",
       "      <td>0.712276</td>\n",
       "      <td>1.004095</td>\n",
       "      <td>1.270394</td>\n",
       "      <td>0.793196</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>1.814698</td>\n",
       "      <td>0.454362</td>\n",
       "      <td>1.372915</td>\n",
       "      <td>0.485649</td>\n",
       "      <td>0.859450</td>\n",
       "      <td>0.954978</td>\n",
       "      <td>5640.482258</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22295.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25558.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>98678.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  TypeofContact     CityTier  DurationOfPitch   Occupation  \\\n",
       "count  1861.000000    1955.000000  1955.000000      1853.000000  1955.000000   \n",
       "mean     37.462117       0.715601     1.641432        15.524015     2.338107   \n",
       "std       9.189948       0.462445     0.908744         8.150057     0.627545   \n",
       "min      18.000000       0.000000     1.000000         5.000000     0.000000   \n",
       "25%      31.000000       0.000000     1.000000         9.000000     2.000000   \n",
       "50%      36.000000       1.000000     1.000000        14.000000     2.000000   \n",
       "75%      43.000000       1.000000     3.000000        20.000000     3.000000   \n",
       "max      61.000000       2.000000     3.000000        36.000000     3.000000   \n",
       "\n",
       "            Gender  NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "count  1955.000000             1955.000000        1942.000000     1955.000000   \n",
       "mean      1.588747                2.922762           3.718332        1.172890   \n",
       "std       0.547326                0.712276           1.004095        1.270394   \n",
       "min       0.000000                1.000000           1.000000        0.000000   \n",
       "25%       1.000000                2.000000           3.000000        0.000000   \n",
       "50%       2.000000                3.000000           4.000000        1.000000   \n",
       "75%       2.000000                3.000000           4.000000        2.000000   \n",
       "max       2.000000                5.000000           6.000000        4.000000   \n",
       "\n",
       "       PreferredPropertyStar  MaritalStatus  NumberOfTrips     Passport  \\\n",
       "count            1945.000000    1955.000000    1898.000000  1955.000000   \n",
       "mean                3.568638       1.275192       3.255532     0.291049   \n",
       "std                 0.793196       0.933826       1.814698     0.454362   \n",
       "min                 3.000000       0.000000       1.000000     0.000000   \n",
       "25%                 3.000000       1.000000       2.000000     0.000000   \n",
       "50%                 3.000000       1.000000       3.000000     0.000000   \n",
       "75%                 4.000000       2.000000       4.000000     1.000000   \n",
       "max                 5.000000       3.000000      19.000000     1.000000   \n",
       "\n",
       "       PitchSatisfactionScore       OwnCar  NumberOfChildrenVisiting  \\\n",
       "count             1955.000000  1955.000000               1928.000000   \n",
       "mean                 3.067519     0.619437                  1.213174   \n",
       "std                  1.372915     0.485649                  0.859450   \n",
       "min                  1.000000     0.000000                  0.000000   \n",
       "25%                  2.000000     0.000000                  1.000000   \n",
       "50%                  3.000000     1.000000                  1.000000   \n",
       "75%                  4.000000     1.000000                  2.000000   \n",
       "max                  5.000000     1.000000                  3.000000   \n",
       "\n",
       "       Designation  MonthlyIncome    ProdTaken  \n",
       "count  1955.000000    1855.000000  1955.000000  \n",
       "mean      1.731458   23624.108895     0.195908  \n",
       "std       0.954978    5640.482258     0.397000  \n",
       "min       0.000000    1000.000000     0.000000  \n",
       "25%       1.000000   20390.000000     0.000000  \n",
       "50%       2.000000   22295.000000     0.000000  \n",
       "75%       2.000000   25558.000000     0.000000  \n",
       "max       4.000000   98678.000000     1.000000  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_all_label_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "59dbed5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>20384.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19599.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>18934.078264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21274.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19907.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>24857.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20675.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20980.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19639.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21364.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age    TypeofContact  CityTier  DurationOfPitch      Occupation  Gender  \\\n",
       "id                                                                             \n",
       "1   28.0  Company Invited         1             10.0  Small Business    Male   \n",
       "2   34.0     Self Enquiry         3             14.0  Small Business  Female   \n",
       "3   45.0  Company Invited         1             14.0        Salaried    Male   \n",
       "4   29.0  Company Invited         1              7.0  Small Business    Male   \n",
       "5   42.0     Self Enquiry         3              6.0        Salaried    Male   \n",
       "6   32.0     Self Enquiry         1             29.0  Small Business    Male   \n",
       "7   43.0  Company Invited         3              8.0        Salaried    Male   \n",
       "8   32.0     Self Enquiry         3             20.0  Small Business    Male   \n",
       "9   36.0  Company Invited         3             14.0  Small Business  Female   \n",
       "10  34.0     Self Enquiry         1              7.0        Salaried    Male   \n",
       "\n",
       "    NumberOfPersonVisiting  NumberOfFollowups ProductPitched  \\\n",
       "id                                                             \n",
       "1                        3                4.0          Basic   \n",
       "2                        2                4.0         Deluxe   \n",
       "3                        2                3.0         Deluxe   \n",
       "4                        3                5.0          Basic   \n",
       "5                        2                3.0         Deluxe   \n",
       "6                        4                4.0         Deluxe   \n",
       "7                        3                3.0         Deluxe   \n",
       "8                        4                5.0         Deluxe   \n",
       "9                        2                1.0         Deluxe   \n",
       "10                       4                4.0          Basic   \n",
       "\n",
       "    PreferredPropertyStar MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                 \n",
       "1                     3.0       Married            3.0         0   \n",
       "2                     4.0        Single            1.0         1   \n",
       "3                     4.0       Married            2.0         0   \n",
       "4                     4.0       Married            3.0         0   \n",
       "5                     3.0      Divorced            2.0         0   \n",
       "6                     3.0      Divorced            3.0         1   \n",
       "7                     3.0       Married            2.0         0   \n",
       "8                     5.0       Married            7.0         1   \n",
       "9                     5.0      Divorced            3.0         0   \n",
       "10                    3.0     Unmarried            3.0         1   \n",
       "\n",
       "    PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting Designation  \\\n",
       "id                                                                         \n",
       "1                        1       0                       1.0   Executive   \n",
       "2                        5       1                       0.0     Manager   \n",
       "3                        4       1                       0.0     Manager   \n",
       "4                        4       0                       1.0   Executive   \n",
       "5                        3       1                       0.0     Manager   \n",
       "6                        5       1                       1.0     Manager   \n",
       "7                        3       1                       2.0     Manager   \n",
       "8                        1       1                       1.0     Manager   \n",
       "9                        1       1                       0.0     Manager   \n",
       "10                       3       1                       1.0   Executive   \n",
       "\n",
       "    MonthlyIncome  ProdTaken  \n",
       "id                            \n",
       "1    20384.000000          0  \n",
       "2    19599.000000          1  \n",
       "3    18934.078264          0  \n",
       "4    21274.000000          1  \n",
       "5    19907.000000          0  \n",
       "6    24857.000000          1  \n",
       "7    20675.000000          0  \n",
       "8    20980.000000          1  \n",
       "9    19639.000000          0  \n",
       "10   21364.000000          1  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6c0f6c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003934</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.025779</td>\n",
       "      <td>0.007857</td>\n",
       "      <td>-0.017126</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>0.482137</td>\n",
       "      <td>-0.026789</td>\n",
       "      <td>-0.095530</td>\n",
       "      <td>0.178143</td>\n",
       "      <td>0.030162</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.060298</td>\n",
       "      <td>0.039495</td>\n",
       "      <td>0.204869</td>\n",
       "      <td>0.440733</td>\n",
       "      <td>-0.135832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TypeofContact</th>\n",
       "      <td>0.003934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006532</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>-0.011438</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>-0.061558</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.036966</td>\n",
       "      <td>-0.042286</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>-0.010283</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>-0.047598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CityTier</th>\n",
       "      <td>0.007875</td>\n",
       "      <td>-0.006532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>0.159750</td>\n",
       "      <td>-0.042483</td>\n",
       "      <td>0.018071</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.160561</td>\n",
       "      <td>-0.011882</td>\n",
       "      <td>0.016228</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>-0.028168</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.141385</td>\n",
       "      <td>0.057705</td>\n",
       "      <td>0.085583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <td>0.025779</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.056010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057220</td>\n",
       "      <td>-0.007149</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>0.047093</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>-0.015087</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>-0.008016</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>0.072899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occupation</th>\n",
       "      <td>0.007857</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.159750</td>\n",
       "      <td>0.057220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009181</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>-0.019433</td>\n",
       "      <td>0.031275</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>-0.042706</td>\n",
       "      <td>-0.033575</td>\n",
       "      <td>-0.006072</td>\n",
       "      <td>-0.032450</td>\n",
       "      <td>-0.025941</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>-0.042101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>-0.017126</td>\n",
       "      <td>-0.011438</td>\n",
       "      <td>-0.042483</td>\n",
       "      <td>-0.007149</td>\n",
       "      <td>-0.009181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>-0.045788</td>\n",
       "      <td>-0.072863</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>-0.118900</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>-0.022631</td>\n",
       "      <td>0.015859</td>\n",
       "      <td>-0.042304</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>-0.068448</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>0.022398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <td>0.010795</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>0.018071</td>\n",
       "      <td>0.096268</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333738</td>\n",
       "      <td>-0.054800</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.214895</td>\n",
       "      <td>0.023638</td>\n",
       "      <td>-0.012981</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.610193</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>0.006483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <td>0.009834</td>\n",
       "      <td>-0.061558</td>\n",
       "      <td>0.023532</td>\n",
       "      <td>0.039485</td>\n",
       "      <td>-0.019433</td>\n",
       "      <td>-0.045788</td>\n",
       "      <td>0.333738</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026845</td>\n",
       "      <td>-0.049151</td>\n",
       "      <td>0.089118</td>\n",
       "      <td>0.135183</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.293942</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.194668</td>\n",
       "      <td>0.105038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProductPitched</th>\n",
       "      <td>0.482137</td>\n",
       "      <td>0.013177</td>\n",
       "      <td>0.160561</td>\n",
       "      <td>0.047093</td>\n",
       "      <td>0.031275</td>\n",
       "      <td>-0.072863</td>\n",
       "      <td>-0.054800</td>\n",
       "      <td>0.026845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>-0.059970</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>-0.022497</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>0.086790</td>\n",
       "      <td>-0.027598</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>0.648318</td>\n",
       "      <td>-0.150399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <td>-0.026789</td>\n",
       "      <td>0.035075</td>\n",
       "      <td>-0.011882</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>0.017057</td>\n",
       "      <td>-0.049151</td>\n",
       "      <td>-0.006162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004805</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>-0.019620</td>\n",
       "      <td>0.031355</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>-0.024338</td>\n",
       "      <td>0.114923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalStatus</th>\n",
       "      <td>-0.095530</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>0.016228</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>-0.042706</td>\n",
       "      <td>-0.118900</td>\n",
       "      <td>0.032742</td>\n",
       "      <td>0.089118</td>\n",
       "      <td>-0.059970</td>\n",
       "      <td>-0.004805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>-0.007714</td>\n",
       "      <td>-0.019474</td>\n",
       "      <td>0.046179</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>0.169245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <td>0.178143</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.020887</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>-0.033575</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.214895</td>\n",
       "      <td>0.135183</td>\n",
       "      <td>0.067993</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.189517</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>0.137093</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Passport</th>\n",
       "      <td>0.030162</td>\n",
       "      <td>-0.036966</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-0.006072</td>\n",
       "      <td>-0.022631</td>\n",
       "      <td>0.023638</td>\n",
       "      <td>-0.005332</td>\n",
       "      <td>-0.022497</td>\n",
       "      <td>0.014701</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.004418</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>-0.045133</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>-0.028542</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>0.293726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <td>0.032860</td>\n",
       "      <td>-0.042286</td>\n",
       "      <td>-0.028168</td>\n",
       "      <td>0.011926</td>\n",
       "      <td>-0.032450</td>\n",
       "      <td>0.015859</td>\n",
       "      <td>-0.012981</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>-0.019620</td>\n",
       "      <td>-0.007714</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073097</td>\n",
       "      <td>0.023842</td>\n",
       "      <td>-0.040420</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.067736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OwnCar</th>\n",
       "      <td>0.060298</td>\n",
       "      <td>0.016879</td>\n",
       "      <td>0.014177</td>\n",
       "      <td>-0.015087</td>\n",
       "      <td>-0.025941</td>\n",
       "      <td>-0.042304</td>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.051920</td>\n",
       "      <td>0.086790</td>\n",
       "      <td>0.031355</td>\n",
       "      <td>-0.019474</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>-0.045133</td>\n",
       "      <td>0.073097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036416</td>\n",
       "      <td>0.043261</td>\n",
       "      <td>0.109662</td>\n",
       "      <td>-0.040465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <td>0.039495</td>\n",
       "      <td>-0.010283</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.047770</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.610193</td>\n",
       "      <td>0.293942</td>\n",
       "      <td>-0.027598</td>\n",
       "      <td>0.027038</td>\n",
       "      <td>0.046179</td>\n",
       "      <td>0.189517</td>\n",
       "      <td>0.030512</td>\n",
       "      <td>0.023842</td>\n",
       "      <td>0.036416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>0.179255</td>\n",
       "      <td>0.006060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Designation</th>\n",
       "      <td>0.204869</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.141385</td>\n",
       "      <td>-0.008016</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>-0.068448</td>\n",
       "      <td>-0.007937</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.353400</td>\n",
       "      <td>-0.018082</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>-0.028542</td>\n",
       "      <td>-0.040420</td>\n",
       "      <td>0.043261</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.331735</td>\n",
       "      <td>-0.096041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <td>0.440733</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.057705</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>-0.043840</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>0.194668</td>\n",
       "      <td>0.648318</td>\n",
       "      <td>-0.024338</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>0.137093</td>\n",
       "      <td>0.017044</td>\n",
       "      <td>-0.005497</td>\n",
       "      <td>0.109662</td>\n",
       "      <td>0.179255</td>\n",
       "      <td>0.331735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.140617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProdTaken</th>\n",
       "      <td>-0.135832</td>\n",
       "      <td>-0.047598</td>\n",
       "      <td>0.085583</td>\n",
       "      <td>0.072899</td>\n",
       "      <td>-0.042101</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.105038</td>\n",
       "      <td>-0.150399</td>\n",
       "      <td>0.114923</td>\n",
       "      <td>0.169245</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.293726</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>-0.040465</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>-0.096041</td>\n",
       "      <td>-0.140617</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Age  TypeofContact  CityTier  DurationOfPitch  \\\n",
       "Age                       1.000000       0.003934  0.007875         0.025779   \n",
       "TypeofContact             0.003934       1.000000 -0.006532         0.001507   \n",
       "CityTier                  0.007875      -0.006532  1.000000         0.056010   \n",
       "DurationOfPitch           0.025779       0.001507  0.056010         1.000000   \n",
       "Occupation                0.007857       0.017613  0.159750         0.057220   \n",
       "Gender                   -0.017126      -0.011438 -0.042483        -0.007149   \n",
       "NumberOfPersonVisiting    0.010795      -0.004574  0.018071         0.096268   \n",
       "NumberOfFollowups         0.009834      -0.061558  0.023532         0.039485   \n",
       "ProductPitched            0.482137       0.013177  0.160561         0.047093   \n",
       "PreferredPropertyStar    -0.026789       0.035075 -0.011882        -0.004448   \n",
       "MaritalStatus            -0.095530      -0.043840  0.016228         0.011337   \n",
       "NumberOfTrips             0.178143      -0.000329 -0.020887         0.022236   \n",
       "Passport                  0.030162      -0.036966  0.013665         0.043478   \n",
       "PitchSatisfactionScore    0.032860      -0.042286 -0.028168         0.011926   \n",
       "OwnCar                    0.060298       0.016879  0.014177        -0.015087   \n",
       "NumberOfChildrenVisiting  0.039495      -0.010283  0.025359         0.047770   \n",
       "Designation               0.204869       0.025136  0.141385        -0.008016   \n",
       "MonthlyIncome             0.440733       0.039107  0.057705         0.016011   \n",
       "ProdTaken                -0.135832      -0.047598  0.085583         0.072899   \n",
       "\n",
       "                          Occupation    Gender  NumberOfPersonVisiting  \\\n",
       "Age                         0.007857 -0.017126                0.010795   \n",
       "TypeofContact               0.017613 -0.011438               -0.004574   \n",
       "CityTier                    0.159750 -0.042483                0.018071   \n",
       "DurationOfPitch             0.057220 -0.007149                0.096268   \n",
       "Occupation                  1.000000 -0.009181                0.004642   \n",
       "Gender                     -0.009181  1.000000                0.001182   \n",
       "NumberOfPersonVisiting      0.004642  0.001182                1.000000   \n",
       "NumberOfFollowups          -0.019433 -0.045788                0.333738   \n",
       "ProductPitched              0.031275 -0.072863               -0.054800   \n",
       "PreferredPropertyStar       0.020325  0.005142                0.017057   \n",
       "MaritalStatus              -0.042706 -0.118900                0.032742   \n",
       "NumberOfTrips              -0.033575  0.026092                0.214895   \n",
       "Passport                   -0.006072 -0.022631                0.023638   \n",
       "PitchSatisfactionScore     -0.032450  0.015859               -0.012981   \n",
       "OwnCar                     -0.025941 -0.042304                0.018545   \n",
       "NumberOfChildrenVisiting    0.002279  0.025800                0.610193   \n",
       "Designation                 0.000432 -0.068448               -0.007937   \n",
       "MonthlyIncome               0.005325 -0.043840                0.168701   \n",
       "ProdTaken                  -0.042101  0.022398                0.006483   \n",
       "\n",
       "                          NumberOfFollowups  ProductPitched  \\\n",
       "Age                                0.009834        0.482137   \n",
       "TypeofContact                     -0.061558        0.013177   \n",
       "CityTier                           0.023532        0.160561   \n",
       "DurationOfPitch                    0.039485        0.047093   \n",
       "Occupation                        -0.019433        0.031275   \n",
       "Gender                            -0.045788       -0.072863   \n",
       "NumberOfPersonVisiting             0.333738       -0.054800   \n",
       "NumberOfFollowups                  1.000000        0.026845   \n",
       "ProductPitched                     0.026845        1.000000   \n",
       "PreferredPropertyStar             -0.049151       -0.006162   \n",
       "MaritalStatus                      0.089118       -0.059970   \n",
       "NumberOfTrips                      0.135183        0.067993   \n",
       "Passport                          -0.005332       -0.022497   \n",
       "PitchSatisfactionScore            -0.007195       -0.010217   \n",
       "OwnCar                             0.051920        0.086790   \n",
       "NumberOfChildrenVisiting           0.293942       -0.027598   \n",
       "Designation                        0.057100        0.353400   \n",
       "MonthlyIncome                      0.194668        0.648318   \n",
       "ProdTaken                          0.105038       -0.150399   \n",
       "\n",
       "                          PreferredPropertyStar  MaritalStatus  NumberOfTrips  \\\n",
       "Age                                   -0.026789      -0.095530       0.178143   \n",
       "TypeofContact                          0.035075      -0.043840      -0.000329   \n",
       "CityTier                              -0.011882       0.016228      -0.020887   \n",
       "DurationOfPitch                       -0.004448       0.011337       0.022236   \n",
       "Occupation                             0.020325      -0.042706      -0.033575   \n",
       "Gender                                 0.005142      -0.118900       0.026092   \n",
       "NumberOfPersonVisiting                 0.017057       0.032742       0.214895   \n",
       "NumberOfFollowups                     -0.049151       0.089118       0.135183   \n",
       "ProductPitched                        -0.006162      -0.059970       0.067993   \n",
       "PreferredPropertyStar                  1.000000      -0.004805       0.035064   \n",
       "MaritalStatus                         -0.004805       1.000000       0.000847   \n",
       "NumberOfTrips                          0.035064       0.000847       1.000000   \n",
       "Passport                               0.014701       0.019800       0.004418   \n",
       "PitchSatisfactionScore                -0.019620      -0.007714       0.034816   \n",
       "OwnCar                                 0.031355      -0.019474       0.005982   \n",
       "NumberOfChildrenVisiting               0.027038       0.046179       0.189517   \n",
       "Designation                           -0.018082       0.004290       0.019940   \n",
       "MonthlyIncome                         -0.024338      -0.083679       0.137093   \n",
       "ProdTaken                              0.114923       0.169245       0.044922   \n",
       "\n",
       "                          Passport  PitchSatisfactionScore    OwnCar  \\\n",
       "Age                       0.030162                0.032860  0.060298   \n",
       "TypeofContact            -0.036966               -0.042286  0.016879   \n",
       "CityTier                  0.013665               -0.028168  0.014177   \n",
       "DurationOfPitch           0.043478                0.011926 -0.015087   \n",
       "Occupation               -0.006072               -0.032450 -0.025941   \n",
       "Gender                   -0.022631                0.015859 -0.042304   \n",
       "NumberOfPersonVisiting    0.023638               -0.012981  0.018545   \n",
       "NumberOfFollowups        -0.005332               -0.007195  0.051920   \n",
       "ProductPitched           -0.022497               -0.010217  0.086790   \n",
       "PreferredPropertyStar     0.014701               -0.019620  0.031355   \n",
       "MaritalStatus             0.019800               -0.007714 -0.019474   \n",
       "NumberOfTrips             0.004418                0.034816  0.005982   \n",
       "Passport                  1.000000                0.018526 -0.045133   \n",
       "PitchSatisfactionScore    0.018526                1.000000  0.073097   \n",
       "OwnCar                   -0.045133                0.073097  1.000000   \n",
       "NumberOfChildrenVisiting  0.030512                0.023842  0.036416   \n",
       "Designation              -0.028542               -0.040420  0.043261   \n",
       "MonthlyIncome             0.017044               -0.005497  0.109662   \n",
       "ProdTaken                 0.293726                0.067736 -0.040465   \n",
       "\n",
       "                          NumberOfChildrenVisiting  Designation  \\\n",
       "Age                                       0.039495     0.204869   \n",
       "TypeofContact                            -0.010283     0.025136   \n",
       "CityTier                                  0.025359     0.141385   \n",
       "DurationOfPitch                           0.047770    -0.008016   \n",
       "Occupation                                0.002279     0.000432   \n",
       "Gender                                    0.025800    -0.068448   \n",
       "NumberOfPersonVisiting                    0.610193    -0.007937   \n",
       "NumberOfFollowups                         0.293942     0.057100   \n",
       "ProductPitched                           -0.027598     0.353400   \n",
       "PreferredPropertyStar                     0.027038    -0.018082   \n",
       "MaritalStatus                             0.046179     0.004290   \n",
       "NumberOfTrips                             0.189517     0.019940   \n",
       "Passport                                  0.030512    -0.028542   \n",
       "PitchSatisfactionScore                    0.023842    -0.040420   \n",
       "OwnCar                                    0.036416     0.043261   \n",
       "NumberOfChildrenVisiting                  1.000000     0.017814   \n",
       "Designation                               0.017814     1.000000   \n",
       "MonthlyIncome                             0.179255     0.331735   \n",
       "ProdTaken                                 0.006060    -0.096041   \n",
       "\n",
       "                          MonthlyIncome  ProdTaken  \n",
       "Age                            0.440733  -0.135832  \n",
       "TypeofContact                  0.039107  -0.047598  \n",
       "CityTier                       0.057705   0.085583  \n",
       "DurationOfPitch                0.016011   0.072899  \n",
       "Occupation                     0.005325  -0.042101  \n",
       "Gender                        -0.043840   0.022398  \n",
       "NumberOfPersonVisiting         0.168701   0.006483  \n",
       "NumberOfFollowups              0.194668   0.105038  \n",
       "ProductPitched                 0.648318  -0.150399  \n",
       "PreferredPropertyStar         -0.024338   0.114923  \n",
       "MaritalStatus                 -0.083679   0.169245  \n",
       "NumberOfTrips                  0.137093   0.044922  \n",
       "Passport                       0.017044   0.293726  \n",
       "PitchSatisfactionScore        -0.005497   0.067736  \n",
       "OwnCar                         0.109662  -0.040465  \n",
       "NumberOfChildrenVisiting       0.179255   0.006060  \n",
       "Designation                    0.331735  -0.096041  \n",
       "MonthlyIncome                  1.000000  -0.140617  \n",
       "ProdTaken                     -0.140617   1.000000  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_all_label_encoded.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41165b",
   "metadata": {},
   "source": [
    "### 2. 열마다 결측치를 처리할 적절한 방법 찾아보기\n",
    "1. 여행상품을 신청하는 여부를 결정하는데 중요하게 작용하는 특징 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bbc35aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_all_label_encoded = pd.DataFrame()\n",
    "# transform all object columns with labelencoder\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        le.fit(train_df[col])\n",
    "        train_df_all_label_encoded[col] = le.transform(train_df[col])\n",
    "    else:\n",
    "        train_df_all_label_encoded[col] = train_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e24dadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20384.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19599.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21274.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19907.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20723.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31595.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21651.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22218.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17853.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  TypeofContact  CityTier  DurationOfPitch  Occupation  Gender  \\\n",
       "id                                                                         \n",
       "1     28.0              0         1             10.0           3       2   \n",
       "2     34.0              1         3              NaN           3       1   \n",
       "3     45.0              0         1              NaN           2       2   \n",
       "4     29.0              0         1              7.0           3       2   \n",
       "5     42.0              1         3              6.0           2       2   \n",
       "...    ...            ...       ...              ...         ...     ...   \n",
       "1951  28.0              1         1             10.0           3       2   \n",
       "1952  41.0              1         3              8.0           2       1   \n",
       "1953  38.0              0         3             28.0           3       1   \n",
       "1954  28.0              1         3             30.0           3       1   \n",
       "1955  22.0              0         1              9.0           2       2   \n",
       "\n",
       "      NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "id                                                                \n",
       "1                          3                4.0               0   \n",
       "2                          2                4.0               1   \n",
       "3                          2                3.0               1   \n",
       "4                          3                5.0               0   \n",
       "5                          2                3.0               1   \n",
       "...                      ...                ...             ...   \n",
       "1951                       3                5.0               0   \n",
       "1952                       3                3.0               4   \n",
       "1953                       3                4.0               0   \n",
       "1954                       3                5.0               1   \n",
       "1955                       2                4.0               0   \n",
       "\n",
       "      PreferredPropertyStar  MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                    \n",
       "1                       3.0              1            3.0         0   \n",
       "2                       4.0              2            1.0         1   \n",
       "3                       4.0              1            2.0         0   \n",
       "4                       4.0              1            3.0         0   \n",
       "5                       3.0              0            2.0         0   \n",
       "...                     ...            ...            ...       ...   \n",
       "1951                    3.0              2            2.0         0   \n",
       "1952                    5.0              0            1.0         0   \n",
       "1953                    3.0              0            7.0         0   \n",
       "1954                    3.0              1            3.0         0   \n",
       "1955                    3.0              0            1.0         1   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting  Designation  \\\n",
       "id                                                                            \n",
       "1                          1       0                       1.0            1   \n",
       "2                          5       1                       0.0            2   \n",
       "3                          4       1                       0.0            2   \n",
       "4                          4       0                       1.0            1   \n",
       "5                          3       1                       0.0            2   \n",
       "...                      ...     ...                       ...          ...   \n",
       "1951                       1       1                       2.0            1   \n",
       "1952                       5       1                       1.0            0   \n",
       "1953                       2       1                       2.0            1   \n",
       "1954                       1       1                       2.0            2   \n",
       "1955                       3       0                       0.0            1   \n",
       "\n",
       "      MonthlyIncome  ProdTaken  \n",
       "id                              \n",
       "1           20384.0          0  \n",
       "2           19599.0          1  \n",
       "3               NaN          0  \n",
       "4           21274.0          1  \n",
       "5           19907.0          0  \n",
       "...             ...        ...  \n",
       "1951        20723.0          0  \n",
       "1952        31595.0          0  \n",
       "1953        21651.0          0  \n",
       "1954        22218.0          0  \n",
       "1955        17853.0          1  \n",
       "\n",
       "[1955 rows x 19 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_all_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "541deb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5180</td>\n",
       "      <td>MonthlyIncome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4222</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4014</td>\n",
       "      <td>DurationOfPitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1542</td>\n",
       "      <td>NumberOfTrips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1394</td>\n",
       "      <td>PitchSatisfactionScore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1087</td>\n",
       "      <td>PreferredPropertyStar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1004</td>\n",
       "      <td>MaritalStatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>917</td>\n",
       "      <td>NumberOfFollowups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>909</td>\n",
       "      <td>CityTier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>804</td>\n",
       "      <td>Passport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798</td>\n",
       "      <td>Occupation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>703</td>\n",
       "      <td>ProductPitched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>670</td>\n",
       "      <td>Gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>626</td>\n",
       "      <td>NumberOfPersonVisiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>594</td>\n",
       "      <td>NumberOfChildrenVisiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>525</td>\n",
       "      <td>TypeofContact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>524</td>\n",
       "      <td>OwnCar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>399</td>\n",
       "      <td>Designation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    importance                    column\n",
       "17        5180             MonthlyIncome\n",
       "0         4222                       Age\n",
       "3         4014           DurationOfPitch\n",
       "11        1542             NumberOfTrips\n",
       "13        1394    PitchSatisfactionScore\n",
       "9         1087     PreferredPropertyStar\n",
       "10        1004             MaritalStatus\n",
       "7          917         NumberOfFollowups\n",
       "2          909                  CityTier\n",
       "12         804                  Passport\n",
       "4          798                Occupation\n",
       "8          703            ProductPitched\n",
       "5          670                    Gender\n",
       "6          626    NumberOfPersonVisiting\n",
       "15         594  NumberOfChildrenVisiting\n",
       "1          525             TypeofContact\n",
       "14         524                    OwnCar\n",
       "16         399               Designation"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_values = pd.DataFrame(lightgbm_model.feature_importances_)\n",
    "feature_columns = pd.DataFrame(train_x.columns)\n",
    "feature_im = pd.concat([feature_values,feature_columns],axis=1)\n",
    "feature_im.columns=['importance','column']\n",
    "feature_im=feature_im.sort_values('importance',ascending=False)\n",
    "feature_im # 랜덤포레스트 모델에서 특징중요도로 선택한 특징들을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2809b",
   "metadata": {},
   "source": [
    "2. monthlyincome, age, numberoftrips는 다른 변수로부터 결측값을 예측함 - LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "c68c74af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['MonthlyIncome', 'Age', 'NumberOfTrips']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMRegressor(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165185c",
   "metadata": {},
   "source": [
    "3. numberoffollowups, PreferredPropertyStar, NumberOfChildrenVisiting 는 다른 변수로 부터 값을 예측함 - LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "375b5243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n",
      "(0, 19)\n",
      "(0, 19)\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[train_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cafab",
   "metadata": {},
   "source": [
    "4. 'TypeofContact'는 다른 변수로 부터 값을 예측함 - LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "cb6ad2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 19)\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['TypeofContact']\n",
    "for col in predict_na_col:\n",
    "    na_idx = train_df[train_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = train_df[list(map(lambda x:not x, train_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    train_y = train_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    train_x = train_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(train_x, train_y)\n",
    "    feature_pred = feature_model.predict(train_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    back2col = lambda x: 'Company Invited' if x==0 else 'Self Enquiry'\n",
    "    feature_pred = list(map(back2col, feature_pred))\n",
    "    train_df.loc[train_df[train_df[col].isna()].index,col] = feature_pred\n",
    "    print(train_df[train_df[col].isna()].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a608f167",
   "metadata": {},
   "source": [
    "5. durationofpitch는 중앙값을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "93123758",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_val = np.median(train_df[train_df['DurationOfPitch'].isna().apply(lambda x:not x)]['DurationOfPitch'])\n",
    "train_df.loc[train_df[train_df['DurationOfPitch'].isna()].index,'DurationOfPitch'] = med_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e028e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "TypeofContact               0\n",
       "CityTier                    0\n",
       "DurationOfPitch             0\n",
       "Occupation                  0\n",
       "Gender                      0\n",
       "NumberOfPersonVisiting      0\n",
       "NumberOfFollowups           0\n",
       "ProductPitched              0\n",
       "PreferredPropertyStar       0\n",
       "MaritalStatus               0\n",
       "NumberOfTrips               0\n",
       "Passport                    0\n",
       "PitchSatisfactionScore      0\n",
       "OwnCar                      0\n",
       "NumberOfChildrenVisiting    0\n",
       "Designation                 0\n",
       "MonthlyIncome               0\n",
       "ProdTaken                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()\n",
    "# 모든 결측치 값 채워넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "4ff1c5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>20384.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19599.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>18934.078264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21274.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19907.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>20723.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Super Deluxe</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AVP</td>\n",
       "      <td>31595.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21651.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>22218.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>17853.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age    TypeofContact  CityTier  DurationOfPitch      Occupation  \\\n",
       "id                                                                       \n",
       "1     28.0  Company Invited         1             10.0  Small Business   \n",
       "2     34.0     Self Enquiry         3             14.0  Small Business   \n",
       "3     45.0  Company Invited         1             14.0        Salaried   \n",
       "4     29.0  Company Invited         1              7.0  Small Business   \n",
       "5     42.0     Self Enquiry         3              6.0        Salaried   \n",
       "...    ...              ...       ...              ...             ...   \n",
       "1951  28.0     Self Enquiry         1             10.0  Small Business   \n",
       "1952  41.0     Self Enquiry         3              8.0        Salaried   \n",
       "1953  38.0  Company Invited         3             28.0  Small Business   \n",
       "1954  28.0     Self Enquiry         3             30.0  Small Business   \n",
       "1955  22.0  Company Invited         1              9.0        Salaried   \n",
       "\n",
       "      Gender  NumberOfPersonVisiting  NumberOfFollowups ProductPitched  \\\n",
       "id                                                                       \n",
       "1       Male                       3                4.0          Basic   \n",
       "2     Female                       2                4.0         Deluxe   \n",
       "3       Male                       2                3.0         Deluxe   \n",
       "4       Male                       3                5.0          Basic   \n",
       "5       Male                       2                3.0         Deluxe   \n",
       "...      ...                     ...                ...            ...   \n",
       "1951    Male                       3                5.0          Basic   \n",
       "1952  Female                       3                3.0   Super Deluxe   \n",
       "1953  Female                       3                4.0          Basic   \n",
       "1954  Female                       3                5.0         Deluxe   \n",
       "1955    Male                       2                4.0          Basic   \n",
       "\n",
       "      PreferredPropertyStar MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                   \n",
       "1                       3.0       Married            3.0         0   \n",
       "2                       4.0        Single            1.0         1   \n",
       "3                       4.0       Married            2.0         0   \n",
       "4                       4.0       Married            3.0         0   \n",
       "5                       3.0      Divorced            2.0         0   \n",
       "...                     ...           ...            ...       ...   \n",
       "1951                    3.0        Single            2.0         0   \n",
       "1952                    5.0      Divorced            1.0         0   \n",
       "1953                    3.0      Divorced            7.0         0   \n",
       "1954                    3.0       Married            3.0         0   \n",
       "1955                    3.0      Divorced            1.0         1   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting Designation  \\\n",
       "id                                                                           \n",
       "1                          1       0                       1.0   Executive   \n",
       "2                          5       1                       0.0     Manager   \n",
       "3                          4       1                       0.0     Manager   \n",
       "4                          4       0                       1.0   Executive   \n",
       "5                          3       1                       0.0     Manager   \n",
       "...                      ...     ...                       ...         ...   \n",
       "1951                       1       1                       2.0   Executive   \n",
       "1952                       5       1                       1.0         AVP   \n",
       "1953                       2       1                       2.0   Executive   \n",
       "1954                       1       1                       2.0     Manager   \n",
       "1955                       3       0                       0.0   Executive   \n",
       "\n",
       "      MonthlyIncome  ProdTaken  \n",
       "id                              \n",
       "1      20384.000000          0  \n",
       "2      19599.000000          1  \n",
       "3      18934.078264          0  \n",
       "4      21274.000000          1  \n",
       "5      19907.000000          0  \n",
       "...             ...        ...  \n",
       "1951   20723.000000          0  \n",
       "1952   31595.000000          0  \n",
       "1953   21651.000000          0  \n",
       "1954   22218.000000          0  \n",
       "1955   17853.000000          1  \n",
       "\n",
       "[1955 rows x 19 columns]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62221425",
   "metadata": {},
   "source": [
    "6. test data도 같은 방식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "fdefd9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>19668.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>20021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>21334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>22950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Large Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>21880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>54.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Super Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AVP</td>\n",
       "      <td>32328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>33.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Fe Male</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>23733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>33.0</td>\n",
       "      <td>Company Invited</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>23987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Small Business</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Executive</td>\n",
       "      <td>22102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>31.0</td>\n",
       "      <td>Self Enquiry</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Salaried</td>\n",
       "      <td>Male</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Deluxe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Manager</td>\n",
       "      <td>22830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2933 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age    TypeofContact  CityTier  DurationOfPitch      Occupation  \\\n",
       "id                                                                       \n",
       "1     32.0  Company Invited         3              NaN  Small Business   \n",
       "2     46.0     Self Enquiry         2             11.0  Small Business   \n",
       "3     37.0     Self Enquiry         3             22.0  Small Business   \n",
       "4     43.0     Self Enquiry         1             36.0  Small Business   \n",
       "5     25.0     Self Enquiry         3              7.0  Large Business   \n",
       "...    ...              ...       ...              ...             ...   \n",
       "2929  54.0     Self Enquiry         1              6.0  Small Business   \n",
       "2930  33.0     Self Enquiry         1              9.0  Small Business   \n",
       "2931  33.0  Company Invited         1             31.0        Salaried   \n",
       "2932  26.0     Self Enquiry         1              9.0  Small Business   \n",
       "2933  31.0     Self Enquiry         1              9.0        Salaried   \n",
       "\n",
       "       Gender  NumberOfPersonVisiting  NumberOfFollowups ProductPitched  \\\n",
       "id                                                                        \n",
       "1        Male                       2                5.0         Deluxe   \n",
       "2        Male                       3                NaN         Deluxe   \n",
       "3        Male                       3                4.0         Deluxe   \n",
       "4        Male                       3                6.0         Deluxe   \n",
       "5      Female                       4                4.0          Basic   \n",
       "...       ...                     ...                ...            ...   \n",
       "2929   Female                       2                3.0   Super Deluxe   \n",
       "2930  Fe Male                       4                2.0         Deluxe   \n",
       "2931     Male                       4                4.0         Deluxe   \n",
       "2932     Male                       4                2.0          Basic   \n",
       "2933     Male                       3                5.0         Deluxe   \n",
       "\n",
       "      PreferredPropertyStar MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                   \n",
       "1                       3.0       Married            1.0         0   \n",
       "2                       4.0       Married            1.0         1   \n",
       "3                       3.0       Married            5.0         0   \n",
       "4                       3.0     Unmarried            6.0         0   \n",
       "5                       4.0     Unmarried            3.0         1   \n",
       "...                     ...           ...            ...       ...   \n",
       "2929                    3.0        Single            7.0         0   \n",
       "2930                    3.0     Unmarried            2.0         0   \n",
       "2931                    3.0      Divorced            3.0         0   \n",
       "2932                    5.0     Unmarried            2.0         0   \n",
       "2933                    3.0      Divorced            3.0         0   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting Designation  \\\n",
       "id                                                                           \n",
       "1                          2       0                       1.0     Manager   \n",
       "2                          5       0                       1.0     Manager   \n",
       "3                          5       1                       0.0     Manager   \n",
       "4                          3       1                       2.0     Manager   \n",
       "5                          4       1                       3.0   Executive   \n",
       "...                      ...     ...                       ...         ...   \n",
       "2929                       4       1                       1.0         AVP   \n",
       "2930                       3       0                       1.0     Manager   \n",
       "2931                       4       1                       1.0     Manager   \n",
       "2932                       2       1                       3.0   Executive   \n",
       "2933                       4       1                       1.0     Manager   \n",
       "\n",
       "      MonthlyIncome  \n",
       "id                   \n",
       "1           19668.0  \n",
       "2           20021.0  \n",
       "3           21334.0  \n",
       "4           22950.0  \n",
       "5           21880.0  \n",
       "...             ...  \n",
       "2929        32328.0  \n",
       "2930        23733.0  \n",
       "2931        23987.0  \n",
       "2932        22102.0  \n",
       "2933        22830.0  \n",
       "\n",
       "[2933 rows x 18 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "4fad6a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['MonthlyIncome', 'Age', 'NumberOfTrips']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMRegressor(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b1019870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18)\n",
      "(0, 18)\n",
      "(0, 18)\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[test_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "51a8f24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 18)\n"
     ]
    }
   ],
   "source": [
    "predict_na_col = ['TypeofContact']\n",
    "for col in predict_na_col:\n",
    "    na_idx = test_df[test_df[col].isna()].index\n",
    "    na_idx = list(map(lambda x:x-1, na_idx))\n",
    "    not_na_idx = test_df[list(map(lambda x:not x, test_df[col].isna()))].index\n",
    "    not_na_idx = list(map(lambda x:x-1, not_na_idx))\n",
    "    test_y = test_df_all_label_encoded.iloc[not_na_idx][col]\n",
    "    test_x = test_df_all_label_encoded.iloc[not_na_idx].drop([col], axis=1) \n",
    "    feature_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "    feature_model.fit(test_x, test_y)\n",
    "    feature_pred = feature_model.predict(test_df_all_label_encoded.iloc[na_idx].drop([col], axis=1))\n",
    "    back2col = lambda x: 'Company Invited' if x==0 else 'Self Enquiry'\n",
    "    feature_pred = list(map(back2col, feature_pred))\n",
    "    test_df.loc[test_df[test_df[col].isna()].index,col] = feature_pred\n",
    "    print(test_df[test_df[col].isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "7a7b2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_val = np.median(test_df[test_df['DurationOfPitch'].isna().apply(lambda x:not x)]['DurationOfPitch'])\n",
    "test_df.loc[test_df[test_df['DurationOfPitch'].isna()].index,'DurationOfPitch'] = med_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5cdb6",
   "metadata": {},
   "source": [
    "## 3. 범주형 데이터 변환\n",
    "### 타깃 인코딩 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "0ef61405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1955 entries, 1 to 1955\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Age                       1955 non-null   float64\n",
      " 1   TypeofContact             1955 non-null   object \n",
      " 2   CityTier                  1955 non-null   int64  \n",
      " 3   DurationOfPitch           1955 non-null   float64\n",
      " 4   Occupation                1955 non-null   object \n",
      " 5   Gender                    1955 non-null   object \n",
      " 6   NumberOfPersonVisiting    1955 non-null   int64  \n",
      " 7   NumberOfFollowups         1955 non-null   float64\n",
      " 8   ProductPitched            1955 non-null   object \n",
      " 9   PreferredPropertyStar     1955 non-null   float64\n",
      " 10  MaritalStatus             1955 non-null   object \n",
      " 11  NumberOfTrips             1955 non-null   float64\n",
      " 12  Passport                  1955 non-null   int64  \n",
      " 13  PitchSatisfactionScore    1955 non-null   int64  \n",
      " 14  OwnCar                    1955 non-null   int64  \n",
      " 15  NumberOfChildrenVisiting  1955 non-null   float64\n",
      " 16  Designation               1955 non-null   object \n",
      " 17  MonthlyIncome             1955 non-null   float64\n",
      " 18  ProdTaken                 1955 non-null   int64  \n",
      "dtypes: float64(7), int64(6), object(6)\n",
      "memory usage: 370.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "480bff0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.201288</td>\n",
       "      <td>0.194292</td>\n",
       "      <td>3</td>\n",
       "      <td>0.189490</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.156944</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212219</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>20384.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.185958</td>\n",
       "      <td>2</td>\n",
       "      <td>0.190852</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>0.202290</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196203</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>19599.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0.235955</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.209354</td>\n",
       "      <td>2</td>\n",
       "      <td>0.193853</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>0.202290</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196203</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>18934.078264</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.235955</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.209354</td>\n",
       "      <td>3</td>\n",
       "      <td>0.255144</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.202290</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>21274.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>0.207087</td>\n",
       "      <td>2</td>\n",
       "      <td>0.171296</td>\n",
       "      <td>0.139489</td>\n",
       "      <td>0.161396</td>\n",
       "      <td>0.124138</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175084</td>\n",
       "      <td>0.139489</td>\n",
       "      <td>19907.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.209354</td>\n",
       "      <td>3</td>\n",
       "      <td>0.255144</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170616</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>20723.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.185115</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.183956</td>\n",
       "      <td>0.192607</td>\n",
       "      <td>3</td>\n",
       "      <td>0.170404</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.270463</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200323</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>31595.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0.220096</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.187302</td>\n",
       "      <td>0.192607</td>\n",
       "      <td>3</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.296763</td>\n",
       "      <td>0.159215</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.183962</td>\n",
       "      <td>0.296763</td>\n",
       "      <td>21651.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.185958</td>\n",
       "      <td>3</td>\n",
       "      <td>0.255144</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.170616</td>\n",
       "      <td>0.131115</td>\n",
       "      <td>22218.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.179021</td>\n",
       "      <td>0.194292</td>\n",
       "      <td>2</td>\n",
       "      <td>0.189490</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.109541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194357</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>17853.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1955 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  TypeofContact  CityTier  DurationOfPitch  Occupation    Gender  \\\n",
       "id                                                                           \n",
       "1     28.0       0.214797         1             10.0    0.201288  0.194292   \n",
       "2     34.0       0.182975         3             14.0    0.191803  0.185958   \n",
       "3     45.0       0.235955         1             14.0    0.186992  0.209354   \n",
       "4     29.0       0.235955         1              7.0    0.191803  0.209354   \n",
       "5     42.0       0.178161         3              6.0    0.183588  0.207087   \n",
       "...    ...            ...       ...              ...         ...       ...   \n",
       "1951  28.0       0.182975         1             10.0    0.191803  0.209354   \n",
       "1952  41.0       0.185115         3              8.0    0.183956  0.192607   \n",
       "1953  38.0       0.220096         3             28.0    0.187302  0.192607   \n",
       "1954  28.0       0.182975         3             30.0    0.191803  0.185958   \n",
       "1955  22.0       0.214797         1              9.0    0.179021  0.194292   \n",
       "\n",
       "      NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "id                                                                \n",
       "1                          3           0.189490        0.304348   \n",
       "2                          2           0.190852        0.131115   \n",
       "3                          2           0.193853        0.131115   \n",
       "4                          3           0.255144        0.305263   \n",
       "5                          2           0.171296        0.139489   \n",
       "...                      ...                ...             ...   \n",
       "1951                       3           0.255144        0.305263   \n",
       "1952                       3           0.170404        0.054348   \n",
       "1953                       3           0.202899        0.296763   \n",
       "1954                       3           0.255144        0.131115   \n",
       "1955                       2           0.189490        0.304348   \n",
       "\n",
       "      PreferredPropertyStar  MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                    \n",
       "1                  0.161111       0.156944            3.0         0   \n",
       "2                  0.202290       0.333333            1.0         1   \n",
       "3                  0.202290       0.152022            2.0         0   \n",
       "4                  0.202290       0.152022            3.0         0   \n",
       "5                  0.161396       0.124138            2.0         0   \n",
       "...                     ...            ...            ...       ...   \n",
       "1951               0.169584       0.333333            2.0         0   \n",
       "1952               0.270463       0.112676            1.0         0   \n",
       "1953               0.159215       0.112676            7.0         0   \n",
       "1954               0.169584       0.152022            3.0         0   \n",
       "1955               0.161111       0.109541            1.0         1   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting  Designation  \\\n",
       "id                                                                            \n",
       "1                          1       0                  0.212219     0.304348   \n",
       "2                          5       1                  0.196203     0.131115   \n",
       "3                          4       1                  0.196203     0.131115   \n",
       "4                          4       0                  0.212264     0.305263   \n",
       "5                          3       1                  0.175084     0.139489   \n",
       "...                      ...     ...                       ...          ...   \n",
       "1951                       1       1                  0.170616     0.305263   \n",
       "1952                       5       1                  0.200323     0.054348   \n",
       "1953                       2       1                  0.183962     0.296763   \n",
       "1954                       1       1                  0.170616     0.131115   \n",
       "1955                       3       0                  0.194357     0.304348   \n",
       "\n",
       "      MonthlyIncome  ProdTaken  \n",
       "id                              \n",
       "1      20384.000000          0  \n",
       "2      19599.000000          1  \n",
       "3      18934.078264          0  \n",
       "4      21274.000000          1  \n",
       "5      19907.000000          0  \n",
       "...             ...        ...  \n",
       "1951   20723.000000          0  \n",
       "1952   31595.000000          0  \n",
       "1953   21651.000000          0  \n",
       "1954   22218.000000          0  \n",
       "1955   17853.000000          1  \n",
       "\n",
       "[1955 rows x 19 columns]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "7ce1327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>ProdTaken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "      <td>1955.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37.278164</td>\n",
       "      <td>0.195677</td>\n",
       "      <td>1.641432</td>\n",
       "      <td>15.444501</td>\n",
       "      <td>0.195760</td>\n",
       "      <td>0.196090</td>\n",
       "      <td>2.922762</td>\n",
       "      <td>0.195825</td>\n",
       "      <td>0.195886</td>\n",
       "      <td>0.195774</td>\n",
       "      <td>0.195785</td>\n",
       "      <td>3.257109</td>\n",
       "      <td>0.291049</td>\n",
       "      <td>3.067519</td>\n",
       "      <td>0.619437</td>\n",
       "      <td>0.195522</td>\n",
       "      <td>0.195886</td>\n",
       "      <td>23409.566558</td>\n",
       "      <td>0.195908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.058419</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.908744</td>\n",
       "      <td>7.941725</td>\n",
       "      <td>0.030149</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.712276</td>\n",
       "      <td>0.048581</td>\n",
       "      <td>0.084497</td>\n",
       "      <td>0.047377</td>\n",
       "      <td>0.080059</td>\n",
       "      <td>1.797209</td>\n",
       "      <td>0.454362</td>\n",
       "      <td>1.372915</td>\n",
       "      <td>0.485649</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.084497</td>\n",
       "      <td>5580.593575</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.178161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.179021</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>0.159215</td>\n",
       "      <td>0.109541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165865</td>\n",
       "      <td>0.054348</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.182975</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>0.192607</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.177829</td>\n",
       "      <td>0.131021</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.146515</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183962</td>\n",
       "      <td>0.131021</td>\n",
       "      <td>20066.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.185115</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.189490</td>\n",
       "      <td>0.154206</td>\n",
       "      <td>0.169584</td>\n",
       "      <td>0.155587</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196203</td>\n",
       "      <td>0.154206</td>\n",
       "      <td>21969.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.191803</td>\n",
       "      <td>0.207087</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.296763</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>0.273171</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212219</td>\n",
       "      <td>0.296763</td>\n",
       "      <td>25401.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.235955</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.310924</td>\n",
       "      <td>0.209354</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.288660</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>98678.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  TypeofContact     CityTier  DurationOfPitch   Occupation  \\\n",
       "count  1955.000000    1955.000000  1955.000000      1955.000000  1954.000000   \n",
       "mean     37.278164       0.195677     1.641432        15.444501     0.195760   \n",
       "std       9.058419       0.019337     0.908744         7.941725     0.030149   \n",
       "min      18.000000       0.178161     1.000000         5.000000     0.179021   \n",
       "25%      31.000000       0.182975     1.000000         9.000000     0.183588   \n",
       "50%      36.000000       0.185115     1.000000        14.000000     0.186992   \n",
       "75%      43.000000       0.214797     3.000000        19.000000     0.191803   \n",
       "max      61.000000       0.235955     3.000000        36.000000     0.310924   \n",
       "\n",
       "            Gender  NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "count  1955.000000             1955.000000        1955.000000     1955.000000   \n",
       "mean      0.196090                2.922762           0.195825        0.195886   \n",
       "std       0.013235                0.712276           0.048581        0.084497   \n",
       "min       0.142857                1.000000           0.054054        0.054348   \n",
       "25%       0.192607                2.000000           0.177829        0.131021   \n",
       "50%       0.198020                3.000000           0.189490        0.154206   \n",
       "75%       0.207087                3.000000           0.202899        0.296763   \n",
       "max       0.209354                5.000000           0.400000        0.305263   \n",
       "\n",
       "       PreferredPropertyStar  MaritalStatus  NumberOfTrips     Passport  \\\n",
       "count            1955.000000    1955.000000    1955.000000  1955.000000   \n",
       "mean                0.195774       0.195785       3.257109     0.291049   \n",
       "std                 0.047377       0.080059       1.797209     0.454362   \n",
       "min                 0.159215       0.109541       1.000000     0.000000   \n",
       "25%                 0.161111       0.146515       2.000000     0.000000   \n",
       "50%                 0.169584       0.155587       3.000000     0.000000   \n",
       "75%                 0.223776       0.273171       4.000000     1.000000   \n",
       "max                 0.288660       0.352273      19.000000     1.000000   \n",
       "\n",
       "       PitchSatisfactionScore       OwnCar  NumberOfChildrenVisiting  \\\n",
       "count             1955.000000  1955.000000               1955.000000   \n",
       "mean                 3.067519     0.619437                  0.195522   \n",
       "std                  1.372915     0.485649                  0.017981   \n",
       "min                  1.000000     0.000000                  0.165865   \n",
       "25%                  2.000000     0.000000                  0.183962   \n",
       "50%                  3.000000     1.000000                  0.196203   \n",
       "75%                  4.000000     1.000000                  0.212219   \n",
       "max                  5.000000     1.000000                  0.247312   \n",
       "\n",
       "       Designation  MonthlyIncome    ProdTaken  \n",
       "count  1955.000000    1955.000000  1955.000000  \n",
       "mean      0.195886   23409.566558     0.195908  \n",
       "std       0.084497    5580.593575     0.397000  \n",
       "min       0.054348    1000.000000     0.000000  \n",
       "25%       0.131021   20066.500000     0.000000  \n",
       "50%       0.154206   21969.000000     0.000000  \n",
       "75%       0.296763   25401.500000     0.000000  \n",
       "max       0.305263   98678.000000     1.000000  "
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "a924ade6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TypeofContact',\n",
       " 'Occupation',\n",
       " 'Gender',\n",
       " 'ProductPitched',\n",
       " 'MaritalStatus',\n",
       " 'Designation']"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_col = []\n",
    "for col in train_df.columns:\n",
    "    # print(type(train_df[col].dtypes))\n",
    "    if train_df[col].dtypes == 'object':\n",
    "        obj_col.append(col)\n",
    "\n",
    "categorical_col = ['CityTier', 'NumberOfPersonVisiting', 'Passport', 'PitchSatisfactionScore', \n",
    "                   'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfChildrenVisiting',\n",
    "                  'OwnCar']\n",
    "obj_col += categorical_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "a2df9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "for c in obj_col:\n",
    "    # target encode test data \n",
    "    data_tmp = pd.DataFrame({c:train_df[c], 'target': train_df['ProdTaken']})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "    test_df[c] = test_df[c].map(target_mean)\n",
    "    \n",
    "    # target encode train data\n",
    "    tmp = np.repeat(np.nan, train_df.shape[0])\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_df):\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        tmp[idx_2] = train_df[c].iloc[idx_2].map(target_mean)\n",
    "        \n",
    "    train_df[c] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "d5b58598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.152792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.133891</td>\n",
       "      <td>0.216398</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>19668.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.178201</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>0.152792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377856</td>\n",
       "      <td>0.245431</td>\n",
       "      <td>0.216398</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>20021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.191617</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.152792</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.245431</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.189448</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>21334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>22950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.288344</td>\n",
       "      <td>0.187861</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.191617</td>\n",
       "      <td>0.299329</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.377856</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.299329</td>\n",
       "      <td>21880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.187861</td>\n",
       "      <td>0.200737</td>\n",
       "      <td>0.178201</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.332378</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>32328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.226316</td>\n",
       "      <td>0.216398</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>23733.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.183420</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.191617</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>23987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.191283</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.201456</td>\n",
       "      <td>0.067416</td>\n",
       "      <td>0.299329</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.133891</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.299329</td>\n",
       "      <td>22102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.183420</td>\n",
       "      <td>0.202154</td>\n",
       "      <td>0.193320</td>\n",
       "      <td>0.259819</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>0.162829</td>\n",
       "      <td>0.117333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.175066</td>\n",
       "      <td>0.183320</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>22830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2933 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  TypeofContact  CityTier  DurationOfPitch  Occupation    Gender  \\\n",
       "id                                                                           \n",
       "1     32.0       0.225352  0.242268             13.0    0.191283  0.202154   \n",
       "2     46.0       0.183850  0.266667             11.0    0.191283  0.202154   \n",
       "3     37.0       0.183850  0.242268             22.0    0.191283  0.202154   \n",
       "4     43.0       0.183850  0.169914             36.0    0.191283  0.202154   \n",
       "5     25.0       0.183850  0.242268              7.0    0.288344  0.187861   \n",
       "...    ...            ...       ...              ...         ...       ...   \n",
       "2929  54.0       0.183850  0.169914              6.0    0.191283  0.187861   \n",
       "2930  33.0       0.183850  0.169914              9.0    0.191283  0.160714   \n",
       "2931  33.0       0.225352  0.169914             31.0    0.183420  0.202154   \n",
       "2932  26.0       0.183850  0.169914              9.0    0.191283  0.202154   \n",
       "2933  31.0       0.183850  0.169914              9.0    0.183420  0.202154   \n",
       "\n",
       "      NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "id                                                                \n",
       "1                   0.200737           0.259819        0.130624   \n",
       "2                   0.193320           0.178201        0.130624   \n",
       "3                   0.193320           0.191617        0.130624   \n",
       "4                   0.193320           0.361702        0.130624   \n",
       "5                   0.201456           0.191617        0.299329   \n",
       "...                      ...                ...             ...   \n",
       "2929                0.200737           0.178201        0.076923   \n",
       "2930                0.201456           0.067416        0.130624   \n",
       "2931                0.201456           0.191617        0.130624   \n",
       "2932                0.201456           0.067416        0.299329   \n",
       "2933                0.193320           0.259819        0.130624   \n",
       "\n",
       "      PreferredPropertyStar  MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                    \n",
       "1                  0.162829       0.152792            1.0  0.121212   \n",
       "2                  0.217631       0.152792            1.0  0.377856   \n",
       "3                  0.162829       0.152792            5.0  0.121212   \n",
       "4                  0.162829       0.276596            6.0  0.121212   \n",
       "5                  0.217631       0.276596            3.0  0.377856   \n",
       "...                     ...            ...            ...       ...   \n",
       "2929               0.162829       0.332378            7.0  0.121212   \n",
       "2930               0.162829       0.276596            2.0  0.121212   \n",
       "2931               0.162829       0.117333            3.0  0.121212   \n",
       "2932               0.281915       0.276596            2.0  0.121212   \n",
       "2933               0.162829       0.117333            3.0  0.121212   \n",
       "\n",
       "      PitchSatisfactionScore    OwnCar  NumberOfChildrenVisiting  Designation  \\\n",
       "id                                                                              \n",
       "1                   0.133891  0.216398                  0.206235     0.130624   \n",
       "2                   0.245431  0.216398                  0.206235     0.130624   \n",
       "3                   0.245431  0.183320                  0.189448     0.130624   \n",
       "4                   0.226316  0.183320                  0.177193     0.130624   \n",
       "5                   0.175066  0.183320                  0.231343     0.299329   \n",
       "...                      ...       ...                       ...          ...   \n",
       "2929                0.175066  0.183320                  0.206235     0.076923   \n",
       "2930                0.226316  0.216398                  0.206235     0.130624   \n",
       "2931                0.175066  0.183320                  0.206235     0.130624   \n",
       "2932                0.133891  0.183320                  0.231343     0.299329   \n",
       "2933                0.175066  0.183320                  0.206235     0.130624   \n",
       "\n",
       "      MonthlyIncome  \n",
       "id                   \n",
       "1           19668.0  \n",
       "2           20021.0  \n",
       "3           21334.0  \n",
       "4           22950.0  \n",
       "5           21880.0  \n",
       "...             ...  \n",
       "2929        32328.0  \n",
       "2930        23733.0  \n",
       "2931        23987.0  \n",
       "2932        22102.0  \n",
       "2933        22830.0  \n",
       "\n",
       "[2933 rows x 18 columns]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2807ffca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "TypeofContact               0\n",
       "CityTier                    0\n",
       "DurationOfPitch             0\n",
       "Occupation                  0\n",
       "Gender                      0\n",
       "NumberOfPersonVisiting      0\n",
       "NumberOfFollowups           0\n",
       "ProductPitched              0\n",
       "PreferredPropertyStar       0\n",
       "MaritalStatus               0\n",
       "NumberOfTrips               0\n",
       "Passport                    0\n",
       "PitchSatisfactionScore      0\n",
       "OwnCar                      0\n",
       "NumberOfChildrenVisiting    0\n",
       "Designation                 0\n",
       "MonthlyIncome               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972dedd5",
   "metadata": {},
   "source": [
    "## 4. 결측치 채움 + 타깃인코딩한 데이터에 GBDT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c6efe3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(n_estimators=1000, random_state=777)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(n_estimators=1000, random_state=777)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(n_estimators=1000, random_state=777)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_x = train_df.drop(['ProdTaken'], axis = 1)\n",
    "train_y = train_df['ProdTaken']\n",
    "lightgbm_model =lgb.LGBMClassifier(random_state=777, n_estimators=1000)\n",
    "lightgbm_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "8601bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['ProdTaken'] = lightgbm_model.predict(test_df)\n",
    "file_name = 'lightgbm_without_null_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f54110",
   "metadata": {},
   "source": [
    "#### 성능이 결측치 채움 + 타깃인코딩한 데이터에서 더 좋지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8aa990",
   "metadata": {},
   "source": [
    "## 5. 신경망 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "7354eec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 19 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "train_col = train_df.columns\n",
    "for i, col in enumerate(train_col, 1):\n",
    "    row = int(np.sqrt(len(train_col)))\n",
    "    plt.subplot(row, int(len(train_col)/row)+1, i)\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dffa793",
   "metadata": {},
   "source": [
    "num_cols = ['Age', 'DurationOfPitch', 'NumberOfTrips', 'MonthlyIncome']\n",
    "1. Age는 rankgauss 방법을 이용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "369dc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "transformer.fit(pd.concat([train_df[['Age']], test_df[['Age']]]))\n",
    "\n",
    "train_df[['Age']] = transformer.transform(train_df[['Age']])\n",
    "test_df[['Age']] = transformer.transform(test_df[['Age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aaca1c",
   "metadata": {},
   "source": [
    "2. DurationOfPitch는 박스-칵스 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "804a9d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "pt.fit(pd.concat([train_df[['DurationOfPitch']], test_df[['DurationOfPitch']]]))\n",
    "\n",
    "train_df[['DurationOfPitch']] = pt.transform(train_df[['DurationOfPitch']])\n",
    "test_df[['DurationOfPitch']] = pt.transform(test_df[['DurationOfPitch']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f28b9",
   "metadata": {},
   "source": [
    "3. NumberOfTrips, MonthlyIncome는 클리핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "1cfb5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p01 = pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]).quantile(0.01)\n",
    "p99 = pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]).quantile(0.90)\n",
    "\n",
    "train_df[['NumberOfTrips', 'MonthlyIncome']] = train_df[['NumberOfTrips', 'MonthlyIncome']].clip(p01, p99, axis=1)\n",
    "test_df[['NumberOfTrips', 'MonthlyIncome']] = test_df[['NumberOfTrips', 'MonthlyIncome']].clip(p01, p99, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f4b3f",
   "metadata": {},
   "source": [
    "4. NumberOfTrips, MonthlyIncome는 rankgauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "fac658f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "transformer.fit(pd.concat([train_df[['NumberOfTrips', 'MonthlyIncome']], test_df[['NumberOfTrips', 'MonthlyIncome']]]))\n",
    "\n",
    "train_df[['NumberOfTrips', 'MonthlyIncome']] = transformer.transform(train_df[['NumberOfTrips', 'MonthlyIncome']])\n",
    "test_df[['NumberOfTrips', 'MonthlyIncome']] = transformer.transform(test_df[['NumberOfTrips', 'MonthlyIncome']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "be1ee267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 19 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "train_col = train_df.columns\n",
    "for i, col in enumerate(train_col, 1):\n",
    "    row = int(np.sqrt(len(train_col)))\n",
    "    plt.subplot(row, int(len(train_col)/row)+1, i)\n",
    "    plt.hist(train_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "c7e4d87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIoAAANeCAYAAAB9GeVCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAADcTElEQVR4nOzde5hkVX3v//cnoIgCAWRArg4qYpATUScEJTEoonhJwPyigeMFDYbowaiJOQGMJ5IYctAoJsZIDooCUUTiJRJFFEm8JYAOilxEwm2EkZEZuShegoLf3x97tdT0VE/3dFVXV3e/X89TT1etvXfVt6r3qr1r7bW+K1WFJEmSJEmS9AvzHYAkSZIkSZLGgw1FkiRJkiRJAmwokiRJkiRJUmNDkSRJkiRJkgAbiiRJkiRJktTYUCRJkiRJkiTAhiJJGltJDkxyXZIfJDl8vuORlookr0/ynjl43l9Pcu2wn1daSpJ8KslRI3y95yW5pR2LHz/Nuj9I8ohZvMZLk3xp9lFKS9ds6502zoaiJSzJ55LcmWSL+Y5FGgftQDNx+1mSH/c8fuE8hPSXwDuraquq+pcW4zOTfCHJ3UnWJfl8kt8a9IWSnJjk/YM+T3uuzyV5+TCeS5pLSf5nkpWtjq9pP0B/rar+uqpe3tZZnqSSbD6D53thz3fGj9v3yM+/V6rqi1W199y/M2luJFnV9u27k9yV5D+TvCLJnPym6HdsqqpnVdWZQ3r+fZKcl+R77T39e5InT1rtrcCr2rH4az2fwQ+S3JbkfUm2arFtVVU3tuc+I8lfDSNOaS61hsork/woyXeSnJpk2/mOq59+55i99U7DY0PREpVkOfDrQAED/8iUFoN2oNmqqrYCbgZ+s6fsA/MQ0sOBqyceJPkd4J+Bs4DdgJ2APwd+cx5ikxa0JH8M/C3w13R1aQ/gXcBhs33OqvpAz3fIs4BbJ32vDBLvtA1V0oj8ZlVtTXeMOhk4Djh9U59kvvfpJI8E/gO4EtgT2AX4GPCZJE/qWXW9Y3Hzm61OPwH4FeANcx+xNHxJXge8GfjfwC8CB9Dt8xcmeeB8xqb5ZUPR0vUS4BLgDODn3XeTPDTJvyb5fpKvJPmr3q6wSR6T5MIkdyS5NskLRh+6NDpJtmj7+//oKduxXU1cluSgJKvbUJXvtiuNL5y0/VuT3NyuPP5jki17lv9+kuvba5yXZJdWfgPwCOBf21XLLYBTgDdV1Xuq6ntV9bOq+nxV/X7b5heSvCHJt5KsTXJWkl9syyZ6RRzVYvlukj9ryw4FXg/8bnutr7fylyW5pl1lvTHJH0z6bA5Lcnn7vrghyaFJTqJrhH5ne653zsX/RRpEqxd/CRxbVR+tqh9W1U+r6l+r6n9P6sXwhfb3rrZP/8bGvhOmed2DkqzuebxLko+k6x14U5JX9yw7McmHk7w/yfeBlw7r/UvD0I5D5wG/CxyVZN/JV/szaUhVOw4dm+Q64LpW9nfphnZ9P8llSX69lU91bPr5a8z2uNecCFxcVX9WVXdU1d1V9Q7gn4A3t+P3D4DNgK+34/Lkz+DbwKeAfXve36OSHAO8EPjTFvu/tuW7J/loq/O3Tz5GtvOFO9v3wbNm+7+RZiLJNsBfAH9YVRe04+Aq4AV0jUUvSrJZO8e9oZ0PXpZk97b9Y3P/78Lbkry+la/Xm67PsW9VkhOSfKPt7+9L8qC2bLskn2h15M52f7e2rO855kS9a/d/sX0PrGvfC29I6/E48X1kPZsZG4qWrpcAH2i3ZybZqZX/A/BD4GF0DUi9jUgPAS4EzgZ2BI4E3pXksSOMWxqpqroHOAd4UU/xkcBnq2pde/wwYAdgV7o6c1qSieElbwYeDewHPKqt8+cASZ4G/F+6A/LOwLfaa1FVj6SnVxPd1c7dgQ9vJNyXtttT6RqZtgImN9T8GrA3cDDw50l+qaouoOtV8aHW8+Fxbd21wHOBbYCXAW9P8oQW+/50PZv+N7At8BRgVVX9GfBF7u+m/6qNxCvNlycBD6LrPTCdp7S/27Z9+vNM/50wrXbi+q/A1+m+Fw4GXpvkmT2rHUZX57elO15LY6eqvgyspvsBNxOHA78K7NMef4XuGLk93TnmPyd50EaOTb1eyiyOe638ELpeupOdCxwIbNbTE/Bx7bi8nvaD+dnA13rLq+o0ujr7lhb7bybZDPgE3bF+OV29P6dns18FrqU7n3gLcHqS9IlPGpYn0x0LP9pbWFU/oGsAPQT4Y7pj3LPpzgd/D/hRkq2BzwIX0PXGexRw0Sa89guBZwKPpDtPnuiV9wvA++gaqvYAfkyr0zM8x/x7up5RjwB+g+4378t6llvPZsiGoiUoya/RVb5zq+oy4Abgf7YD2P8HvLGqflRV3wB6x4A/l+6H4Puq6t6q+irwEeB3RvwWpFE7k66OTHxnvpjuimOv/1NV97QfkZ8EXtAOPL8P/NHE1Uq6k94j2jYvBN5bVV9tDVInAE9KNzR0soe2v2s2EucLgVOq6sZ2kD8BOCLrd+//i6r6cVV9ne4Har8TbwCq6pNVdUN1Pg98hvt/CBzdYr+w9Wz6dlV9cyOxSePkocB3q+reWW4/k++E6fwKsKyq/rKqftLyK7yb+78foOvt8C+tjv14lrFKo3ArXUPPTPzfdkz8MUBVvb+qbm/nlm8DtqBr2JmJQY57O9D/mLqG7jfSdht53X9JchfwJeDzdMf26exP94P6f7dejP9dVb0JrL9VVe+uqvvovmN2phsWK82VHZj6WLimLX858IaquradD369qm6n+134nap6W9uX766qSzfhtd9ZVbdU1R3ASXSNUbTvgo+036J3t2W/MZMnbL9lfxc4ocWzCngb3TF6gvVshhzvvjQdBXymqr7bHp/dyj5It0/c0rNu7/2HA7/aDowTNmfTT46lBaWqLk3yQ+A3kqyhu2pyXs8qd1bVD3sef4vuZHAZ8GDgsp6LFaHrxk5b56s9r/ODJLfTXWVcNSmM29vfnYGbpgh1l/bavXFszvoHwO/03P8R3dXXvlp33DfSXen5hfZermyLdwfOn2pbaczdDuyQZPPZNBbN4DthJh4O7DLpmLoZ3dXSCbcgLQy7AnfMcN319ut0OVJeTncMK7peCzvM8LkGOe59l+6YOtnOwM+AOzfyuodX1WdnGOOE3el+pE71nfPzOKvqR+28YaDcZtI0vsvUx8Kd2/ID6ToVTLb7FOUz1fs9MHHeTJIHA28HDuX+xtqtk2zWGnc2ZgfggWz4nbBrz2Pr2QzZo2iJSZcb5QV0J7ffSfId4I/orq7sBNxLlyR3wu49928BPl9V2/bctqqqV44qfmkenUk31OTFwIer6r97lm3XhmZO2IPu6up36brMPranzvxiT1f2W+l+LAI/H975UODbfV7/Wro6+P9tJMb1nq/FcS9w2wzeX/U+SJcT6SN0s73sVFXb0jUMTbR43ULXXXja55LG0MXAf9MNgZnOVPvzxr4TZuIW4KZJx9Stq+rZM3htaWwk+RW6H2Jfoktf8OCexQ/rs8nP9+t0+YiOozs33a4da77H/cea6erAIMe9zwLP71P+ArrefD+awXNszOTYbwH2iInpNT4uBu4Bfru3sJ2PPotuKNlU53sbOw+cyfdA72/MifNmgNfR9Sj81arahvuHf8/kO+G7wE/Z8Duh33m1pmFD0dJzOHAf3bjw/drtl+iuYL6EbozqiUkenOQxrWzCJ4BHJ3lxkge026/0jPWWFrN/Ap5H98PwrD7L/yLJA9tJ73OBf66qn9ENJXl7kh0Bkuzak4PkbOBlSfZrDTN/DVzausqup6qKbpz4/0mXZHqbdEk8fy3JaW21DwJ/lGTPdFP1TuR2mEmPiduA5T1DaR5I1/1/HXBv6130jJ71T2+xH9zi2LV9Z0w81yNm8JrSvKiq79HlCvuHJIe3Y94DkjwryVsmrb6OrnfB5H16uu+E6XwZ+H6S45JsmS5h6L7tR7c09tpx6Ll0eXbeX1VXApcDv93q1KPohilvzNZ0DTvrgM2T/Dldj6IJk49Nkw1y3PsL4MlJTkqyfZKtk/wh3bnvcTPYfjqTj4VfphvOc3KShyR5UJIDh/A60qy0Y+FfAH+fbkKSB7T0B/9Ml3fsn4D3AG9Kslc6v5zkoXS/Cx+W5LXpEr9vneRX21NfDjy71auHAa/t8/LHJtktyfZ0Ses/1Mq3prvIeldb9sZJ2015jtl6HJ0LnNTieTjdufP7+62vjbOhaOk5CnhfVd1cVd+ZuNElCXsh8Cq6BGDfofty+CBdSzNtnOgz6PIn3NrWeTPdj0lpUauq1XTDxIr1h4ZAVxfupKsXHwBe0ZOv5zjgeuCSdDMXfZaWe6GqLgL+D13PnTV0V2aOYApV9WG6sde/117rNuCvgI+3Vd5LV2+/QDc87b+BP5zhW5xI6Hl7kq+2+v5qugPuncD/pGdoTXXJS19G1z34e3Q5Giau4Pwd8DvpZpR4xwxfXxqpqjqF7gTyDXQ/Um+hOwb+y6T1fkSXI+E/ktyV5IBWvrHvhJm8/n3Ab9JdsLmJ7kroe+iOwdI4+9ckd9PVmT+jm5FzIlns24Gf0B2fzmT6JOyfpkua+190Q0T+m/WHpKx3bOqz/ayPe1V1HV2i68fRDfdeQ9dr95lV9R8zeY5pnA7s0743/qWnzj+KbrKK1XTHdGneVNVb6Bpq3gp8H7iUrg4e3PJnnkJ3LviZtvx0YMt2nngI3T79HbpZDJ/anvaf6PKBrWrbTTQC9Tq7Lbux3SZmSftbYEu6Y+IldMmye013jvmHdD2abqTr5Xg23feENlG6i9RSf0neDDysqo6admVpkUvyXuDWqnpDT9lBdFdSd5tqO0mLU7/vBEmSNLUkq4CXzyLPl0bIMbJaTxs68kC6hLW/Qtdl+OXzGpQ0BlpX3N8GHj/PoUgaA34nSJKkxcqhZ5psa7o8RT+k62b4Nu4f1iItSUneBFwF/E1VTTXjmARAkt2T/HuSa5JcneQ1rXz7JBcmua793a5nmxOSXJ/k2p4cViR5YpIr27J3JPdPn6f543eCJElazBx6JknSECXZGdi5qr6aZGvgMrqJBF4K3FFVJyc5nm6Gn+OS7EOXD25/uulhPws8uqruS/Jl4DV04/TPB95RVZ8a+ZuSJEnSkmGPIkmShqiq1lTVV9v9u4Fr6KaOPowuuSvt7+Ht/mHAOVV1T+udcj2wf2tw2qaqLm6z3p3FzKZzlyRJkmZt1jmKkuzN+hnMH0E31exZrXw5XabzF1TVnW2bE+hy3twHvLqqPj3d6+ywww61fPny2YYpLWiXXXbZd6tq2XzH0Y91U0vZTOtmy2PzeLpZRHaqqjXQNSYl2bGttitdj6EJq1vZT9v9yeVTsl5qqRvX46Z1U0uddVMaT1PVzVk3FFXVtXRTupJkM+DbwMeA44GLerrWHw9MdK0/AngsrWt9kke3qSKntHz5clauXDnbMKUFLcm35juGqVg3tZTNpG4m2Qr4CPDaqvr+RtIL9VtQGymf/DrHAMcA7LHHHtZLLWnjetz0mKmlzropjaep6uawhp4dDNxQVd9iE7vWD+n1JUkaG0keQNdI9IGq+mgrvq0NJ5vIY7S2la8Gdu/ZfDfg1la+W5/y9VTVaVW1oqpWLFs2dhdrJUmStMAMq6HoCLpEnDCpaz3Q27X+lp5tpuxCn+SYJCuTrFy3bt2QQpQkae61mclOB66pqlN6Fp0HHNXuH8X9M0qeBxyRZIskewJ7AV9ux9C7kxzQnvMlOAulJEmS5tjADUVJHgj8FvDP063ap6zvlGteHZUkLWAHAi8Gnpbk8nZ7NnAycEiS64BD2mOq6mrgXOAbwAXAsT3Dsl8JvIeuF+4NgDOeSXMgyaokV7b6urKVbZ/kwiTXtb/b9ax/QpLrk1yb5JnzF7kkScM36xxFPZ4FfLWqbmuPb0uyc0vUOZOu9ZIkLRpV9SX6XxyBbqh2v21OAk7qU74S2Hd40UnaiKdW1Xd7Hg8176YkSQvFMIaeHcn9w85gE7vWD+H1JUmSpGEz76YkaUkaqKEoyYPpus9/tKd4Nl3rJUmSpPlSwGeSXNZmEoQB826ac1OStFANNPSsqn4EPHRS2e1sYtd6SZIkaR4dWFW3JtkRuDDJNzey7ozyblbVacBpACtWrOibl1OSpHE0jBxFWqCWH//JaddZdfJzRhCJFiP3L2k8WTelDVXVre3v2iQfoxtKtmTzbvo9Id3P+qClaBg5iiRJkqQFKclDkmw9cR94BnAV5t2UJC1R9iiSJEnSUrYT8LEk0J0bn11VFyT5CnBukqOBm4HnQ5d3M8lE3s17Me+mJGmRsaFIWqCS7A6cBTwM+BlwWlX9XZLtgQ8By4FVwAuq6s62zQnA0cB9wKur6tOt/InAGcCWwPnAa6rKfAqSpEWvqm4EHten3LybkqQlyaFn0sJ1L/C6qvol4ADg2CT7AMcDF1XVXsBF7TFt2RHAY4FDgXcl2aw916nAMXTd5/dqyyVJkiRJS4wNRdICVVVrquqr7f7dwDV00/MeBpzZVjsTOLzdPww4p6ruqaqbgOuB/VuCzm2q6uLWi+isnm0kSZIkSUuIDUXSIpBkOfB44FJgp6paA11jErBjW21X4JaezVa3sl3b/cnl/V7nmCQrk6xct27dUN+DJEmSJGn+2VAkLXBJtgI+Ary2qr6/sVX7lNVGyjcsrDqtqlZU1Yply5ZterCSJElSjyS7J/n3JNckuTrJa1r5iUm+neTydnt2zzYnJLk+ybVJntlT/sQkV7Zl70jLUi9p05jMWlrAkjyArpHoA1X10VZ8W5Kdq2pNG1a2tpWvBnbv2Xw34NZWvlufckmSJGmuTeTd/GqSrYHLklzYlr29qt7au/KkvJu7AJ9N8ug2++BE3s1L6CZoORT41Ijeh7Ro2KNIWqDaFZLTgWuq6pSeRecBR7X7RwEf7yk/IskWSfakS1r95TY87e4kB7TnfEnPNpIkSdKc2UjezamYd1OaYzYUSQvXgcCLgadN6pJ7MnBIkuuAQ9pjqupq4FzgG8AFwLHtygvAK4H30B1ob8ArL5IkSRqxSXk3AV6V5Iok702yXSsbKO+mOTel6Tn0TFqgqupL9M8vBHDwFNucBJzUp3wlsO/wopMkSZJmbnLezSSnAm+iy535JuBtwO8xYN7NqjoNOA1gxYoVffNySkudPYokSZIkSfOmX97Nqrqtqu6rqp8B7wb2b6ubd1OaYzYUSZIkSZLmxVR5N1vOoQnPA65q9827Kc0xh55JkiRJkubLRN7NK5Nc3speDxyZZD+64WOrgD+ALu9mkom8m/eyYd7NM4At6XJumndTmgUbiiRJkiRJ82IjeTfP38g25t2U5tBAQ8+SbJvkw0m+meSaJE9Ksn2SC5Nc1/5u17P+CUmuT3JtkmcOHr4kSZIkSZKGZdAcRX8HXFBVjwEeB1wDHA9cVFV7ARe1xyTZBzgCeCxwKPCuJJsN+PqSJEmSJEkaklk3FCXZBngKXeIxquonVXUXcBhwZlvtTODwdv8w4JyquqeqbgKu5/7M9ZIkSZIkSZpng/QoegSwDnhfkq8leU+ShwA7tYzztL87tvV3BW7p2X51K9tAkmOSrEyyct26dQOEKEmSJEmSpJkapKFoc+AJwKlV9Xjgh7RhZlPol6Cs+q1YVadV1YqqWrFs2bIBQpQkSZIkSdJMDdJQtBpYXVWXtscfpms4ui3JzgDt79qe9Xfv2X434NYBXl+SJEmSJElDNOuGoqr6DnBLkr1b0cHAN4DzgKNa2VHAx9v984AjkmyRZE9gL+DLs319SZIkSZIkDdfmA27/h8AHkjwQuBF4GV3j07lJjgZuBp4PUFVXJzmXrjHpXuDYqrpvwNeXJEmSJEnSkAzUUFRVlwMr+iw6eIr1TwJOGuQ1JUmSJEmSNDcGyVEkSZIkSZKkRcSGIkmSJEmSJAE2FEmSJEmSJKmxoUiSJEmSJEmADUWSJEkSSTZL8rUkn2iPt09yYZLr2t/tetY9Icn1Sa5N8sz5i1qSpOGzoUiSJEmC1wDX9Dw+HrioqvYCLmqPSbIPcATwWOBQ4F1JNhtxrJIkzRkbiiRJkrSkJdkNeA7wnp7iw4Az2/0zgcN7ys+pqnuq6ibgemD/EYUqSdKcs6FIkiRJS93fAn8K/KynbKeqWgPQ/u7YyncFbulZb3UrW0+SY5KsTLJy3bp1cxK0JElzYfP5DkCSpMUkyXuB5wJrq2rfVnYi8PvAxK/F11fV+W3ZCcDRwH3Aq6vq0638icAZwJbA+cBrqqpG906kpSHJRH29LMlBM9mkT9kGdbOqTgNOA1ixYoV1t4/lx39y2nVWnfycEUQiSepljyJJkobrDLq8JZO9var2a7eJRqKN5To5FTgG2Kvd+j2npMEdCPxWklXAOcDTkrwfuC3JzgDt79q2/mpg957tdwNuHV24kiTNLRuKJEkaoqr6AnDHDFfvm+uk/Sjdpqoubr2IzuL+/CiShqiqTqiq3apqOV3D7b9V1YuA84Cj2mpHAR9v988DjkiyRZI96RpyvzzisCVJmjM2FEmSNBqvSnJFkvf2TLM9Va6TXdv9yeUbMA+KNGdOBg5Jch1wSHtMVV0NnAt8A7gAOLaq7pu3KCVJGjIbiiRJmnunAo8E9gPWAG9r5VPlOplRDhTo8qBU1YqqWrFs2bIhhCotXVX1uap6brt/e1UdXFV7tb939Kx3UlU9sqr2rqpPzV/EkiQNnw1FkiTNsaq6raruq6qfAe/m/qm0p8p1srrdn1wuSZIkzSkbiiRJmmMTCXGb5wFXtft9c520qbjvTnJAkgAv4f78KJIkLRpJdk/y70muSXJ1kte08u2TXJjkuvZ3u55tTkhyfZJrkzyzp/yJSa5sy97RjqGSNpENRZIkDVGSDwIXA3snWZ3kaOAt7cT1CuCpwB/BtLlOXgm8hy7B9Q2Aw1skSYvRvcDrquqXgAOAY9usoMcDF1XVXsBF7bEzhkojsPkgG7dpRO8G7gPuraoVSbYHPgQsB1YBL6iqO9v6JwBHt/VfXVWfHuT1JUkaN1V1ZJ/i0zey/knASX3KVwL7DjE0SZLGTutFu6bdvzvJNXQTOBwGHNRWOxP4HHAcPTOGAjclmZgxdBVtxlCAJBMzhnqhRdpEw+hR9NSq2q+qVrTHs2n5lSRJkiQtYUmWA48HLgV2ao1IE41JO7bVBpox1NlCpenNxdCzw+hafGl/D+8pP6eq7qmqm+i60u+/4eaSZqJNsb02yVU9ZScm+XaSy9vt2T3LHMstSZKksZRkK+AjwGur6vsbW7VP2YxnDHW2UGl6Aw09o6t4n0lSwP+rqtOY1PKbpLfl95Kebfu28ELXyks3tpQ99thjwBA1iOXHf3LadVad/JwRRKI+zgDeCZw1qfztVfXW3oJJPfp2AT6b5NEtF8rEWO5LgPPpevzZRVeSJEkjkeQBdI1EH6iqj7bi25Ls3H5T7gysbeXOGCrNsUF7FB1YVU8AnkWXdOwpG1l3Ri28YCuvNBNV9QXgjhmu3rdHXzvoblNVF1dV0TU6HT4nAUuSJEmTtN7spwPXVNUpPYvOA45q94/i/tk/nTFUmmMDNRRV1a3t71rgY3RDyW6bmAZ4hi2/kobrVUmuaEPTJqYRHWgstyRJkjRHDgReDDxtUvqEk4FDklwHHNIeO2OoNAKzHnqW5CHAL7TM9A8BngH8Jfe3/J7Mhi2/Zyc5hW7oy17AlweIXdKGTgXeRNdb703A24DfY8Cx3BMcFipJkqRhqqov0f+cFODgKbZxxlBpDg2So2gn4GMt7+3mwNlVdUGSrwDnJjkauBl4PnQtv0kmWn7vZf2WX0lDUFW3TdxP8m7gE+3hUMZytzxkpwGsWLFiygYlSZIkSdLCNOuGoqq6EXhcn/Lb2cSWX0nDMZHwrz18HjAxI1rfHn1VdV+Su5McQDcN6UuAvx913JIkSZKk8TDorGfStDOjOSva3EjyQeAgYIckq4E3Agcl2Y9u+Ngq4A9g2h59r6SbQW1LunHcjuWWJEmSpCXKhiJpgaqqI/sUn76R9R3LLUmSJEnaqIFmPZMkSZIkSdLiYUORJEmSJEmSABuKJEmSJEmS1NhQJEmSJEmSJMCGIkmSJEmSJDU2FEmSJEmSJAmwoUiSJEmSJEmNDUWSJEmSJEkCbCiSJEmSJElSY0ORJEmSJEmSABuKJEmSJEmS1Gw+3wFIkiRJ8yXJg4AvAFvQnRt/uKremGR74EPAcmAV8IKqurNtcwJwNHAf8Oqq+vQ8hK5m+fGfnHadVSc/ZwSRSNLiYI8iSZIkLWX3AE+rqscB+wGHJjkAOB64qKr2Ai5qj0myD3AE8FjgUOBdSTabj8AlSZoL9iiSJEnSklVVBfygPXxAuxVwGHBQKz8T+BxwXCs/p6ruAW5Kcj2wP3Dx6KJeOmbSW0iSNFwD9yhKslmSryX5RHu8fZILk1zX/m7Xs+4JSa5Pcm2SZw762pIkSdKg2vns5cBa4MKquhTYqarWALS/O7bVdwVu6dl8dSub/JzHJFmZZOW6devmNH5JkoZpGEPPXgNc0/PYbrqSJElaMKrqvqraD9gN2D/JvhtZPf2eos9znlZVK6pqxbJly4YUqSRJc2+ghqIkuwHPAd7TU3wYXfdc2t/De8rPqap7quomYKKbriRJkjTvquouuiFmhwK3JdkZoP1d21ZbDezes9luwK2ji1KSpLk1aI6ivwX+FNi6p2y9brpJervpXtKzXt9uutB11QWOAdhjjz0GDHFpcjy3JEnS9JIsA35aVXcl2RJ4OvBm4DzgKODk9vfjbZPzgLOTnALsAuwFfHnkgUuSNEdm3aMoyXOBtVV12Uw36VO2QTddsKuuJEmSRmZn4N+TXAF8hS5H0SfoGogOSXIdcEh7TFVdDZwLfAO4ADi2qu6bl8ilRSDJe5OsTXJVT9mJSb6d5PJ2e3bPsr55b5M8McmVbdk7kvT7/SlpBgbpUXQg8Fut0j4I2CbJ+2nddFtvIrvpSpIkaWxV1RXA4/uU3w4cPMU2JwEnzXFo0lJxBvBO4KxJ5W+vqrf2FkzKe7sL8Nkkj26NtafSjUq5BDifbgjpp+Y2dGlxmnWPoqo6oap2q6rldJX136rqRdzfTRc27KZ7RJItkuyJ3XQlSZIkaUmrqi8Ad8xw9b55b1sHhW2q6uKqKrpGp8PnJGBpCRjGrGeT2U1XkiRJkjSIVyW5og1N266V7Qrc0rPORN7bXdv9yeUbSHJMkpVJVq5bt24u4pYWvKE0FFXV56rque3+7VV1cFXt1f7e0bPeSVX1yKrau6rsBihJkiRJmuxU4JHAfsAa4G2tfKq8t+bDlYZoLnoUSZIkSZI0K1V1W1XdV1U/A94N7N8WTZX3dnW7P7lc0izYUCRJkiRJGhst59CE5wETM6L1zXtbVWuAu5Mc0GY7ewn358qVtIlsKJIkaYimmOZ3+yQXJrmu/d2uZ5nT/EqSlqwkHwQuBvZOsjrJ0cBb2jHwCuCpwB/BtHlvXwm8hy7B9Q0445k0a5vPdwCSJC0yZ7DhNL/HAxdV1clJjm+Pj3OaX0nSUldVR/YpPn0j658EnNSnfCWw7xBDk5YsexRJkjREU0zzexhwZrt/JvdP2es0v5IkSRorNhRJkjT3dmr5E2h/d2zlTvMrSZKksWJDkSRJ88dpfiVJkjRWzFEkSdLcuy3JzlW1pg0rW9vKneZXkhaR5cd/ctp1Vp38nBFEIkmzZ48iSZLm3nnAUe3+Udw/Za/T/EqSJGms2FAkLVBOwS2Npymm+T0ZOCTJdcAh7bHT/EqSJGnsOPRMWrjOwCm4pbEzxTS/AAdPsb7T/EqSJGls2KNIWqCcgluSJEmSNGz2KJIWl/Wm4E7SOwX3JT3rTUy1/VNmOAU3dNNw0/U+Yo899hhi2JKkYZsuqa4JdSVJUj/2KJKWhoGn4Aan4ZYkSZKkxc6GImlxua0NJ8MpuCVJkiRJm8qGImlxcQpuSZIkSdKszbqhKMmDknw5ydeTXJ3kL1r5Jk/PLWnTOQW3JEmSJGnYBklmfQ/wtKr6QZIHAF9K8ingt9n06bklbSKn4JYkSZIkDdusexRV5wft4QPardjE6bln+/qSJEmSJEkaroFyFCXZLMnldAlzL6yqS5k0PTfQOz33LT2bTzkNd5JjkqxMsnLdunWDhChJkiRJkqQZGmToGW3Y2H5JtgU+lmRjw1dmPA13VZ0GnAawYsWKKafqliRJknS/5cd/cqPLV538nBFFIklaqIYy61lV3QV8DjiUTZ+eW5IkSZIkSWNg1j2KkiwDflpVdyXZEng68Gbun577ZDacnvvsJKfQJbPeC/jyALEvadNdLZIkSdL0kuwOnAU8DPgZcFpV/V2S7YEPAcuBVcALqurOts0JwNHAfcCrq+rT8xC6JElzYpChZzsDZybZjK5n0rlV9YkkFwPntqm6bwaeD9303Ekmpue+l/Wn55YkSZLmw73A66rqq0m2Bi5LciHwUpzJV5K0BM26oaiqrgAe36f8djZxem5JkkbF/B2SerXJVyYmYrk7yTV0E64cBhzUVjuTLs3CcfTM5AvclGRiJt+LRxu5JElzYyg5iiRJkqSFLslyuguhA8/k6yy+kqSFyoYiSZIkLXlJtgI+Ary2qr6/sVX7lG0wS29VnVZVK6pqxbJly4YVpiRJc86GIkmSJC1pSR5A10j0gar6aCt2Jl9pBJK8N8naJFf1lG2f5MIk17W/2/UsOyHJ9UmuTfLMnvInJrmyLXtHkn6NupJmwIYiSZIkLVntx+TpwDVVdUrPoomZfGHDmXyPSLJFkj1xJl9pUGcAh04qO54umfxewEXtMZOSyR8KvKtNrgRwKnAMXZ3cq89zSpqhQWY9k2ZkusSxYPJYSZI0bw4EXgxcmeTyVvZ64GTGbCbfmZxTSQtNVX2h5QfrtUnJ5JOsArapqosBkpwFHA58ao7DlxYlG4okSZK0ZFXVl+ifdwicyVeaL+slk0/Sm0z+kp71JpLJ/7Tdn1y+gSTH0PU8Yo899hhy2NLi4NAzSZIkSdJCMFUy+RklmQcTzUszYUORJEmSJGmcbGoy+dXt/uRySbNgQ5EkSZIkaZxsUjL5Nkzt7iQHtAT1L+nZRtImMkeRJEmSpCXNyVfmT5IP0iWu3iHJauCNzC6Z/CvpZlDbki6JtYmspVmyoUiSJEmSNC+q6sgpFm1SMvmqWgnsO8TQpCXLhiJJkiRJmsZMeh1J0mJgjiJJkiRJkiQBNhRJkiRJkiSpsaFIkiRJkiRJgA1FkiRJkiRJambdUJRk9yT/nuSaJFcneU0r3z7JhUmua3+369nmhCTXJ7k2yTOH8QYkSZIkSZI0HIP0KLoXeF1V/RJwAHBskn2A44GLqmov4KL2mLbsCOCxwKHAu5JsNkjwkiRJkiRJGp5ZNxRV1Zqq+mq7fzdwDbArcBhwZlvtTODwdv8w4JyquqeqbgKuB/af7etLkiRJkiRpuIaSoyjJcuDxwKXATlW1BrrGJGDHttquwC09m61uZf2e75gkK5OsXLdu3TBClCRJkiRJ0jQGbihKshXwEeC1VfX9ja3ap6z6rVhVp1XViqpasWzZskFDlCRJkiRJ0gxsPsjGSR5A10j0gar6aCu+LcnOVbUmyc7A2la+Gti9Z/PdgFsHeX1JkhaSJKuAu4H7gHurakWS7YEPAcuBVcALqurOtv4JwNFt/VdX1afnIWxJ2sDy4z857TqrTn7OCCKRJA3bILOeBTgduKaqTulZdB5wVLt/FPDxnvIjkmyRZE9gL+DLs319SZIWqKdW1X5VtaI9dhIISZIkjY1Bhp4dCLwYeFqSy9vt2cDJwCFJrgMOaY+pqquBc4FvABcAx1bVfQNFL0nSwuckEJIkSRobsx56VlVfon/eIYCDp9jmJOCk2b6mJEkLXAGfSVLA/6uq05g0CUSS3kkgLunZtu8kEEmOAY4B2GOPPeYydkmSJC0BQ5n1TNJ4SbIqyZWtp9/KVrZ9kguTXNf+btez/glJrk9ybZJnzl/k0qJ3YFU9AXgWcGySp2xk3RlNAuEEEJIkSRqmgZJZSxprT62q7/Y8nsiDcnKS49vj4yblQdkF+GySRzs0VBq+qrq1/V2b5GN0Q8mcBEKS5thMkm9Lkjo2FElLx2HAQe3+mcDngOPoyYMC3JRkIg/KxfMQo7RoJXkI8AtVdXe7/wzgL7l/EoiT2XASiLOTnELXiOskEJIWFBtnJGlhsqFIWpyGngcFzIUiDWgn4GPdpKFsDpxdVRck+QpwbpKjgZuB50M3CUSSiUkg7sVJICRJkjQCNhRJi9OBVXVrawy6MMk3N7LujPKgQJcLBTgNYMWKFX3XkdRfVd0IPK5P+e04CYQkSZLGhMmspUWoNw8KsF4eFADzoEiSJEmS+rGhSFpkkjwkydYT9+nyoFzF/XlQYMM8KEck2SLJnpgHRZK0hCR5b5K1Sa7qKXOmUEnSkmVDkbT47AR8KcnX6Rp8PllVF9Alyj0kyXXAIe0xVXU1MJEH5QLMgyJJWlrOAA6dVDYxU+hewEXtMZNmCj0UeFeSzUYXqiRJc88cRdIiYx4USZJmrqq+kGT5pGJnCpUkLVn2KJIkSZLWt95MoUDvTKG39Kw35UyhkgaXZFWSK5NcnmRlK3NoqDTHbCiSJEmSZmbGM4UmOSbJyiQr161bN8dhSYvaU6tqv6pa0R47NFSaYzYUSZIkSesbeKbQqjqtqlZU1Yply5bNabDSEnMY3ZBQ2t/De8rPqap7quomYGJoqKRNZEORJEmStD5nCpXGQwGfSXJZkmNa2UBDQ+3tJ03PZNYaC8uP/+S066w6+TkjiESSJC0lST5Il7h6hySrgTfSzQx6bpKjgZuB50M3U2iSiZlC78WZQqW5dmBV3ZpkR+DCJN/cyLozGhpaVacBpwGsWLGi79BRaamzoUiSJElLVlUdOcUiZwqV5llV3dr+rk3yMbqhZLcl2bmq1sx2aKikjXPomSRJkiRprCR5SJKtJ+4DzwCuwqGh0pwbqKEoyXuTrE1yVU+Z0xVKkiRJkgaxE/ClJF+na/D5ZFVdQDc09JAk1wGHtMdU1dXAxNDQC3BoqDRrgw49OwN4J3BWT9nEdIUnJzm+PT5u0nSFuwCfTfJoK++GZpKvR5IkSZIWq6q6EXhcn/LbcWioNKcG6lFUVV8A7phU7HSFkiRJkiRJC9Bc5CgaaLpCcMpCSZIkSZKk+TDKZNYzmq4QuikLq2pFVa1YtmzZHIclSZIkSZIkmJuGotvaNIU4XaEkSZIkSdLCMRcNRU5XKEmSJEmStAANNOtZkg8CBwE7JFkNvJFuesJzkxwN3Aw8H7rpCpNMTFd4L05XKEmSJEmSNFYGaiiqqiOnWOR0hZIkSZIkSQvMKJNZS5IkSZIkaYwN1KNIkiRJkiRt3PLjP7nR5atOfs6IIpGmZ48iSZIkSZIkATYUSZIkSZIkqbGhSJIkSZIkSYANRZIkSZIkSWpsKJIkSZIkSRLgrGdaQKabKQCcLUCSJEmSpEHYUDRiM2nskCRJkiRJmg82FEmSJGle2WtYS8l0+7v7uqT5Zo4iSZIkSZIkATYUSZIkSZIkqbGhSJIkSZIkSYANRZIkSZIkSWpsKJIkSZIkSRIwDw1FSQ5Ncm2S65McP+rXl9SfdVMaT9ZNaTxZN6XxZN2UBrf5KF8syWbAPwCHAKuBryQ5r6q+MdvndDpVaXBzUTclDc66KY0n66Y0nqyb0nCMtKEI2B+4vqpuBEhyDnAYYMXVojNdI+aYNWBaN6XxZN2UxpN1UxpP1k1pCEbdULQrcEvP49XAr444Bkkbsm5K48m6KY2nodfNmfSSl0ZtAY7e8LgpDcGoG4rSp6w2WCk5BjimPfxBkmsHetE3b/ImOwDfHeQ1h2AcYoAFFscs/tdDj2GmZhjrw4f1etNYKHWzn3HZR6ezUOKEhRPrnMS50OrmsOslzPgzGMf9ZNxiGut45viYOROb9Pks8bo5tH1pDP7vMzVu9WcUdgC+u4D+R8CSrJsb7JvD+J/Nwf99IdUhY50bM4m1b90cdUPRamD3nse7AbdOXqmqTgNOG1VQkyVZWVUr5uv1xyUG4xi/GObQgqib/SyU/8tCiRMWTqwLJc4BTVs356tejuPnP24xGc/GjVs8m2ikdXOBf1az4nvWLM153Vwo/6eFEicY61wZJNZRz3r2FWCvJHsmeSBwBHDeiGOQtCHrpjSerJvSeLJuSuPJuikNwUh7FFXVvUleBXwa2Ax4b1VdPcoYJG3IuimNJ+umNJ6sm9J4sm5KwzHqoWdU1fnA+aN+3U00DkNrxiEGMI5e4xDDnFkgdbOfhfJ/WShxwsKJdaHEOZAxrpvj+PmPW0zGs3HjFs8mGXHdXNCf1Sz5njUrI6ibC+X/tFDiBGOdK7MfYlm1Qb5aSZIkSZIkLUGjzlEkSZIkSZKkMWVD0UYk+ZMklWSHeXr9v0nyzSRXJPlYkm1H+NqHJrk2yfVJjh/V606KYfck/57kmiRXJ3nNfMTRYtksydeSfGK+YliKptsPkzwmycVJ7knyJ5OWvSbJVW3fee08x/nCVo+vSPKfSR43023HKM73Jlmb5Kq5jHGQOMfpO2OhG7Du/VH7/K9K8sEkDxpBPCOtY+O2jw7y+bTlQz/GDfg/2zbJh9OdA12T5EnDimuhyjyeE47aKI+L48Bj13ga9Ht1lGZaZ5L8SpL7kvzOKOObFMO0sSY5KMnlrT58ftQx9sQx3T7wi0n+NcnXW6wvm6c4N3qOns472vu4IskTZvTEVeWtz41uWsVPA98CdpinGJ4BbN7uvxl484hedzPgBuARwAOBrwP7zMP73xl4Qru/NfBf8xFHe/0/Bs4GPjEfr78UbzPZD4EdgV8BTgL+pKd8X+Aq4MF0udg+C+w1j3E+Gdiu3X8WcOlMtx2HONvjpwBPAK4ag//7VJ/n2HxnLOTbgHVvV+AmYMv2+FzgpfO4Twy9jo3bPjpo3W5lQz3GDeH75kzg5e3+A4Ft57tezPeNeTonnIf3ORbnoCN+zx67xuw2jO/VcYq1Z71/o8vd9DvjGiuwLfANYI/2eMcxjvX1E9/FwDLgDuCB8xDrRs/RgWcDnwICHDDTfdUeRVN7O/CnwLwlcaqqz1TVve3hJcBuI3rp/YHrq+rGqvoJcA5w2Ihe++eqak1VfbXdvxu4hu5HyEgl2Q14DvCeUb/2EjftflhVa6vqK8BPJ237S8AlVfWjVoc+DzxvHuP8z6q6sz3srcujrGuDxElVfYHuADjXZh3nuHxnLAKD1D3oGme3TLI5XWPtrSOIZ5R1bNz20YHq9hwd42YdU5Jt6E56T2/r/aSq7hpibAvSPJ4TjtpYnIOOkseusTTQ9+qIzbTO/CHwEWDtKIObZCax/k/go1V1M3TnGyOOccJMYi1g6yQBtqI7T76XEZvBOfphwFnVuQTYNsnO0z2vDUV9JPkt4NtV9fX5jqXH79G1BI7CrsAtPY9XM88HrCTLgccDl87Dy/8tXaPhz+bhtZeyQfbDq4CnJHlokgfTtaTvPuT4JmxqnEdzf10eZV0bJM5RGkqc8/ydsdDNer+sqm8DbwVuBtYA36uqz4w4nrmuY+O2jw4az98y/GPcIDE9AlgHvK8Nh3tPkocMMbbFYJTnhKM2duego+Sxa2wslHMmmEGsSXalu2D6jyOMq5+ZfK6PBrZL8rkklyV5yciiW99MYn0n3cXpW4ErgddU1Tj+XpzV9+rmcxbOmEvyWeBhfRb9GV03smfMdxxV9fG2zp/RtU5+YBQx0XVLm2zeelYl2YquBfy1VfX9Eb/2c4G1VXVZkoNG+dqa/X5YVdckeTNwIfADuu6ic9XCP+M4kzyV7mTi1zZ12yEYJM5RGjjO+fzOWCRmvV8m2Y7uytWewF3APyd5UVW9fxTxjKiOjds+Out45vAYN8hntDldF/o/rKpLk/wdcDzwf4YY31ga03PCURurc9BR8tg1VhbKORPMLNa/BY6rqvu6zi/zZiaxbg48ETgY2BK4OMklVfVfcx3cJDOJ9ZnA5cDTgEcCFyb54hjW31l9ry7ZhqKqenq/8iT/g+4E9+utIu0GfDXJ/lX1nVHF0RPPUcBzgYOrDTIcgdWs3/tiNwYfOjArSR5Ad9D8QFV9dB5COBD4rSTPBh4EbJPk/VX1onmIZakZaD+sqtNpQxeS/HV7vrkwoziT/DLd0I5nVdXtm7LtGMQ5SgPFOQbfGYvBIPvl04GbqmodQJKP0uVxGKShaNzq2Ljto4PEM1fHuEH/Z6uraqJHxYfpGooWvTE9Jxy1sTkHHSWPXWNnoZwzwcxiXQGc037b7gA8O8m9VfUvI4nwfjOJdTXw3ar6IfDDJF8AHkeXu2uUZhLry4CT2/fx9UluAh4DfHk0Ic7Y7L5Xax6SQy2kG7CK+UtmfShdMq9lI37dzYEb6RrMJpJ3PXYe3n+As4C/ne/9oMVzECazHuXnPeP9EDiRnoS6rWzH9ncP4Ju0hIPzEWeL4XrgybN9j/MZZ8/y5cx9MutBPs+x+s5YqLdB6h7wq8DVdLmJQpeU+A/ncZ8Yeh0bt310GHW7rXMQw0tmPVBMwBeBvXv2sb8ZZR0YxxvzdE44D+9zLM5BR/yePXaN2W1Y36vjEuuk9c9g/pJZz+Rz/SXgorbug+nSSew7prGeCpzY7u8EfJv5azdYztTJrJ/D+smsvzyT51yyPYoWiHcCW9B1Y4MuOe8r5vpFq+reJK+im/VtM+C9VXX1XL9uHwcCLwauTHJ5K3t9VZ0/D7FoxKbaD5O8oi3/xyQPA1YC2wA/S/JauhkJvg98JMlD6ZLtHlv3JxwceZzAnwMPBd7V6vK9VbVilHVtkDgBknyQ7ofkDklWA2+srtfWOMXpd8YQDFj3Lk3yYeCrdMNjvgacNtfxMMI6Nm776KB1ey4MIaY/BD6Q5IF0J+rzMuXwmJmXc8JRG6Nz0FHy2DVmxvF7dcBYx8JMYq0ufcQFwBV0ufPeU1V9p32f71iBNwFnJLmSrhHmuKr67qhj7XeODjygJ87z6fK1Xg/8iBkeU9NamSRJkiRJkrTEOeuZJEmSJEmSABuKJEmSJEmS1NhQJEmSJEmSJMCGIkmSJEmSJDU2FGlWkrw0yZfmOw5pGJKckeSv5uF1X5nktiQ/aDO0LWhJ/jHJ/5nBelcnOWgjyz+V5KhhxqalYSHX5cnH1SSV5FHDi1JaGJJ8LsnLR/yaMzp+TbHt0OrqfLx3LQ5J9k7ytSR3J3n1PLz+8lYXnFV9kbChaJFJckSSS5P8MMnadv9/pc3bKC0USVa1H14P6Sl7eZLPzWNYM5bkyUn+rR2wv5fkX5Ps07P8AcApwDOqaququr0dYH/Yfmx+O8kpSTabv3dxvyQnJPlCn/Idkvwkyb5V9YqqetN0z1VVj62qz7XtT0zy/knLn1VVZw4teM0r63J+kOSueXsD0hxpdfvHbR+/Lcn7kmw133EBJDmoTRPdW3Zikp9O1Mkk/5nkSQC9x69+20pzZYj16E+Bz1XV1lX1jmHHualao+d/t/f13SQfTbLzfMcFP//Mn74J6x+W5PIk32/v5aIky9uyDc5jFwsbihaRJK8D/g74G+BhwE7AK4ADgQfOY2jrGZcfvloQNgdeM99BbIokm7UTz88AHwd2AfYEvg78R5JHtFV3Ah4EXD3pKR5XVVsBBwP/E/j9TXz9ubqS80/Ak5PsOan8CODKqrpqjl5Xi8OSrcvttu2IwpZG7TfbMesJwK8Ab+hdOIa9Cz7U4l0GfAn4qBdTNQaGUY8ezobHoRnp9/xDqruvau/r0cC2wNvn6HVmZDav1XoLngW8DvhFuvOAdwE/m6+YRsWGokUiyS8Cfwn8r6r6cFXdXZ2vVdULq+qeJFskeWuSm1uL9T8m2bJtf1CS1Ule13oirUnysp7nf2iS81pL6peBR056/cckuTDJHUmuTfKCnmVnJDk1yflJfgg8dTSfihaBvwH+JMm2vYXp0701Pd210w3h+I8kb29XDW9svQJemuSWto9PHtq0Q9uH707y+SQP73nuTd2/3wKcVVV/1+riHVX1BuAS4MQkjwaubU9xV5J/m/zGq+qbwBeBfdvrPLddzZi4CvrLPTGsSnJckiuAHybZvD3+dns/1yY5uK27RZK/TXJru/1tki3asim/B6pqNfBvwIsnhfoS4Myez+Kv2v0dknyixXtHki8m+YWeeJ+e5FDg9cDvprvi9PUp/pdfat9ddya5Kcmzet77nkm+0N7nZ5P8QxbplZ0FbsnW5Unv9xeTnJVkXZJvJXnDRL2Y7Xbt8RPb/Re1z3Of9vjlSf6l5/39Vc9zrtdrotXLE5J8o9W19yV5UFs2ZX2WAKrq28CngH3bPnhskuuA6wCS/H6S69v+c16SXSa2TXJIkm+m67H3TiA9y9a7Wj/5OyPJ9m1fvbXtt/+Srvfip4Bdcn+vvp+/Xov3p3THrocBD52oH1Ntm67h+PVJbmjfLZcl2b3nKZ+e5LoWwz8k6X0Pv5fkmrbs05O+k6Z871p6ZlCP+p4LtmPPU4F3tn320ZnZ777jknwHeF+rax9O8v4k3wde2o49p6c7H/x2qyObtefYrD3/d5PcCDxnI+/rDuAj3H9O2++89bfSpSa4K915wC9NbL+x49PGPpcpXuuDwB7Av7bP6k+TfDLJH/bGnOSKJIcD+wE3VdVF7bf13VX1kaq6OVOfx76s1fm70523/EHP827w2c94BxkxD/KLx5OALeiuek7lzXQtuvsBjwJ2Bf68Z/nD6FpKdwWOBv4hyXZt2T8A/w3sDPxeuwHQDqoXAmcDOwJHAu9K8tie5/6fwEnA1nRXcKSZWAl8DviTWWz7q8AVwEPp9s1z6K7SPAp4Ed3BtLdr7wuBNwE7AJcDH4BZ7d//CTwZ+Oc+MZ0LHFJV/wVMbL9tVT1t8orpfuj9OvC1JE8A3gv8QXs//w84L62BpzmS7iC9LV1D7quAX6mqrYFnAqvaen8GHED3PfA4YH/Wv3K1se+BM+lpKEqyd3ueD/Z5r68DVtNdtd2J7kBavStU1QXAX9Ou8FbV4/o8D3T/y2vp/jdvAU7vOQk/G/hy+1xOZMOGLI2HJVuXJ/l7uvr1COA36BpaX7bRLabf7vPAQe3+U4Ab2zoTjz8/g+ef8EK674tH0p0vTHw3TFuftbS1RpNnA19rRYfT1d19kjwN+L/AC+jOI79FV49JsgPdD8g30NXZG+h6ws/UPwEPpquHOwJvr6ofAs8Cbu3p1XfrpHi3AF4KrK6q706Ub2TbP6b7zng2sA3defCPep7yuXTfS49r7/OZ7XUOp6svv01Xf75IO2YO4b1rkZmmHk15LtiOPV+k9eBpx6aZ/O7bnq4n0jGt7DDgw3Tnkh+gO++7t23/eOAZwEQOrd+n2+8fD6wAfmcj72sH4P/reV+w/nnrI+jqxWvp6sn5dA05vSNi+h6fNvUcuaqOBG6m9eKqqre09/minngf1z6v84GvAo9Jd8Hqqb3nGxs5j13bPptt6I7Vb29xTuj32Y+fqvK2CG50O/d3JpX9J3AX8GO6k8YfAo/sWf4kuhZS6E4yfwxs3rN8Ld0Pys2AnwKP6Vn218CX2v3fBb446bX/H/DGdv8Muiuy8/45eVs4N7qGjafTXX34Ht2B4+V0PzaX0/1I6d1fPwe8vN1/KXBdz7L/0dbfqafsdmC/dv8M4JyeZVsB9wG7b+r+DezWXusxfd7TocBP2/1+76GA7wN30p0w/hVdg/6pwJsmPde1wG/0fFa/17PsUa3+Ph14wKTtbgCe3fP4mcCqdn/K74F2/8Etvie3xycBH+9Z9wzgr9r9v6RruH7UVP/bdv9E4P2Tlk/+X17fs+zB7XN6GN0VoXuBB/csf//k5/NmXWb+6vJd7fYOumPpPcA+Pev9AV1OiYn3+qVJz/GoGWx3NHBeu39N+2zPaY+/BTyh5/39Vc9zHET3I7n3//SKnsfPBm5o96esz96W7q3tMz9o+/i36IZjbNn23af1rHc68Jaex1vRnVcup2v0vKRnWegaJSe+A06k5zu9t77RNTr9DNiuT2zr7d89z/WTFu9aul6yT2zLfl4/ptj2WuCwKT6HAn6t5/G5wPHt/qeAo3uW/QJdA9PDp3vv3pbGbRPq0XTngp/rqTdh+t99PwEe1LP8ROALPY93ojv2bNlTdiTw7+3+v7H+MeMZ9BwLWzw/au/r23QNT8t63nPveev/Ac7tefwLbZuDetaf6vi0SefIPWVP73m8BXAHsFd7/FbgXT3LD2j1eh1dx4kzgK16PreNnncC/wK8ZqrPflxv9ihaPG6n627/8+77VfXk6vIi3E5X2R8MXNa65d0FXEB3wv7z56iqe3se/4juYL6M7oB8S8+yb/XcfzjwqxPP2577hXQ/5Cb0bivNWHW5bz4BHL+Jm97Wc//H7bkml/X2Qvj5PlpVP6A7YOzCpu/fd9KduPZL2Lcz8N0+5b2eUFXbVdUjq+oNVfWzFsPrJsWwe4uvX/zX012VORFYm+Sc3N/tfhfWr7/fmvQ8U30PUFU/outd8ZLWo+eFtGFnffwNcD3wmdbtdlP/f72+M3GnxUCLaRfgjp4y8LtmbC3Rurxtu72arsfAA9mw/u06zfNMt93ngV9P8jC6RqUPAQemS7T5i3S9qmZq8nF+4rthmPVZi8vhbR9/eFX9r6r6cSvv3ZfWO+60enk73T68C+vX2WLm3+O70x0D7tyEeM9t8e5YVU+rqss24bVu2Mjy7/Tc//lxk+575+96vnPuoPsRP+h71+Iyk3o0k3PBCcuY/nffuqr670nbTX69BwBrep7j/9H13INJ+y/rH6MmvLq9r12rS4WyborXmvwd8bO2fNcp1u89Pm3SOXI/VXUPXUPQi9INqz6SrrfixPJLquoFVbWMrrf/U+h66PeV5FlJLkk31PYuuoatHXpW6ffZjx0bihaPi+lafQ+bYvl36U6mH9tz4vqL1SUYm846uqv2vWOx9+i5fwvw+Z7n3ba67nev7FmnZv5WpA28ka6L68QB44ft74N71nkYg/n5/t26lW4P3Mom7t/VdVu/GHh+n9d4AXDRLGK7BThpUgwPrqreIV/r1bGqOruqfo3uAFp0XZBp7+nhPavu0cpm6ky693EI3fCcT/Rbqbox3K+rqkcAvwn8cVqepMmrbsJrT7YG2D5J736w+1Qrayws5br8XbpeFJPr37cH2a41DP8IeDXd1eC76X60HkPXQ2ki4eYPmf5znnycv7W9xkzrszSh97t9veNOGwb6ULp9eA3r19mw/n64sf32FrpjwLbTvP6m6rftLUzKzzlDtwB/MOl7Z8uq+k+mf+9S7744k3PBCTP53ddvP5/8evcAO/Q8xzZVNTHcer39l/V/G27qe5v8HTFRF3qPj32PT8ziHLnPY+jOb19IN5nMj6rq4r5BV30F+Cgt39Lk52pD3j5C1ytpp9Zp43zWzz+2IH4X21C0SFTVXcBf0OVb+J0kWyX5hST7AQ+huyr6broxkjsCJNk1yTNn8Nz30VWIE5M8uOVOOapnlU8Aj07y4iQPaLdfSU8SMmkQ7YfQh+h+CNGuSHybruV/syS/x+xO4Ho9O8mvtfHQbwIurapbmN3+fTxwVJJXJ9k6yXbpksg+ia6ebqp3A69I8qvpPCTJc5Js3W/lJHsneVo7WP033cnCfW3xB4E3JFnWxoz/Od1wrZn6Il034tPohrf8ZIoYnpvkUe1g//32+vf1WfU2YHlmkRi3qr5Fl/vmxCQPTDdD1W9u6vNodJZyXW7H0nOBk9pzPZwu78lG698Mt/s8XV6yiXxEn5v0GLqeRc9Ol/z3YXS9Dic7NsluSbany6vyIdik+iz1czbwsiT7tePSX9PVy1XAJ4HHJvntdL3iX836jUGXA09Jske6iVtOmFhQVWvohna9q9XNByR5Slt8G12S6l+cRbz9tn0P8KYke7Xj8C8neegMnusfgRPScqGlSw480fg83XuXes34XLBdIJjV776e51hDN+vn25Js035XPjLJb7RVzgVe3Y4Z27HpvYV7nQs8J8nBSR5AlxfvHro0KhP6Hp/YxHPk5ja6vEi97/diut/Lb6OnN1E7n/j9ns/xMcBv0U1qMfFcveexD6QbyrYOuDfdBCzP2KRPY0zYULSIVJeM64+BP6Ube30bXRfB4+gq2nF0XccvSZfN/rPA3jN8+lfRdaP9Dt24zPf1vO7ddBXgCLrW3e/Q9V7YYoNnkWbvL+kaPSf8PvC/6bqvP5b1DyazcTZdb4c7gCfSXVWY1f5dVV+iy/3z23RXXL5Fl+zv16rquk0NrKpW0r3fd9INh7meLp/JVLYATqa7ovQdum7Cr2/L/oquceUK4Eq6JH1/1ec5poql6KYJfXj7O5W96L5jfkDXK+NdVfW5PutNJAq+PclXZxpHjxfS/Wi/ne59fIju5ELja8nWZeAP6XpI3Eg3scPZdEk4B93u83Q9/L4wxWPoTny/Tpeb4TPcf5Ld6+y27MZ2m/humGl9ljZQVRfR5SD5CF09eiRdPaS6RNLPpztm3U63r/1Hz7YX0u2rVwCXsWEv1hfT9bj7Jt2572vbdt+kuzByY7rhKP2G50wVb79tT6H7MfsZusbS0+nyyEz3XB+j+545p517X0WXLHva9y71msW54CC/+ya8hK7h4xvtNT/M/cOx3w18mu648lW6TgWzUlXX0uXb/Xu6c9ffpEs23Xsxsu/xaRafC3TJ9d/Q6nfvJBtn0eVC7L0Qcxddw9CVSX5AN4TvY3STq8Ck89h2rvFquu+LO+kmyThv2g9hDKU755ckaeFL8iHgm1X1xvmORVpIkqyiS4T62fmORZKkCaM6PiV5CXBMS92w5NmjSJK0YLWhQ49sXaIPpcvT9i/zHJYkSZIWiHT5Lv8XXWoFYUORJGlhexhdPpYf0E1B/sqq+tq8RiRJkqQFoeVuWkeXtuXseQ5nbDj0TJIkSZIkSYA9iiRJkiRJktRsPt8BTGeHHXao5cuXz3cY0ry47LLLvltVy/otS/IguhlttqCryx+uqje2aSM/BCynm93mBVV1Z9vmBOBoummNX11Vn27lT6SbzW5L4HzgNTVNd0PrppayjdXN+WS91FJn3ZTGk3VTGk9T1c1pG4qSvBd4LrC2qvZtZR/i/un1tgXuqqr9kiwHrgGubcsuqapXtG02+YcowPLly1m5cuV0q0mLUpJvbWTxPcDTquoHSR4AfCnJp+imcb6oqk5OcjxwPHBckn3opqN9LLAL8Nkkj66q+4BTgWOAS+jq56HApzYWm3VTS9k0dXPeWC+11Fk3pfFk3ZTG01R1cyZDz86g+9H4c1X1u1W1X1XtB3wE+GjP4hsmlk00EjUTP0T3arf1nlPSpqnOD9rDB7Rb0c36dGYrPxM4vN0/DDinqu6pqpuA64H9k+wMbFNVF7fG27N6tpEkSZIkLSHTNhRV1ReAO/otSxLgBcAHN/Yc/hCV5kaSzZJcDqwFLqyqS4GdqmoNQPu7Y1t9V+CWns1Xt7Jd2/3J5f1e75gkK5OsXLdu3VDfiyRJkiRp/g2azPrXgduq6rqesj2TfC3J55P8eiub8Q9RSTNXVfe1nn270fUO2ncjq6ffU2ykvN/rnVZVK6pqxbJlYzfMXJIkSZI0oEGTWR/J+r2J1gB7VNXtLSfRvyR5LJvwQxS6Xgt0w9TYY489BgxRWvyq6q4kn6Mb0nlbkp2rak3rzbe2rbYa2L1ns92AW1v5bn3KJUmSJElLzKx7FCXZnC5p7ocmylruk9vb/cuAG4BHs4k/RO21IE0vybIk27b7WwJPB74JnAcc1VY7Cvh4u38ecESSLZLsSZcr7MtteNrdSQ5ow0lf0rONJEmSJGkJGaRH0dOBb1bVz4eUJVkG3FFV9yV5BN0P0Rur6o4kdyc5ALiU7ofo3w8SuCR2Bs5Mshldo++5VfWJJBcD5yY5GrgZeD5AVV2d5FzgG8C9wLFtxjOAV3L/rISfYpoZzyRJkiRJi9O0DUVJPggcBOyQZDXwxqo6nW6a7clJrJ8C/GWSe4H7gFdU1UQibH+ISkNUVVcAj+9Tfjtw8BTbnASc1Kd8JbCx/EaSJEmSpCVg2oaiqjpyivKX9in7CPCRKdb3h6iWlOXHf3Kjy1ed/JwRRSKpl3VT/Uy3X4D7hqSlZxTfjUneCzwXWFtV+7ayDwF7t1W2Be6qqv2SLAeuAa5tyy6pqle0bZ7I/R0Tzgde02bcHojHBy1FgyazliRJkiRpts4A3gmcNVFQVb87cT/J24Dv9ax/Q5v1d7JT6SZEuoSuoehQHMUizcqsk1lLkqQNJXlvkrVJruop+5sk30xyRZKPTSSib8tOSHJ9kmuTPLOn/IlJrmzL3tGSzUuStKhU1ReAO/ota8e+F7BhypPJ6+0MbFNVF7deRGcBhw85VGnJsKFIkqThOoPuKmavC4F9q+qXgf8CTgBIsg9dzr/Htm3e1RLUw/1XRvdqt8nPKUnSYvfrwG1VdV1P2Z5Jvpbk80l+vZXtSjfT9oTVrUzSLNhQJEnSEPW7MlpVn6mqe9vDS4Dd2v3DgHOq6p6qugm4HtjfK6OSJAFwJOv3JloD7FFVjwf+GDg7yTZAv163ffMTJTkmycokK9etWzf0gKXFwIYiSZJG6/e4P2fCrsAtPcsmroDO+MqoJ7ySpMUoyebAbwMfmihrF1Zub/cvA24AHk13nNytZ/PdgFv7PW9VnVZVK6pqxbJly+YqfGlBs6FIkqQRSfJnwL3AByaK+qxWGynfsNATXknS4vR04JtV9fMLJ0mWTQzRTvIIuqHZN1bVGuDuJAe0vEYvAT4+H0FLi4ENRZIkjUCSo+im/31hz3S9q4Hde1abuAI64yujkiQtZEk+CFwM7J1kdZKj26Ij2DCJ9VOAK5J8Hfgw8Iqqmhju/UrgPXTDuG/AGc+kWdt8vgOQJGmxS3IocBzwG1X1o55F59HlVzgF2IXuyuiXq+q+JHcnOQC4lO7K6N+POm5JS9Py4z857TqrTn7OCCLRUlBVR05R/tI+ZR8BPjLF+iuBfYcanLRE2VAkSdIQtSujBwE7JFkNvJFulrMtgAvbLPeXVNUrqurqJOcC36AbknZsVd3XnuqVdDOobUl3VdQro5IkSZpzNhRJkjREU1wZPX0j658EnNSn3CujkiRJGjlzFEmSJGnRS/LeJGuTXNVTtn2SC5Nc1/5u17PshCTXJ7k2yTN7yp+Y5Mq27B0tca4kSYuGDUWSJElaCs4ADp1UdjxwUVXtBVzUHpNkH7pEuo9t27xrYqYl4FTgGLqcYnv1eU5JkhY0G4okSZK06FXVF4A7JhUfBpzZ7p8JHN5Tfk5V3VNVN9HNorR/kp2Bbarq4jZ74Vk920iStCjYUCRJkqSlaqeqWgPQ/u7YyncFbulZb3Ur27Xdn1y+gSTHJFmZZOW6deuGHrgkSXPFhiJJkiRpff3yDtVGyjcsrDqtqlZU1Yply5YNNThJkuaSDUWSJElaqm5rw8lof9e28tXA7j3r7Qbc2sp361MuSdKiMW1D0RQzRJyY5NtJLm+3Z/csc4YISZIkLQTnAUe1+0cBH+8pPyLJFkn2pEta/eU2PO3uJAe0c9mX9GwjSdKiMJMeRWfQfzaHt1fVfu12PjhDhCRJksZTkg8CFwN7J1md5GjgZOCQJNcBh7THVNXVwLnAN4ALgGOr6r72VK8E3kOX4PoG4FMjfSOSJM2xzadboaq+kGT5DJ/v5zNEADclmZghYhVthgiAJBMzRHhglSRJ0pyrqiOnWHTwFOufBJzUp3wlsO8QQ5MkaawMkqPoVUmuaEPTtmtlA88QAc4SIc1Ekt2T/HuSa5JcneQ1rdyhoZIkSZKkWZltQ9GpwCOB/YA1wNta+cAzRICzREgzdC/wuqr6JeAA4Ng2/BMcGipJkiRJmoVZNRRV1W1VdV9V/Qx4N7B/W+QMEdKIVNWaqvpqu383cA0b6alHz9DQqrqJLrfC/m2Wl22q6uKqKmBiaKgkSZIkaYmZVUPRxDSizfOAiRnRnCFCmgctj9jjgUtb0ZwMDXVYqCRJkiQtbtM2FE0xQ8RbWj6TK4CnAn8EzhAhzYckWwEfAV5bVd9nDoeGOixUkiRJw9QubK5NclVPmTk3pXk0k1nP+s0QcfpG1neGCGlEkjyArpHoA1X1UeiGhvYsfzfwifbQoaGSJEkaN2cA76RLf9Dr7VX11t6CSTk3dwE+m+TRrXPCRM7NS4Dz6XJu2jlBmoVBZj2TNI/aVZLTgWuq6pSecoeGSpIkaUGoqi8Ad8xwdXNuSiMwbY8iSWPrQODFwJVJLm9lrweOTLIf3fCxVcAfQDc0NMnE0NB72XBo6BnAlnRXXrz6IkmSpPn0qiQvAVbSzfR7J10ezUt61pnIrflTNiHnJl3PI/bYY485CFta+GwokhaoqvoS/fMLnb+RbRwaKkmSpHF3KvAmugufb6LLufl7DCnnJnAawIoVK/quIy11Dj2TJGmIpkjKuX2SC5Nc1/5u17PMpJySJPWoqtuq6r6q+hnwbmD/tsicm9II2FAkSdJwnUGXQLPX8cBFVbUXcFF7PDkp56HAu5Js1raZSMq5V7tNfk5JkhYlc25K88uhZ5IkDVFVfSHJ8knFhwEHtftnAp8DjqMnKSdwU5KJpJyraEk5AZJMJOU0f5gkaVFJ8kG6Y+QOSVYDbwQOMuemNH9sKJIkae7t1K52UlVrkuzYygdOyilJ0kJWVUf2KT59I+ubc1OaYw49kyRp/gyclDPJMUlWJlm5bt26oQYnSZKkpceGIkmS5t5tE/kW2t+1rXzgpJxVdVpVraiqFcuWLRt64JIkSVpabCiSJGnunQcc1e4fxf0JNk3KKUmSpLFijiJJkoZoiqScJwPnJjkauBl4PpiUU5IkSePHhiJJkoZoiqScAAdPsb5JOSVJkjQ2HHomSZIkSZIkwIYiSZIkSZIkNTYUSZIkSZIkCbChSJIkSZIkSc20DUVJ3ptkbZKresr+Jsk3k1yR5GNJtm3ly5P8OMnl7faPPds8McmVSa5P8o423a8kSZI0r5L8UZKrk1yV5INJHpRk+yQXJrmu/d2uZ/0T2jnttUmeOZ+xS5I0bDOZ9ewM4J3AWT1lFwInVNW9Sd4MnAAc15bdUFX79XmeU4FjgEuA84FDcapfSZI0x5Yf/8n5DkFjLMmuwKuBfarqx0nOBY4A9gEuqqqTkxwPHA8cl2SftvyxwC7AZ5M8uqrum6e3IEnSUE3bo6iqvgDcMansM1V1b3t4CbDbxp4jyc7ANlV1cVUVXaPT4bOKWJIkSRquzYEtk2wOPBi4FTgMOLMtP5P7z10PA86pqnuq6ibgemD/0YYrSdLcGUaOot9j/Z5Beyb5WpLPJ/n1VrYrsLpnndWtrK8kxyRZmWTlunXrhhCiJEmStKGq+jbwVuBmYA3wvar6DLBTVa1p66wBdmyb7Arc0vMUfc9rPZ+VJC1UAzUUJfkz4F7gA61oDbBHVT0e+GPg7CTbAP3yEdVUz1tVp1XViqpasWzZskFClCRJkqbUcg8dBuxJN5TsIUletLFN+pRtcF7r+awkaaGadUNRkqOA5wIvbMPJaF1wb2/3LwNuAB5Nd6Wld3jabnRdeiVJkqT59HTgpqpaV1U/BT4KPBm4raVPmEijsLatvxrYvWd7z2ulATh5kjR+ZtVQlORQuuTVv1VVP+opX5Zks3b/EcBewI2tu+7dSQ5oFfYlwMcHjl6SJEkazM3AAUke3M5TDwauAc4DjmrrHMX9567nAUck2SLJnnTnu18ecczSYnIG3URHvS4E9q2qXwb+i27ypAk3VNV+7faKnvKJyZP2arfJzylphqZtKEryQeBiYO8kq5McTTcL2tbAhZNacp8CXJHk68CHgVdU1UQi7FcC76FL+HcDzngmDSTJ7kn+Pck1bUrf17TyTZ7O1yswkqSlqqoupTtv/SpwJd358WnAycAhSa4DDmmPqaqrgXOBbwAXAMc645k0e06eJI2fzadboaqO7FN8+hTrfgT4yBTLVgL7blJ0kjbmXuB1VfXVJFsDlyW5EHgpmz6d78QVmEuA8+muwNiYK0laEqrqjcAbJxXfQ9e7qN/6JwEnzXVckoBu8qQP9TzeM8nXgO8Db6iqL7KJkydJ2rhhzHomaR5U1Zqq+mq7fzddN/ld2cTpfL0CI0mSpHE0F5MnOSOhND0biqRFIMly4PHApWz6dL4zvgLjgVWSJEmjMFeTJzkjoTQ9G4qkBS7JVnRDPl9bVd/f2Kp9ymoj5RsWemCVJEnSHHPyJGl+2VAkLWBJHkDXSPSBqvpoK97U6XxnfAVGkiRJGiYnT5LGz7TJrCWNp3a15HTgmqo6pWfRxHS+J7PhdL5nJzmFLpn1XsCXq+q+JHcnOYBu6NpLgL8f0duQJEnSEubkSdL4saFIWrgOBF4MXJnk8lb2eroGonPb1ZibgedDN51vkonpfO9l/el8XwmcAWxJd/XFKzCSJEmStATZUKSxsPz4T067zqqTnzOCSBaOqvoS/fMLwSZO5+sVGGk0kvwR8HK6PGBXAi8DHkw37e9yYBXwgqq6s61/AnA0cB/w6qr69OijliRJ0lJijiJJkkYgya7Aq4EVVbUvsBlwBHA8cFFV7QVc1B6TZJ+2/LHAocC7JhJ4SpIkSXPFhiJJkkZnc2DLJJvT9SS6FTgMOLMtPxM4vN0/DDinTQV8E11yzv1HG64kSZKWGhuKJEkagar6NvBWutxha4DvVdVngJ3atL60vzu2TXYFbul5itWtbD1JjkmyMsnKdevWzeVbkCRJ0hJgQ5EkSSOQZDu6XkJ70s08+JAkL9rYJn3KaoOCqtOqakVVrVi2bNlwgpUkSdKSZUORJEmj8XTgpqpaV1U/BT4KPBm4LcnOAO3v2rb+amD3nu13oxuqJkmSJM0ZG4okSRqNm4EDkjw4SehmJ7wGOA84qq1zFPDxdv884IgkWyTZE9gL+PKIY5YkSdISs/l8ByBJ0lJQVZcm+TDwVeBe4GvAacBWwLlJjqZrTHp+W//qJOcC32jrH1tV981L8JIkSVoybCiSJGlEquqNwBsnFd9D17uo3/onASfNdVySJEnShGmHniV5b5K1Sa7qKds+yYVJrmt/t+tZdkKS65Ncm+SZPeVPTHJlW/aO1u1ekiRJkiRJY2ImOYrOAA6dVHY8cFFV7QVc1B6TZB/gCOCxbZt3JdmsbXMqcAxdjoW9+jynJEmSJEmS5tG0DUVV9QXgjknFhwFntvtnAof3lJ9TVfdU1U3A9cD+bRaXbarq4qoq4KyebSRJkiRJkjQGZjvr2U5VtQag/d2xle8K3NKz3upWtmu7P7lckiRJkiRJY2K2DUVT6Zd3qDZS3v9JkmOSrEyyct26dUMLTpIkSZIkSVObbUPRbW04Ge3v2la+Gti9Z73dgFtb+W59yvuqqtOqakVVrVi2bNksQ5QkSZIkjTMnT5LGz2wbis4Djmr3jwI+3lN+RJItkuxJl7T6y2142t1JDmgV9iU920iSJEmSlqYzcPIkaaxM21CU5IPAxcDeSVYnORo4GTgkyXXAIe0xVXU1cC7wDeAC4Niquq891SuB99AluL4B+NSQ34skSZK0yZJsm+TDSb6Z5JokT5pNjwZJm87Jk6Txs/l0K1TVkVMsOniK9U8CTupTvhLYd5OikyRJkube3wEXVNXvJHkg8GDg9XQ9Gk5Ocjxdj4bjJvVo2AX4bJJH91wclTS49SZPStI7edIlPetNTJL0U2Y4eVKSY+h6HrHHHnsMOWxpcRh2MmtJkiRpwUiyDfAU4HSAqvpJVd3FJvZoGGXM0hI28ORJ5sOVpmdDkSRJkpayRwDrgPcl+VqS9yR5CJN6NAC9PRpu6dm+b88FZ/GVBjKnkydJ2jgbiiRJkrSUbQ48ATi1qh4P/JCWOHcKM+q5YK8FaSBOniTNIxuKpAVqiqlET0zy7SSXt9uze5Y5lagkSRtaDayuqkvb4w/TNRxtao8GSbPg5EnS+Jk2mbWksXUG8E66WR16vb2q3tpbME3izYmpRC8BzqebStQDqyRpSaiq7yS5JcneVXUt3YQt32i3o+h+oE7u0XB2klPojql7AV8efeTS4uDkSdL4saFIWqCq6gtJls9w9Z8n3gRuSjIxlegq2lSiAEkmphK1oUiStJT8IfCBNuPZjcDL6Hren9t6N9wMPB+6Hg1JJno03Mv6PRokSVrwbCiSFp9XJXkJsBJ4XVXdyRCmEgWnE5UkLU5VdTmwos+iTerRIEnSYmCOImlxORV4JLAfsAZ4WysfeCpRMDGnJEmSJC12NhRJi0hV3VZV91XVz4B3A/u3RU4lKkmSJEmalg1F0iIyMTtL8zxgYkY0pxKVxkCSbZN8OMk3k1yT5ElJtk9yYZLr2t/tetbvO1uhJEmSNFdsKJIWqCmmEn1Lm+r+CuCpwB+BU4lKY+TvgAuq6jHA44BrgOOBi6pqL+Ci9njybIWHAu9Kstm8RC1JkqQlw2TW0gI1xVSip29kfacSleZRkm2ApwAvBaiqnwA/SXIYcFBb7Uzgc8BxTDFbIV0DsSRJkjQn7FEkSdJoPAJYB7wvydeSvCfJQ4Cd2jBQ2t8d2/q7Arf0bN93VsIkxyRZmWTlunXr5vYdSJIkadGzoUiSpNHYHHgCcGpVPR74IW2Y2RRmNCuhsxFKkiRpmGwokiRpNFYDq6vq0vb4w3QNR7dNJKJvf9f2rN9vtkJJkiRpzthQJEnSCFTVd4Bbkuzdig6mSzB/HnBUKzuK+2ce7Dtb4QhDliRJ0hI062TW7UT3Qz1FjwD+HNgW+H26PAwAr6+q89s2JwBHA/cBr66qT8/29SVJWoD+EPhAkgcCNwIvo7toc26bufBm4PnQzVaYZGK2wntZf7ZCSZIkaU7MuqGoqq4F9gNo0/V+G/gY3Unv26vqrb3rT5rmdxfgs0ke7UmvJGmpqKrLgRV9Fh08xfp9ZyuUJEmS5sqwhp4dDNxQVd/ayDo/n+a3qm4CJqb5lSRJkiRJ0hgYVkPREcAHex6/KskVSd6bZLtWNqNpfsGpfiVJkiRJkubDwA1FLc/CbwH/3IpOBR5JNyxtDfC2iVX7bL7BNL/gVL+SJEmStJQl2TvJ5T237yd5bZITk3y7p/zZPduckOT6JNcmeeZ8xi8tZLPOUdTjWcBXq+o2gIm/AEneDXyiPXSaX0mSJEnStMyJK82fYQw9O5KeYWdJdu5Z9jzgqnbfaX4lSZIkSZvKnLjSCA3UUJTkwcAhwEd7it+S5MokVwBPBf4Iuml+gYlpfi/AaX4lSZIkSdMbWk5c8+FK0xuooaiqflRVD62q7/WUvbiq/kdV/XJV/VZVrelZdlJVPbKq9q6qTw3y2pIkSZKkxW3YOXHNhytNb1iznkmSJEmSNGwb5MStqvuq6mfAu7l/eJk5caUhGUYya0mSJEkauuXHf3LadVad/JwRRKJ5tEFO3J5RK5Nz4p6d5BS6ZNbmxJVmyYYiSZIkSdLY6cmJ+wc9xW9Jsh/dsLJVE8uq6uokEzlx78WcuNKs2VAkSZIkSRo7VfUj4KGTyl68kfVPAk6a67ikxc4cRZIkSVrykmyW5GtJPtEeb5/kwiTXtb/b9ax7QpLrk1yb5JnzF7UkScNnQ5EkSZIErwGu6Xl8PHBRVe0FXNQek2Qfuqm6HwscCrwryWYjjlWSpDljQ5EkSZKWtCS7Ac8B3tNTfBhwZrt/JnB4T/k5VXVPVd0EXM/9sy5JkrTg2VAkLVBJ3ptkbZKreso2uZt8kicmubIte0eSjPq9SJI0z/4W+FPgZz1lO03MrNT+7tjKdwVu6VlvdStbT5JjkqxMsnLdunVzErQkSXPBhiJp4TqDrst7r9l0kz8VOIZuCtG9+jynJEmLVpLnAmur6rKZbtKnrDYoqDqtqlZU1Yply5YNFKMkSaPkrGfSAlVVX0iyfFLxYcBB7f6ZwOeA4+jpJg/clOR6YP8kq4BtqupigCRn0XWt/9Qchy9JGsDy4z857TqrTn7OCCJZFA4EfivJs4EHAdskeT9wW5Kdq2pNkp2BtW391cDuPdvvBtw60oglSZpD9iiSFpdN7Sa/a7s/ubwvu9FLkhabqjqhqnarquV0vW//rapeBJwHHNVWOwr4eLt/HnBEki2S7EnXG/fLIw5bkqQ5Y0ORtDRM1U1+Rt3nf77AbvTSwJyCW1owTgYOSXIdcEh7TFVdDZwLfAO4ADi2qu6btyglSRoyG4qkxeW21j2eGXaTX93uTy6XNHecglsaU1X1uap6brt/e1UdXFV7tb939Kx3UlU9sqr2riqHa0uSFhVzFEmLy0Q3+ZPZsJv82UlOAXahdZOvqvuS3J3kAOBS4CXA348+bGlp6JmC+yTgj1vxJuUWAy4eYciSJEkaM3Odq9AeRdICleSDdD8Y906yOsnRzK6b/CuB9wDXAzdgImtpLv0tTsEtSZKkMWaPImmBqqojp1h08BTrn0TXi2Fy+Upg3yGGJqmP3im4kxw0k036lPWdghs4DWDFihVT5hiTJEmSZmKghqI2tfbdwH3AvVW1Isn2wIeA5cAq4AVVdWdb/wTg6Lb+q6vq04O8viRJC4hTcEuSpCVjuuFRgwyN0twaxtCzp1bVflW1oj02KackSZM4BbckSZIWgrnIUXQYXTJO2t/De8rPqap7quomunwo+8/B60uStJA4BbckSX0kWZXkyiSXJ1nZyrZPcmGS69rf7XrWPyHJ9UmuTfLM+YtcWtgGbSgq4DNJLktyTCsbKCknmJhTkrS4OQW3JEkz5ggWacQGbSg6sKqeADwLODbJUzay7oySckKXmLOqVlTVimXLlg0YoiRJkiRpkXAEizTHBmooqqpb29+1wMfoKuJtLRknJuWUJEmSJM3S0EewOHpFmt6sG4qSPCTJ1hP3gWcAV2FSTkmSJEnS4IY+gsXRK9L0Nh9g252AjyWZeJ6zq+qCJF8Bzk1yNHAz8HzoknImmUjKeS8m5ZQkSZIkTaF3BEuS9UawVNUaR7BIc2PWDUVVdSPwuD7ltwMHT7HNScBJs31NSZIkSdLi10at/EJV3d0zguUvuX8Ey8lsOILl7CSnALvgCBZp1gbpUSRJkiRJ0lxwBIs0T2wokiRJkiSNFUewSPNnoFnPJEmSJEmStHjYUCRJkiRJkiTAhiJJkiRJkiQ1NhRJkiRJkiQJsKFIkiRJkiRJjQ1FkiRJkiRJAmwokiRJkiRJUmNDkSRJkpasJLsn+fck1yS5OslrWvn2SS5Mcl37u13PNickuT7JtUmeOX/RS5I0fDYUSZIkaSm7F3hdVf0ScABwbJJ9gOOBi6pqL+Ci9pi27AjgscChwLuSbDYvkUuSNAc2n+8AJA1fklXA3cB9wL1VtSLJ9sCHgOXAKuAFVXVnW/8E4Oi2/qur6tPzELYkSSNXVWuANe3+3UmuAXYFDgMOaqudCXwOOK6Vn1NV9wA3Jbke2B+4eJA4lh//yWnXWXXycwZ5CUmSZsQeRdLi9dSq2q+qVrTHXhmVJGkjkiwHHg9cCuzUGpEmGpN2bKvtCtzSs9nqViZJ0qJgQ5G0dBxGd0WU9vfwnvJzquqeqroJmLgyKmmIzIMijbckWwEfAV5bVd/f2Kp9yqrP8x2TZGWSlevWrRtWmJIkzTkbiqTFqYDPJLksyTGtbOAro570SgMxD4o0ppI8gK6R6ANV9dFWfFuSndvynYG1rXw1sHvP5rsBt05+zqo6rapWVNWKZcuWzV3wkiQN2awbijZyZfTEJN9Ocnm7PbtnG6+MSqNxYFU9AXgW3Y/Rp2xk3RldGQVPeqVBVNWaqvpqu3830JsHxd5+0jxJEuB04JqqOqVn0XnAUe3+UcDHe8qPSLJFkj2BvYAvjypeSZLm2iDJrCeujH41ydbAZUkubMveXlVv7V150pXRXYDPJnl0Vd03QAyS+qiqW9vftUk+Rvfj8rYkO1fVmtlcGZU0PBvLg5Kkt7ffJT2b9e3t13oNHgOwxx57zGHU0qJ1IPBi4Mokl7ey1wMnA+cmORq4GXg+QFVdneRc4Bt058PHej4rSVpMZt2jaCNXRqfilVFpBJI8pDXekuQhwDOAq/DKqDQWhp0HxZ5+0mCq6ktVlar65TYJxH5VdX5V3V5VB1fVXu3vHT3bnFRVj6yqvavqU/MZv7RYOYJFmj+D9Cj6uUlXRg8EXpXkJcBKul5HdzLDK6OSBrYT8LGuJz2bA2dX1QVJvoJXRqV5tbE8KPb2kyRpPY5gkebJwMms+1wZPRV4JLAfsAZ428SqfTbvmwfFhLnS7FXVjVX1uHZ7bFWd1Mq9MirNI/OgSJI0c45gkebPQA1F/a6MVtVtVXVfVf0MeDf3V84ZXxm1G70kaRGayIPytEnd5U8GDklyHXBIe0xVXQ1M9Pa7AHv7SZKWqEkjWKAbwXJFkvcm2a6VzWgmXzslSNMbZNazvldGJ6YRbZ5HlxsFvDIqSVrCzIMiSdKmG/YIFjslSNMbJEfRVDNEHJlkP7pKuQr4AzAPiiRJkjTXlh//yY0uX3Xyc0YUiTS4qUaw9Cx/N/CJ9tDcftKQzLqhqKq+RP9W2/M3ss1JwEmzfU1JkiRJ0uK3sREsVbWmPZw8guXsJKfQJbN2BIs0S0OZ9UySJC0M0/U2AHscSJLGgiNYpHliQ5EkSZIkaaw4gkWaPwPNeiZJkiRJkqTFw4YiSZIkSZIkATYUSZIkSZIkqTFHkSRJGlsm35YkSRotexRJkiRJkiQJsEeRJEmS5pk9xyRJGh/2KJIkSZIkSRJgQ5EkSZIkSZKaBT/0zK7KkiRJkiRJw2GPIkmSJEmSJAE2FEmSJEmSJKmxoUiSJEmSJEmADUWSJEmSJElqbCiSJEmSJEkSMA8NRUkOTXJtkuuTHD/q15fUn3VTGk/WTWk8WTel8WTdlAY30oaiJJsB/wA8C9gHODLJPqOMQdKGrJvSeLJuSuPJuimNJ+umNBybj/j19geur6obAZKcAxwGfGPEcUha39Dr5vLjPzntOqtOfs5sn15aKjxuSuPJuimNJ+umNASjbijaFbil5/Fq4Fcnr5TkGOCY9vAHSa6dtMoOwHdn+qJ58yZGOVybFOs8Gvs42/9x7ONspo1zhvvlw4cRzAwMq272GtZnMCoLZd+ChRUrLKx4d8ibZxTr2NTNTayXM7KJdXPe/7+j+i7Jm+f/vW6KAT+XsXuvC+24OY91c6P/u2HVlwGeZ+j71hC/A8Zuv+8xtrHN8Bzdujme/8N5janP5+ZnNDMzimmQ4+aoG4rSp6w2KKg6DThtyidJVlbVimEGNlcWSqzGOVwLJc4eQ6mb6z3hAvsMFlK8CylWWFjxjmGs09bNTamXc2EMP7M543tVj7Gtm+P+vxvn+Ixt9sYoPuvmJhi3mMYtHli6MY06mfVqYPeex7sBt444Bkkbsm5K48m6KY0n66Y0nqyb0hCMuqHoK8BeSfZM8kDgCOC8EccgaUPWTWk8WTel8WTdlMaTdVMagpEOPauqe5O8Cvg0sBnw3qq6ehZPNW9d7GdhocRqnMO1UOIEhlo3ey2oz4CFFe9CihUWVrxjFesc1c1hG6vPbI75XgWMfd0c9//dOMdnbLM3FvFZNzfZuMU0bvHAEo0pVRukIZEkSZIkSdISNOqhZ5IkSZIkSRpTNhRJkiRJkiQJGMOGoiSHJrk2yfVJju+z/DFJLk5yT5I/mbRs2yQfTvLNJNckedK4xZlk7ySX99y+n+S14xZnW/ZHSa5OclWSDyZ50FzFOYRYX9PivHouP88ZxvnCJFe0238medxMt10oZvAZJMk72vIrkjxhptuOWayrklzZ6urKuY51hvFurB6M22e7sVhH+tlabwczyP96IRpkf1loZvBeD2vv8/IkK5P82nzEqc44HyPG+Zgw7seAAeOb789uyu+IxXz8nO3+no38FkxyYpJv9yx79pBj2uT9PMn2SS5Mcl37u90oYkqye5J/T/eb/uokr+nZZtaf01zUtXn8jOZsXwKgqsbmRpdw7AbgEcADga8D+0xaZ0fgV4CTgD+ZtOxM4OXt/gOBbccxzknP8x3g4eMWJ7ArcBOwZXt8LvDScfzfA/sCVwEPpkvQ/llgr3mM88nAdu3+s4BLZ7rtQrjN8DN4NvApIMAB8/UZDBJrW7YK2GHMPtup6sE4frYb+74e2WdrvZ3f//VCuw2yvyy02wzf61bcn9Pyl4FvznfcS/U2zseIcT4mjPsxYNDvnDH47Pp+R4zis5uv26D7+6Tn+flvQeDEqdadr/0ceAtwfLt/PPDmEcW0M/CEdn9r4L96YprV5zRXdW2+PqO52pcmbuPWo2h/4PqqurGqfgKcAxzWu0JVra2qrwA/7S1Psg3wFOD0tt5PququcYtzkoOBG6rqW2Ma5+bAlkk2p2uEuXWO4hw01l8CLqmqH1XVvcDngefNY5z/WVV3toeXALvNdNsFYibv4zDgrOpcAmybZOcZbjsusc6HQerB2H22M/weHAXr7WAW0v96GAbZXxaambzXH1Q76wQeAjgLyvwZ52PEOH9PjPsxYJy/cwb5jljMx89x/C04V/v5YXSdMWh/Dx9FTFW1pqq+2u7fDVxD15FhEHNV1+blM5pk6O0K49ZQtCtwS8/j1cx8h3gEsA54X5KvJXlPkocMO8BmkDh7HQF8cCgR9TfrOKvq28BbgZuBNcD3quozQ4/wfoN8plcBT0ny0CQPpushsvuQ45uwqXEeTddbZTbbjquZvI+p1hn1ZzBIrNCd7HwmyWVJjpmzKGcWy1xuOxuDvt4oP1vr7WCW2mcwyP6y0MzovSZ5XpJvAp8Efm9EsWlD43yMGOdjwrgfAwb9zpn3z26K74jFfOyYy9+Cr2pDjN67iUOY5mo/36mq1kDXeEPXU2oUMf1ckuXA44FLe4pn8znNVV2b98+I4e5LwPg1FKVP2UyvXG0OPAE4taoeD/yQruvXXBgkzu4JkgcCvwX881AimuJl+pTNKM62Mx0G7AnsAjwkyYuGGNsGL9mnbEaxVtU1wJuBC4EL6Lrs3Tu80NYz4ziTPJWuMh+3qduOuZm8j6nWGfVnMEisAAdW1RPounkem+Qpwwyuj0E+n3H8bDdmlJ+t9XYwS+0zGGR/WWhm9F6r6mNV9Ri6q6RvmuugNKVxPkaM8zFh3I8Bg37nzPtnN8V3xGI+dszVb8FTgUcC+9FdqH/bXMQ0wv184ONpkq2AjwCvrarvt+LZfk7jWNeG8RkNe18Cxq+haDXr9wTZjZkPd1oNrK6qiZbGD9M1HM2FQeKc8Czgq1V129Ci2tAgcT4duKmq1lXVT4GP0o2PnCsDfaZVdXpVPaGqngLcAVw35PgmzCjOJL8MvAc4rKpu35RtF4CZvI+p1hn1ZzBIrFTVxN+1wMfouofOpUG/A8fts53SiD9b6+1gltpnMMj+stBs0v+2qr4APDLJDnMdmPoa52PEOB8Txv0YMNB3zjh8dj2x9H5HLOZjx5z8Fqyq26rqvqr6GfBuNu1/OVf7+W0TKRna37UjiokkD6BrJPpAVX10onyAz2mu6tq8fUbNsPclYPwair4C7JVkz9YydgRw3kw2rKrvALck2bsVHQx8Y27CnH2cPY5kboedwWBx3gwckOTBSUL3eV4zR3HCgJ9pkh3b3z2A32buPttp42wxfBR4cVX916Zsu0DM5H2cB7wknQPohi6umeG2YxFrkock2RqgDWN9Bt0wx7k0yOczjp9tX/Pw2VpvB7PUPoNB9peFZibv9VHtPIB0s0I+EFioDWML3TgfI8b5mDDux4BZxzcmn91U3xGL+dgxJ78Fs36OzOexaf/LudrPzwOOavePAj4+ipjaPnU6cE1VnTJpm9l+TnNV1+blM+ox7H2pU3OYEX42N7r8Mv9Fl/37z1rZK4BXtPsPo2t5+z5wV7u/TVu2H7ASuAL4F1p28DGM88F0X6C/OOaf518A32w71j8BW4xxrF+kaxj8OnDwPMf5HuBO4PJ2W7mxbRfibQafQYB/aMuvBFbM12cw21jp8p59vd2uHtX/a8B6MG6fbd9Y5+Oztd7O3365EG+D7C8L7TaD93pcq6eXAxcDvzbfMS/l2zgfI8b5mDDux4DZxjcmn92U3xGj+Ozm6zZgXez7W5Du99aVdL9lzwN2nu/9HHgocBHdaI2LgO1HtG//Gt0QrCt6lj170M9pLurafH1Gc7kvVdXPpzKUJEmSJEnSEjduQ88kSZIkSZI0T2wokiRJkiRJEmBDkSRJkiRJkhobiiRJkiRJkgTYULRoJdkjyQ+SbDbL7U9M8v5hxyUJkpyR5K/m4XVfmeS29t3w0E3Y7oVJPjOXsUmSRqsdCx4xi+1emuRLcxHTFK83ozhne4ybwfP+epJrh/V8krQQ2FA0BpKsSvKTJDtMKr88SSVZvqnPWVU3V9VWVXVfe67PJXn5ADEeneSbSe5uB+FPJtm6LdukH72jPsGQZqLVw9uSPKSn7OVJPjePYc1Ykicn+bdWR7+X5F+T7NOz/AHAKcAzgH2Ab7WT6R+075kf9jz+9d7nrqoPVNUzRvuOpOFr9fzHbT+/Lcn7kmw133FtTJLlrY5uPt+xaGGaar9v54k3tnWGdgEjyWOTfCbJnUnuSnJZkmfPcNsNzld749zIdj8/xrX1bx8g/kryqJ7X/2JV7T3b55v03Ie18/vvJ/lukotmc54vzYf2G+7KJD9K8p0kpybZdsivkSSvTnJVOzddneSfk/yPYb6OpmdD0fi4CThy4kGrDFvO5omGfTKZ5DeAvwaOrKqtgV8Czh3ma0hjYnPgNfMdxKZIslmSJwGfAT4O7ALsCXwd+I+eq7A7AQ8Cru5pSN6qqiZ+JD+up+yLPc/vj1MtNr/Z9vsnAL8CvGGe45mS9U9DNMr9/l+BC+mOOzsCrwa+P4evBz3HuDl+nVlrjU9nAa8DfpHuWP0u4GdDfI0k8fedhi7J64A3A/+bbv89AHg4cGGSBw7xpf6O7lz81cD2wKOBfwGes6lP5DF0MH6RjI9/Al7S8/gouoMJAEmek+Rr7QrELUlO7Fk2cbXx6CQ3A//WewUyyUnArwPvbFeT3tm2+7v2XN9vV3vW60XQ41eAi6vqawBVdUdVnVlVdyc5Bngh8Kftuf+1PffxSW5ovRu+keR5rfyXgH8EntTWv6uVr3cFqbfXUTvovT3J2tZT4ook+87+o5am9DfAn0y+OtLvin7vPtv21/9o++ldSW5M18Pnpa2OrU1y1KTX2iHJha2OfD7Jw3ue+zFt2R1Jrk3ygp5lZ7QrOOcn+SHw/7f373GSlvWd//96BxRFJYIMBBlwUEcjsB4nhMSsISIBxXVwN5pxPaCSzOpi1ERXB81vdZNMfmMOnpJgMh7iGBFkPQRWIhExxJggOCjKOYyCMDIyI4jiIejg5/vHfbUUPdUz3V3V1TXdr+fjUY+uuu77rvtT1fdV912fug6/BvwJ8IGqekdV3dnq6O8DnwfenORRwESz+TuSfGaqN2DSa7m9bX+vVoDtvXhle53fSvKnExemSR7ZXs932rIPz+D9l0amqr4BfBL4T0k+kWRbuhYQn0iydGK9dvx/rdXVG5I8v5VPeazvoo78TJLfT/L19tnwgSQ/25btcD4HPtue9o523vylEb1FWoB6jvsj27H2yJ1cyx2S5GOtbtw2cf04IcmftTpzQ5Knt7L96RIg766qH7Xbv1bVxDXdvlPVt0x9vfrTFj5JnpHuuvLOJN9I8tqpznHZyXVuuh9Z3pB7rlUva693or59ucXwm0mOSbK5Z9vHpLsGuCPJVUme1bPs/Un+Kl3L+zuTXJLkEW3x44EbqurC6txZVR+tqpt2FlNb9stJvtA+b76Q5Jd79nlRkrVJ/hX4AfDw7OQ6QpqpJPsA/wf4nao6v6p+XFU3As+lSxb9VrpWi/u39X8/yfa2HUn+KMnb2/0p60iS5cCpdI0TPlNVd1XVD1rL9nVtnRl9Jx7RW7QwVZW3eb4BNwJPozvJPQbYA7iZruIVsAw4BvhPdMm9xwK3Aie17Ze19T4APICuJdJE2Z5tnYuA35q03xcAD6FrRfEa4JvA/dqyNwMfbPf/M/BDug+IJwN7TXqe9wN/NKnsOXQtG34G+E3g+8BBbdmLgc9NWv9e8fWuAxwPXAY8GEh7jw6a7/+bt4V166mHH5s4noHfasfmvepTW/bTY7Ydr9uBl7T6+0fATcBfAXvRdfe6E3hgW//97fFT2vJ39BzvD2j1/yWtbj4R+BZwRM+232l18WeAvYG7gV/r85peAmxp93d4DT3rFfDISa/ld9r+7z+5zrb1/4nul55DgX/veS/OBN7YYrsf8Cvz/b/15m3iNlHP2/1D6Fof/AXw31pdehDwf4G/b+s8gK4lxKPb44N66uKUx/ou6shLgU3Aw4EHts+cv2vLJurplOdzb95mepviuP/DSZ/976fnWo7uXPZl4G3tWPzpMd7OCT8Gfrut93LgFrprtADXA58ATgIOnBTLQ6aqb235Rex4vdob5xbgP7f7+wJPbPd3qCfs/Dr3fwFXAI9uMT8OeMjk/bXHxwCb2/37tPr7BuC+wFPpzueP7nkfbweOavs9AzirLXs48B/tPf012jVBz376xkT3OfJt4IXtOZ/XHj+k5z27CTiiLf9ZdnId4c3bTG/ACXTXhv2uITfQnQ8/C/y3VvYp4KvA09vjzwLPbvd3VkdeBnx9F7Ecwwy+E8/3e7c732xRNF4mWhUdB1wLfGNiQVVdVFVXVNVPquordBXyVydt/+aq+n5V/XA6O6uqD1bVbVW1var+nO4L6w59sKvrhvJf6U405wG3JXlrdjJQdlX936q6pcX7YbqLhqOmE1cfP6a7mPh5IFV1TVVtmeVzSbvyv4HfSbJkhtvdUFV/W924YB+muxj/g+p+DfkU8CPgkT3rn1dVn62qu+i+bP5S++XwmcCN7bm2V9UXgY8Cv9Gz7TnV/UL7E7oLyJ+hu3iebAuwf5/yXbmlqv6i7X+qz5O3VNdy6Sbg7dzTdfbHdEnuh1bVf1T7FVkaI3+frjXr54B/Bl5X3a/6P6iqO4G13Pv8+hO61hf3r6otVTXRtWVXx/pUdeT5wFur6mtV9T3gNGBV7t1Efkbnc2kaJh/3f7yL9Y+i+8Hvf7VjcfIx/vWqenc7522gS6IeWFVFlwS5EfhzYEuSz7aWArTrzp3Vt135MXB4kn2q6tvtHNnXLq5zfwv4/aq6rjpfrumNa3Q0XYJ3XXWtpT5DlxR7Xs86H6uqS6tqO92X4Me3eL5G9yX3YLohHL7VWldMdAGfKqYTgeur6u/aazmT7nvCf+nZ5/ur6qq2zxPY9XWENBP7A99qx9dkE9ea/wz8ajuXPRZ4Z3t8P7reKf/Ss03fOkKXGN3pd7y5+E6s/kwUjZe/A/473S81H+hdkOQXk/xTa6r7HbqM6+QvgDfPZGdJXpPkmtaM9Q66XyD6fqmsqk9W1X+h+1K6ssU45eDYSV6UbrC+O9pzHznVc+9KOwn/JV3rjFuTrJ9oyigNW1VdSXfRt2aGm97ac/+H7bkml/UOmvvT+tq+LN5Od1H+MOAXJ+pOqz/PB36u37Z0vyr+hO4ifbKD6H5FnKnpfJb0rvN1utgBXkf3S+ilrUn+S2exf2kunVRVD66qh1XV/6Tr4fw36bqCfZful88HJ9mjqr5P1yr2ZXRfeM9L8vPteXZ1rE9VRx7aHvcu25NujJV+20rDcK/jfhpfoA6hSwb1+2IIXescAKrqB+3uA9vjzVX1iqp6BN057fu069oke09V36b5Ov4b8Ay6CRn+OTvpirmL69xD6Fo8zNRDgZvbDzUTvk6X/JnwzZ77P6Dn3F9Vn6+q51bVEroW+0+h+7FoZzFN/szot8/ez4zpXEdIM/EtuiET+o35M3Gt+c90idAn0rWMu4AugXM0sKmqeq9Hp6ojt9H/evan5uI7sfozUTRGqurrdINaP4OuKXqvDwHnAodU1c/SjfOTyU+xs6fvfZCun/br6fqW7ltVD6brzjL5OSfH+JOqupCuz+fEOEGTn/thwLuBV9A1i30wcGXPc/eL8/t0zZAn3OtkVlXvrKon0TWrfRRd81xprryJrkn9xEXY99vfKY/RWThk4k77NXE/uqb7NwP/3C7oJ24PrKqX92z70zrUvsheTNfdc7LnAhfOIradfZbsED9d15pbWjzfrKrfrqqHAv8DOD09s8dIY+g1dK0MfrGq9qH74gbtnFVV/1hVx9FdvF5Ld36bzrHet460vw+btGw790421xT3pbky+Ti7GTh0ii+G03/SqpvpfuibuGbcaX3rE8fk5/tCVa2kGyT775licpVpXOfeDDyi37a7cAtwSO49YPSh9PQCmK6q+gLd9f7EezNVTJM/M/rts/d9m851hDQTFwN30fUw+al0MwU/ne5a89/o6vaz6Y6/q+mO0xPpkkjTcSGwNMmKnawz6HdiTZOJovFzCvDU9uWv14OA26vqP5IcRdfyaCZupesb3ft824FtwJ5J/jfQt5VOuqk8V6UbgDBt/79KN1Buv+d+AF0F3da2fwn3nAQn1l+ae4+QfznwX9svTY+kex8m9v8LLXt8H7ov7P9BNyaLNCeqahNd97FXtsfb6C7IXpBusMmXMrsLzF7PSPIrrR78IXBJu6D+BPCoJC9Mcp92+4V0A8FPZQ1wcrrBcx/U6uofAb9EN7bYXPhfbT+H0M1O8WGAJM/JPQMBf5vus8D6qnH2ILoWf3ck2Y8uUQxAkgOTPKtdDN8FfI92PE/jWO9bR+iayf9uksNakviPgQ/vpOXGNrpWgw+fYrk0DJOv5S6l6wKyLskDktwvyZN39STtmP8/6QbJ/pl0g9u+lHuuGaesb1PE0fvc903y/CQ/W1U/phs/bKrzy66uc98D/GGS5e3a9rFJHrKrGIBL6K5FX9fOz8fQdQE7a4r1e+P/lSS/neSA9vjngWdxz3szVUz/QHdd8N/TTVLzm8DhdNcL/czmOkKaUlV9h+568i+SnNCOqWV0Y4xtphtn7wd0Y8qeyj2JoX+j+yFlWomiqrqebibAM9MNIn/f9tmzKslES/9BvxNrmkwUjZmq+mpVbeyz6H8Cf5DkTroxVGY6Pf07gN9IN8PEO4F/pJv14t/pmq/+B1M30/s2XeuK6+lOyh8E/rSqzmjL30vXX/yOJH/fMsh/Tpd9vpVuwLF/7Xm+z9ANpPjNJBPNEN9GN4bLrXR93c/oWX8ful9wv91ivQ34sxm+fmmm/oAu6Tnht+last1G17Lt3wZ8/g/RXSDfDjyJrlk4bcyGXwdW0f2K+E266Uj3muqJqhs34ni6X3q20NWTJ9ANPHr9gHFO5Ry6C4LL6cYue28r/wXgkiTfo/vF51VVdcMcxSANw9vpBo3+Ft0XtvN7lv0MXQuIW+jq6q/SnY9h18f6VHXkfXRdzT9L14r4P+gGj++rXXyvBf61nWePnuXrlHZm8rXc3XQJkEfSDZS8ma4b5q78iG5A2U/TXTNeSZdkfXFb/namrm+w4/XqZC8EbkzXbe1ldANW97Or69y30l1Lf6rF+d4WF3QTumxo78W9Zgurqh/RJXee3l7D6cCLquraKeLodUfb9or2uXE+8HG6mUunjKmNU/RMus+i2+i6vT5zUlee3hhnfB0h7UpV/QndIO5/Rnd8XkJXp46tbrxN6BJC96FLNE88fhD3zN45Ha/kniFH7qDrjvls4P+15YN+J9Y0pcqWWZKk6UtSwPLW8krSJNYRSZK0O7NFkSRJkiRJkgATRZIkSZIkSWrseiZJkiRJkiTAFkWSJEmSJElq9pzvAHZl//33r2XLls13GNK8uOyyy75VVUvmO45+rJtazMa1blovtdhZN6XxZN2UxtNUdXPsE0XLli1j48Z+s8VLC1+Sr893DFOxbmoxG9e6ab3UYmfdlMaTdVMaT1PVTbueSZIkSZIkCTBRJEmSJEmSpGaXiaIk70uyNcmVfZa9Nkkl2b+n7LQkm5Jcl+T4nvInJbmiLXtnkgzvZUiSJEmSJGlQ02lR9H7ghMmFSQ4BjgNu6ik7HFgFHNG2OT3JHm3xu4DVwPJ22+E5JUmSJEmSNH92mSiqqs8Ct/dZ9DbgdUD1lK0Ezqqqu6rqBmATcFSSg4B9quriqirgA8BJgwYvSZIkSZKk4ZnVGEVJngV8o6q+PGnRwcDNPY83t7KD2/3J5VM9/+okG5Ns3LZt22xClCRJkiRJ0gztOdMNkuwNvBH49X6L+5TVTsr7qqr1wHqAFStWTLneqC1bc95Ol9+47sQRRSJpd7WrzxHws0Rzy2NQknZ/fpaPju+1FqMZJ4qARwCHAV9u41EvBb6Y5Ci6lkKH9Ky7FLillS/tUy5JkiRJkqQxMeOuZ1V1RVUdUFXLqmoZXRLoiVX1TeBcYFWSvZIcRjdo9aVVtQW4M8nRbbazFwHnDO9lSJIkSZIkaVC7TBQlORO4GHh0ks1JTplq3aq6CjgbuBo4Hzi1qu5ui18OvIdugOuvAp8cMHZJkiRJkiQN0XRmPXteVR1UVfepqqVV9d5Jy5dV1bd6Hq+tqkdU1aOr6pM95Rur6si27BVt9jNJkhaUJIck+ack1yS5KsmrWvl+SS5Icn37u2/PNqcl2ZTkuiTH95Q/KckVbdk7W6tcSZIWjCTvS7I1yZU9ZR9Ocnm73Zjk8la+LMkPe5b9dc82njOlIZnVrGeSJGlK24HXVNVjgKOBU5McDqwBLqyq5cCF7TFt2SrgCOAE4PQke7Tnehewmq4r9/K2XJKkheT9TDq/VdVvVtXjq+rxwEeBj/Us/urEsqp6WU+550xpSEwUSZI0RFW1paq+2O7fCVwDHAysBDa01TYAJ7X7K4GzququqrqBrov2UUkOAvapqotbK9wP9GwjSdKCUFWfBW7vt6y1CnoucObOnsNzpjRcJookSZojSZYBTwAuAQ5skzvQ/h7QVjsYuLlns82t7OB2f3L55H2sTrIxycZt27YN/TVIkjSP/jNwa1Vd31N2WJIvJfnnJP+5lU3rnAmeN6XpMFEkSdIcSPJAuubyr66q7+5s1T5ltZPyexdUra+qFVW1YsmSJbMLVpKk8fQ87t2aaAtwaFU9Afg94ENJ9mGa50zwvClNx57zHYAkSQtNkvvQJYnOqKqJcRVuTXJQVW1pTeS3tvLNwCE9my8FbmnlS/uUS5K04CXZE/ivwJMmyqrqLuCudv+yJF8FHoXnTGmobFEkSdIQtfEU3gtcU1Vv7Vl0LnByu38ycE5P+aokeyU5jG4Azktb97Q7kxzdnvNFPdtIkrTQPQ24tqp+2qUsyZKJCR+SPJzunPk1z5nScJkokhagJL/bpuW+MsmZSe43m6m5Jc3Kk4EXAk/tmb73GcA64Lgk1wPHtcdU1VXA2cDVwPnAqVV1d3uulwPvoRvg+qvAJ0f6SiRJmmNJzgQuBh6dZHOSU9qiVew4iPVTgK8k+TLwEeBlVTUxELbnTGlI7HomLTBJDgZeCRxeVT9McjbdifZwuqm51yVZQzc19+snTc39UODTSR7V80VV0gxU1efoP1YCwLFTbLMWWNunfCNw5PCikyRpvFTV86Yof3Gfso/Sde3ut77nTGlIbFEkLUx7Avdvfbv3puujPaOpuUcbriRJkiRpHJgokhaYqvoG8GfATXQzQ3ynqj7FzKfm3oHTiUqSJEnSwmaiSFpg2thDK4HD6LqSPSDJC3a2SZ8ypxOVJEmSpEXIRJG08DwNuKGqtlXVj4GPAb9Mm5obYJpTc0uSJEmSFhkTRdLCcxNwdJK92/SgxwLXMMOpuUccsyRJ88bZQiVJuoeJImmBqapL6KYL/SJwBV09X8/spuaWJGlB65ktdEVVHQnsQTcb6Bq62UKXAxe2x0yaLfQE4PQke8xH7JIkzYU95zsAScNXVW8C3jSp+C5mODW3JEmLxMRsoT/mntlCTwOOacs3ABcBr6dntlDghiQTs4VePOKYJUmaE7tsUZTkfUm2Jrmyp+xPk1yb5CtJPp7kwT3L+jbFTfKkJFe0Ze9sXWIkSZKkeTNXs4U6U6gkaXc1na5n76drVtvrAuDIqnos8O90v7jsqinuu4DVdOOfLO/znJIkSdJIzdVsoc4UKknaXe0yUVRVnwVun1T2qara3h5+nm6WJOhpiltVNwCbgKPaDEv7VNXFVVXAB4CThvQaJEmSpNlytlBJknoMYzDrlwKfbPenaop7cLs/ubwvm+pKkiRpRJwtVJKkHgMNZp3kjcB24IyJoj6r1U7K+6qq9XSzNLFixYop15MkSZIGUVWXJJmYLXQ78CW669AHAmcnOYUumfSctv5VSSZmC92Os4VKkhaYWSeKkpwMPBM4tnUng6mb4m7mnu5pveWSJEnSvHK2UEmS7jGrrmdJTqCbHvRZVfWDnkV9m+K2mSLuTHJ0a9L7Iu5pvitJkiRJkqQxsMtEUZIzgYuBRyfZ3Jrf/iXwIOCCJJcn+WvomuICE01xz+feTXFfDryHboDrr3LPuEaSJEmSpEUoyfuSbE1yZU/Zm5N8o33XvDzJM3qWnZZkU5LrkhzfU/6kJFe0Ze9sDRQkzcIuu55V1fP6FL93J+v3bYpbVRuBI2cUnSRJkiRpIXs/XUOED0wqf1tV/VlvQZLDgVXAEcBDgU8neVRrnPAuYDXdrNz/AJyAjROkWRnGrGeSJEmSJM1YVX0WuH2aq68Ezqqqu6rqBrreKkclOQjYp6oubuPnfgA4aU4ClhYBE0WSJEmSpHHziiRfaV3T9m1lBwM396yzuZUd3O5PLt9BktVJNibZuG3btrmIW9rtmSiSJEmSJI2TdwGPAB4PbAH+vJX3G3eodlK+Y2HV+qpaUVUrlixZMoRQpYXHRJEkSZIkaWxU1a1VdXdV/QR4N3BUW7QZOKRn1aXALa18aZ9ySbNgokiSJEmSNDbamEMTng1MzIh2LrAqyV5JDgOWA5dW1RbgziRHt9nOXgScM9KgpQVkl7OeSZIkSZI0F5KcCRwD7J9kM/Am4Jgkj6frPnYj8D8AquqqJGcDVwPbgVPbjGcAL6ebQe3+dLOdOeOZNEsmiiRJkiRJ86Kqnten+L07WX8tsLZP+UbgyCGGJi1aJookaTe0bM15u1znxnUnjiASSZIkSQuJYxRJkiRJkiQJMFEkSZIkSZKkxkSRJEmSJEmSABNFkiRJkiRJakwUSZIkSZIkCTBRJEmSJEmSpMZEkSRJkiRJkgATRZIkDVWS9yXZmuTKnrI3J/lGksvb7Rk9y05LsinJdUmO7yl/UpIr2rJ3JsmoX4skSZIWn10miqa44N0vyQVJrm9/9+1Z5gWvJGkxez9wQp/yt1XV49vtHwCSHA6sAo5o25yeZI+2/ruA1cDyduv3nJIkSdJQTadF0fvZ8eJ0DXBhVS0HLmyPveCVJC16VfVZ4PZprr4SOKuq7qqqG4BNwFFJDgL2qaqLq6qADwAnzUnAkiRJUo9dJoqmuOBdCWxo9zdwz8WrF7ySJPX3iiRfaS11J1riHgzc3LPO5lZ2cLs/uXwHSVYn2Zhk47Zt2+YibkmSJC0isx2j6MCq2gLQ/h7Qyge+4AUveiVJC867gEcAjwe2AH/eyvt1w66dlO9YWLW+qlZU1YolS5YMIVRJkiQtZsMezHrgC17wolcaVJIHJ/lIkmuTXJPkl2Yztpik4aiqW6vq7qr6CfBu4Ki2aDNwSM+qS4FbWvnSPuWSJEnSnJptoujW1p2M9ndrK/eCVxoP7wDOr6qfBx4HXMPsxhaTNAQT58zm2cDEBBHnAquS7JXkMLox/C5trXXvTHJ0m/zhRcA5Iw1akqQRmGLypD9tP3h+JcnHkzy4lS9L8sOeWUT/umcbJ0+ShmS2iaJzgZPb/ZO55+LVC15pniXZB3gK8F6AqvpRVd3BDMcWG2XM0kKS5EzgYuDRSTYnOQX4k3bx+hXg14DfBaiqq4CzgauB84FTq+ru9lQvB95DVye/CnxytK9EkqSReD87TnR0AXBkVT0W+HfgtJ5lX+2ZRfRlPeVOniQNyZ67WqFd8B4D7J9kM/AmYB1wdrv4vQl4DnQXvEkmLni3s+MF7/uB+9Nd7HrBK82NhwPbgL9N8jjgMuBVTBpbLEnv2GKf79l+p2OISdq5qnpen+L37mT9tcDaPuUbgSOHGJokSWOnqj6bZNmksk/1PPw88Bs7e47eyZPa44nJk/zOKc3CLhNFU1zwAhw7xfpe8Erza0/gicDvVNUlSd5B62Y2hWmPIZZkNd0vNRx66KGDxilJkiTtykuBD/c8PizJl4DvAr9fVf/CDGcLxetZaad2mSiStNvZDGyuqkva44/QJYpuTXJQa000nbHFdlBV64H1ACtWrJhyQHqNh2Vrztvp8hvXnTiiSCRpvLXxT95D96Nm0X0xvY7uy+ky4EbguVX17bb+acApwN3AK6vqH0cetLQIJHkjXU+VM1rRFuDQqrotyZOAv09yBDOcLRSvZ6WdGvasZ5LmWVV9E7g5yaNb0bF03UFnNLbYCEOWJGm+OQmENGaSnAw8E3h+VRVAG1Pztnb/Mrox/B6FkydJQ2WLImlh+h3gjCT3Bb4GvIQuMTzTscUkSVrQeiaBeDF0k0AAP0qykm6cTugmgbgIeD09k0AANySZmATi4pEGLi1gSU6gq2+/WlU/6ClfAtxeVXcneTjdD5xfq6rbk9yZ5GjgErrJk/5iPmKXFgITRdICVFWXAyv6LJrR2GKSJC0CTgIhzaMpJk86DdgLuKDNcv/5NsPZU4A/SLKdruvny6rq9vZUTp4kDYmJIkmSJC1mczIJhAPmStMzk9lCq+qjwEenWObkSdKQOEaRJEmSFrN+k0A8kTYJBPx06u0ZTQJRVeurakVVrViyZMmcBS9J0rDZokiSJEmLVlV9M8nNSR5dVddxzyQQV9NN/rCOHSeB+FCStwIPxUkgtIDtagZVcBZVaSEyUSRJkqTFzkkgJElqTBRJkiRpUXMSCEmS7uEYRZIkSZIkSQJMFEmSJEmSJKkxUSRJkiRJkiTAMYokSZIk9XCmK0kab3P9OW2LIkmSJEmSJAEmiiRJkiRJktQMlChK8rtJrkpyZZIzk9wvyX5JLkhyffu7b8/6pyXZlOS6JMcPHr4kSZIkSZKGZdZjFCU5GHglcHhV/TDJ2cAq4HDgwqpal2QNsAZ4fZLD2/IjgIcCn07yqKq6e+BXIUmSJGm34lhIkjSeBu16tidw/yR7AnsDtwArgQ1t+QbgpHZ/JXBWVd1VVTcAm4CjBty/JEmSJEmShmTWiaKq+gbwZ8BNwBbgO1X1KeDAqtrS1tkCHNA2ORi4uecpNreyHSRZnWRjko3btm2bbYiSJEmSJEmagVknitrYQyuBw+i6kj0gyQt2tkmfsuq3YlWtr6oVVbViyZIlsw1RkiRJkjTGkrwvydYkV/aUzXjc2yRPSnJFW/bOJP2+f0qahkG6nj0NuKGqtlXVj4GPAb8M3JrkIID2d2tbfzNwSM/2S+m6qkmSJEmSFqf3AydMKltDN+7tcuDC9phJ496eAJyeZI+2zbuA1cDydpv8nJKmaZBE0U3A0Un2btnaY4FrgHOBk9s6JwPntPvnAquS7JXkMLrKe+kA+5ckSZIk7caq6rPA7ZOKZzTubWugsE9VXVxVBXygZxtJMzTrWc+q6pIkHwG+CGwHvgSsBx4InJ3kFLpk0nPa+le1mdGubuuf6oxnkiRJkqRJ7jXubZLecW8/37PexLi3P273J5fvIMlqupZHHHrooUMOW1oYZp0oAqiqNwFvmlR8F13ron7rrwXWDrJPSZIkSdKiNNW4tzMaD5eugQMrVqzou4602A3S9UySJEmSpGGb6bi3m9v9yeWSZsFEkSRJkiRpnMxo3NvWTe3OJEe38XNf1LONpBkaqOuZJEmSJEmzleRM4Bhg/ySb6YY2WcfMx719Od0MavcHPtlukmbBRJEkSZIkaV5U1fOmWDSjcW+raiNw5BBDkxYtu55JkiRJkiQJMFEkSZIkSZKkxkSRJEmSJEmSABNFkiQNVZL3Jdma5Mqesv2SXJDk+vZ3355lpyXZlOS6JMf3lD8pyRVt2TvbLC6SJEnSnDJRJEnScL0fOGFS2RrgwqpaDlzYHpPkcGAVcETb5vQke7Rt3gWsppv6d3mf55QkSZKGzkSRJElDVFWfBW6fVLwS2NDubwBO6ik/q6ruqqobgE3AUUkOAvapqourqoAP9GwjSZIkzRkTRdIClWSPJF9K8on2eMZdXyQNzYFVtQWg/T2glR8M3Nyz3uZWdnC7P7l8B0lWJ9mYZOO2bduGHrgkSZIWFxNF0sL1KuCansez6foiaW71G3eodlK+Y2HV+qpaUVUrlixZMtTgJEmStPiYKJIWoCRLgROB9/QUz6jry4hClRaLW1t3Mtrfra18M3BIz3pLgVta+dI+5ZIkSdKcMlEkLUxvB14H/KSnbKZdX3ZgFxdp1s4FTm73TwbO6SlflWSvJIfRDVp9aaujdyY5us129qKebSRJkqQ5Y6JIWmCSPBPYWlWXTXeTPmV2cZFmKcmZwMXAo5NsTnIKsA44Lsn1wHHtMVV1FXA2cDVwPnBqVd3dnurldK0CNwFfBT450hciLTKO7SdJUmfPQTZO8mC6i9gj6b5YvhS4DvgwsAy4EXhuVX27rX8acApwN/DKqvrHQfYvqa8nA89K8gzgfsA+ST5I6/pSVVum2fVF0ixU1fOmWHTsFOuvBdb2Kd9Id36VNBoTY/vt0x5PjO23Lsma9vj1k8b2eyjw6SSP6knySpK0WxsoUQS8Azi/qn4jyX2BvYE34ElVmjdVdRpwGkCSY4DXVtULkvwpXZeXdezY9eVDSd5KVzeXA5cOGseyNeftcp0b15046G4kSRpYz9h+a4Hfa8UrgWPa/Q3ARcDr6RnbD7ghycTYfhePMGRJkubMrBNFSfYBngK8GKCqfgT8KIknVWk8rQPObt1gbgKeA13XlyQTXV+2c++uL9KCs6skpglMaVF6O93Yfg/qKbvX2H5Jesf2+3zPen3H9kuyGlgNcOihh85ByJIkzY1Bxih6OLAN+NvWn/s9SR6AA+ZKY6OqLqqqZ7b7t1XVsVW1vP29vWe9tVX1iKp6dFU5DookadGYq7H9HNdPGkySRye5vOf23SSvTvLmJN/oKX9GzzaOHyYNwSCJoj2BJwLvqqonAN+n62Y2FQfMlSRJ0riZGNvvRuAs4Km9Y/sBOLafNHpVdV1VPb6qHg88CfgB8PG2+G0Ty6rqHwAmDXVyAnB6kj3mIXRptzdIomgzsLmqLmmPP0KXOPKkKkmSpN1CVZ1WVUurahndl8zPVNUL6MbwO7mtNnlsv1VJ9kpyGEMa20/STh0LfLWqvr6TdX461ElV3UA3a+hRI4lOWmBmPUZRVX0zyc1JHl1V19FV3qvbzQFzJUmStDtzbD9pfKwCzux5/IokLwI2Aq9ps2xPa/wwSbs26KxnvwOc0WY8+xrwErpWSp5Uteg5YK4kSdMzLj/8VdVFdBOxUFW30f0Q2m+9tXQzpEmaY+275rNos/oC7wL+kG4Ykz8E/hx4KdMc6sSB5qVdGyhRVFWXAyv6LPKkKkmSJEka1NOBL1bVrQATfwGSvBv4RHs4raFOqmo9sB5gxYoVfcfMlRa7QcYokiRJkiRpLj2Pnm5nE+PhNs8Grmz3HT9MGpJBu55JkiRJkjR0SfYGjgP+R0/xnyR5PF23shsnljnUiTQ8JookSZIkSWOnqn4APGRS2Qt3sr5DnUhDYKJIkiRJmmfjMqC3JEmOUSRJkiRJkiTARJEkSZIkSZIaE0WSJEmSJEkCTBRJkiRJkiSpMVEkSZIkSZIkwESRJEmSJEmSGhNFkiRJkiRJAkwUSZIkSZIkqTFRJEmSJEmSJMBEkSRJkiRJkhoTRZIkSZIkSQKGkChKskeSLyX5RHu8X5ILklzf/u7bs+5pSTYluS7J8YPuW5IkSZIkScMzjBZFrwKu6Xm8BriwqpYDF7bHJDkcWAUcAZwAnJ5kjyHsX5IkSZIkSUMwUKIoyVLgROA9PcUrgQ3t/gbgpJ7ys6rqrqq6AdgEHDXI/iVJkiRJkjQ8g7YoejvwOuAnPWUHVtUWgPb3gFZ+MHBzz3qbW5kkSZIkSfeS5MYkVyS5PMnGVuZQJ9Ic23O2GyZ5JrC1qi5Lcsx0NulTVlM892pgNcChhx462xC1wCxbc94u17lx3YkjiESSJEnSiPxaVX2r5/HEUCfrkqxpj18/aaiThwKfTvKoqrp79CFLu7dBWhQ9GXhWkhuBs4CnJvkgcGuSgwDa361t/c3AIT3bLwVu6ffEVbW+qlZU1YolS5YMEKIkSZIkaQFxqBNpjs06UVRVp1XV0qpaRpe5/UxVvQA4Fzi5rXYycE67fy6wKsleSQ4DlgOXzjpySZIkSdJCVsCnklzWep3AgEOdJFmdZGOSjdu2bZvD0KXd16y7nu3EOuDsJKcANwHPAaiqq5KcDVwNbAdOtRmgJEmSJGkKT66qW5IcAFyQ5NqdrDutoU6qaj2wHmDFihV9h0KRFruhJIqq6iLgonb/NuDYKdZbC6wdxj4lSdrdtO7adwJ3A9urakWS/YAPA8uAG4HnVtW32/qnAae09V9ZVf84D2FLkjQvquqW9ndrko/TdSW7NclBVbVltkOdSNq5QWc9kyRJM/NrVfX4qlrRHk8MyrkcuLA9ZtKgnCcApyfZYz4CliRp1JI8IMmDJu4Dvw5ciUOdSHPORJG0wCQ5JMk/JbkmyVVJXtXKnUpUGk8OyilJ0o4OBD6X5Mt0CZ/zqup8uqFOjktyPXBce0xVXQVMDHVyPg51Is3aXIxRJGl+bQdeU1VfbL/CXJbkAuDFOJWoNN8mBuUs4G/aOAn3GpSzjcMA3QCcn+/Ztu+gnJIkLURV9TXgcX3KHepEmmO2KJIWmKraUlVfbPfvBK6h+3JpqwVp/j25qp4IPB04NclTdrLutAbldPYWaTC2xJUk6d5MFEkLWJJlwBOASxhwKtH2fH4hlQbQOygncK9BOQFmMyhnVa2vqhVVtWLJkiVzGb60UE20xH0McDRdEvdwHD9MkrRImSiSFqgkDwQ+Cry6qr67s1X7lPWdKtQvpNLsOSinNJ5siStJ0r05RpG0ACW5D12S6Iyq+lgrdipRaX4dCHw8CXTn3w9V1flJvgCcneQU4CbgOdANyplkYlDO7TgopzTndtYSd6bjhyVZDawGOPTQQ+cwakmShstEkbTApPsW+l7gmqp6a8+iiVYL69ix1cKHkryVbjBrWy1Ic8BBOaXxNrklbkvq9l21T9kOLXHbYPXrAVasWNG3pa4kSePIRJG08DwZeCFwRZLLW9kb6BJEtlqQJGkSW+JKknQPE0XSAlNVn6P/r51gqwVJku7FlriSJN2biSJJkiQtZrbElSSph4kiSZIkLVq2xJUk6d5+Zr4DkCRJkiRJ0ngwUSRJkiRJkiTARJEkSZIkSZIaE0WSJEmSJEkCBkgUJTkkyT8luSbJVUle1cr3S3JBkuvb3317tjktyaYk1yU5fhgvQJIkSZIkScMxSIui7cBrquoxwNHAqUkOB9YAF1bVcuDC9pi2bBVwBHACcHqSPQYJXpIkSZK08OykYcKbk3wjyeXt9oyebWyYIA3BnrPdsKq2AFva/TuTXAMcDKwEjmmrbQAuAl7fys+qqruAG5JsAo4CLp5tDJIkSZKkBWmiYcIXkzwIuCzJBW3Z26rqz3pXntQw4aHAp5M8qqruHmnU0gIwlDGKkiwDngBcAhzYkkgTyaQD2moHAzf3bLa5lfV7vtVJNibZuG3btmGEKEmSJEnaTVTVlqr6Yrt/JzDRMGEqP22YUFU3ABMNEyTN0MCJoiQPBD4KvLqqvruzVfuUVb8Vq2p9Va2oqhVLliwZNERJkiRJ0m5qUsMEgFck+UqS9/WMiTuthgk2SpB2baBEUZL70CWJzqiqj7XiW5Mc1JYfBGxt5ZuBQ3o2XwrcMsj+JUmSJEkLV5+GCe8CHgE8nm4olD+fWLXP5js0TLBRgrRrg8x6FuC9wDVV9daeRecCJ7f7JwPn9JSvSrJXksOA5cCls92/JEmSJGnh6tcwoapuraq7q+onwLu5p3uZDROkIRmkRdGTgRcCT5004vw64Lgk1wPHtcdU1VXA2cDVwPnAqQ4sJkmSJEmabKqGCRO9V5pnA1e2+zZMkIZkkFnPPkf/5n0Ax06xzVpg7Wz3KUmSJElaFCYaJlyR5PJW9gbgeUkeT9et7Ebgf0DXMCHJRMOE7dgwQZq1WSeKJEmSJEmaCztpmPAPO9nGhgnSEAw865kkSZIkSZIWBhNFkiRJkiRJAkwUSZIkSZIkqTFRJEmSJEmSJMBEkSRJkiRJkhoTRZIkSZIkSQJMFEmSJEmSJKkxUSRJkiRJkiTARJEkSZIkSZIaE0WSJEmSJEkCTBRJkiRJkiSpMVEkSZIkSZIkwESRJEmSJEmSGhNFkiRJkiRJAuYhUZTkhCTXJdmUZM2o9y+pP+umNJ6sm9J4sm5K48m6KQ1uz1HuLMkewF8BxwGbgS8kObeqrh5lHJLuzbo5XpatOW++Q9CYsG5K48m6KY0n66Y0HCNNFAFHAZuq6msASc4CVgJWXGl+WTcXoekkpG5cd+IIItFOWDel8WTdlMaTdVMaglEnig4Gbu55vBn4xckrJVkNrG4Pv5fkukF2mrcMsvW97A98awT7GcROYxwDcxrfkP4HI3kPpxnrw+Y4jAm7S90ch+N7HGKA8ThOF+R7sbvVzWHXS5hx3VyQx8EAjOPehhaHdXMo1zmj/n/M1A7xjcn1NYxPneqnb2yjeu+mea1g3RyOcT4OZ8vXNEcGOW+OOlGUPmW1Q0HVemD93IczM0k2VtWK+Y5jZ8Y9xnGPD3aPGOfAblE3x+F/Mw4xjEsc4xDDOMUxR3ZZN62XxmEc82Ls62Y/4/7/GOf4jG125iG23bJuTtc4/69ny9c0nkY9mPVm4JCex0uBW0Ycg6QdWTel8WTdlMaTdVMaT9ZNaQhGnSj6ArA8yWFJ7gusAs4dcQySdmTdlMaTdVMaT9ZNaTxZN6UhGGnXs6ranuQVwD8CewDvq6qrRhnDgHaHJorjHuO4xwe7R4xDtRvVzXH434xDDDAecYxDDDA+cQzdblI3x+X9N457M445tJvUzX7G/f8xzvEZ2+yMNLbduG5O1zj/r2fL1zSGUrXDMCSSJEmSJElahEbd9UySJEmSJEljykSRJEmSJEmSABNFACQ5Icl1STYlWdNn+c8nuTjJXUleO2nZjUmuSHJ5ko3zGOPzk3yl3f4tyeOmu+2YxDjn7+M04lvZYrs8ycYkvzLdbTVaSV6bpJLsP0/7/9Mk17bj5eNJHjzCfc/7sZjkkCT/lOSaJFcledV8xNFi2SPJl5J8Yr5iWKjG5dw4Lue/cTrHjcv5bMA4RnL9tNDNRT1Nsl+SC5Jc3/7uO8rYkjy6xTRx+26SV7dlb07yjZ5lz5ij2Gb8uTLC961vbNnJuXlM3rc5Pd4WggHr86uSXNn+968eWdC7MBd1bb4N+Jrel2RrkitHG/UsVNWivtENcvZV4OHAfYEvA4dPWucA4BeAtcBrJy27Edh/DGL8ZWDfdv/pwCXT3Xa+YxzF+zjN+B7IPeN2PRa4dpTvobdp/y8PoRug8OtzXfd2EsOvA3u2+28B3jKi/Y7FsQgcBDyx3X8Q8O/zVSeA3wM+BHxiPva/UG/jcm4cl/PfOJ3jxuV8Nkgcw35PFuttruop8CfAmnZ/DbM4xw0a26Tn+SbwsPb4zVOtO+TYZvy5MsL3barYpjw3z/f7NtfH20K4DVJngCOBK4G96Sar+jSwfDd5TfP6HXaUr6k9fgrwRODK+X4tu7rZogiOAjZV1deq6kfAWcDK3hWqamtVfQH48XwEyPRi/Leq+nZ7+Hlg6XS3HYMYR2E68X2vWg0GHgDUdLfVSL0NeB33/H9Grqo+VVXb28NRHstjcSxW1Zaq+mK7fydwDXDwqONIshQ4EXjPqPe9CIzLuXFczn/jdI4bl/PZIHFoOOaqnq4ENrT7G4CT5jG2Y4GvVtXXZxHDILHN5nNlVO9b39hGcG6eq8/BYbxvC8EgdeYxwOer6gft+vSfgWePIuhdGJdz+DANVA+q6rPA7aMKdhAmiroP0Jt7Hm9mZh+qBXwqyWVJVg81snvMNMZTgE/OctvZGiRGmPv3cVrxJXl2kmuB84CXzmRbzb0kzwK+UVVfnu9YeryUex/Lc2nsjsUky4AnAJfMw+7fTpc0/Mk87HuhG5dz47ic/8bpHDcu57NB4oDRXD8tdHNVTw+sqi3QJR/oWjGMOrYJq4AzJ5W9onXpeN8suynN1efKfLxvkz9rgCnPzfP5vsHcHm8LwSB15krgKUkekmRv4Bl0LfDn27icw4dpKHV0d7DnfAcwBtKnbCa/eD25qm5JcgBwQZJrW6ZwmKYdY5JfozsgJ8YBGPT1TdcgMcLcv4/Tiq+qPg58PMlTgD8EnjbdbTUcST4N/FyfRW8E3kDX7Wte46iqc9o6bwS2A2eMIibG7FhM8kDgo8Crq+q7I973M4GtVXVZkmNGue9FYlzOjeNy/hunc9y4nM8GiQNGc/200I1LPZ2L2EhyX+BZwGk9xe+iO46q/f1z7p2AHGps83BdPehnzVTn5vl+38A6vyuzPraq6pokbwEuAL5H1x1q+863GolxrmuzNXAd3V3YoqjLAvZmXJcCt0x346q6pf3dCnycrjnasE0rxiSPpeuCsbKqbpvJtvMc4yjexxm9D+3E9Yh0gyWP6j0UUFVPq6ojJ9+ArwGHAV9OciPd/+GLSfolc+Ysjp4k0cnAM4Hn93StmGtjcywmuQ/dhegZVfWxeQjhycCz2rFwFvDUJB+chzgWqnE5N47L+W+cznHjcj4bJI5RXT8tdHNVT29NchBA+7t11LE1Twe+WFW39sR8a1XdXVU/Ad7N7I6bufpcGdn7NtVnzVTn5jF43+b6eFsIBq3P762qJ1bVU+i6Nl0/5PhmY1zO4cM0UD3YrdQYDJQ0nze6VlUTX0AnBqQ6Yop138y9Bw57APCgnvv/BpwwHzEChwKbgF+e7eubxxjn/H2cZnyP5J5BN58IfIMuazyS99DbjP+nNzJ/g1mfAFwNLBnxfsfiWGz14gPA2+f7OGjxHIODWQ/7PR2Lc+O4nP/G6Rw3LuezAeMYyfXTQr/NVT0F/pR7Dy78J6OMraf8LOAlk8oO6rn/u8BZcxHbbD5XRvW+7SS2Kc/NY/C+zenxthBug9YZ4ICe/8G1tMGUx/01zaau7a6vqWf5MnaDwaznPYBxuNH14/x3uhHM39jKXga8rN3/Obrs4XeBO9r9fehGO/9yu101se08xfge4NvA5e22cWfbjlOMo3ofpxHf69v+LwcuBn5l1O+htxn9P29k/hJFm+j6J08cy389wn3P+7FI14S2gK/0vAfPmMdj4RhMFM3F+zoW58ZxOf+N0zluXM5ns41jLt6TxXqbi3oKPAS4kK5FwoXAfqOMrS3bG7gN+NlJz/l3wBV0559z6UmADDm2GX+ujPB9m+qzZspz83y/b6M43hbCbcA68y90P2J+GTh2vl/LoMfMVNuOw23A13QmsIVuQPLNwCnz/Xqmuk38yiNJkiRJkqRFzjGKJEmSJEmSBJgokiRJkiRJUmOiSJIkSZIkSYCJojmT5P1J/mge9vvyJLcm+V6Sh0xj/Z3G2Z7n4dNct5I8cnaRDybJVUmO2cU6h7bXs8dO1vnp65VGZTrH7xzu22Ne6rGzc1mSFyf53KhjkiRJGqVFlShKcmNLojygp+y3klw0j2FNW5JfTvKZJHcm+U6S/5fk8J7l9wHeCvx6VT2wqm5L55VJrkzy/SSbk/zfJP9pOvtsz/O1uXpN05HkH5P8QZ/ylUm+mWTPqjqiqi7a2fNU1U3t9dzdtr8oyW9NWmfeX6/GV/sM+WGrg3ck+bckL0sy0GfpdI7fYfCY1+6m1bkfJdl/UvnlLaGzbMDn36FODEuL/Wlz8dySJElzaVElipo9gVfNdxAzkWSPJL8EfAo4B3gocBjd9If/2tMa4EDgfnTTTk54B93rfSWwH/Ao4O+BE0cSfLOzVjzT8H7ghUkyqfyFwBlVtX2A55Zm6r9U1YOAhwHr6KZ/fu/8hiQtaDcAz5t40H7ouP/8hSNJkrSwLcZE0Z8Cr03y4N7CJMvar5N79pT99JfG1tz8X5O8rbUk+Fpr4fPiJDcn2Zrk5En72j/JBa31wT8neVjPc/98W3Z7kuuSPLdn2fuTvCvJPyT5PvBrwJ8AH6iqd1TVnVV1e1X9PvB54M1JHgVc157ijtbyaDlwKvC8qvpMVd1VVT+oqjOqal1PnPsmOa/FeUmSR/TEsrMm+P8ryZYktyR56aRlO7yGJA9N8tEk25LckOSVPeu/OcnZST7Q4rgqyYq2+O/pklz/uWf9fYFnAh9oj3/6y22So5JsTPLd1oLsrZP/x0nWtuf7y3Rdb/5y8uttr+GvdvLe/Hr7330nyentfzwnv0xr/FTVd6rqXOA3gZOTHJlkryR/luSmduz9dZL7AyTZP8kn2ufH7Un+ZaIl0qTj9/5JNiT5dpJrkrwuyeaJ/bZ1X5vkK+3Y+3CS+7Vl+7Z9bGvbfyLJ0rZsOsf8z7Y6uC3J15P8fk+ML07yufb6vt3q8NNH9X5rUfs74EU9j0+mffbD7I/bqepE87Qk17dt/irZ4YcKWvmfTyr7f0le3WfdndafJPsl+dt059NvJ/n7nmW/nWRT+9w4N8lDe5ZVkv/ZYr0zyR8meUSSi9s58Owk9+1Z/5npWmNNtIh87K7ffkmStNgsxkTRRuAi4LWz2PYXga8ADwE+BJwF/ALwSOAFdBebD+xZ//nAHwL7A5cDZwCk6/p2QXuOA+h+KT09yRE92/53YC3wIODfgF8G/m+fmM4Gjquqfwcmtn9wVT0VOBbYXFWX7uJ1PQ/4P8C+wKa2351KcgLde3gcsBzo17x+8mv4f3StoA5usb06yfE96z+L7j19MHAu8JcAVfXD9jp7vyg8F7i2qr7cZ7/vAN5RVfsAj2jb3ktVvRH4F+AVrevNK6Z4qX3fm3TdID4CnEZ3PFxH9z/SItPq12a6L5xvoWu193i6z4WDgf/dVn1NW28JXeu/NwDV5ynfBCwDHk5Xv17QZ53nAifQtSx8LPDiVv4zwN/StXY6FPgh99Sj6RzzfwH8bNv3r9LVuZf0LP9FumN9f7rk9Xv7fYGWhuzzwD5JHpOudepvAh/sWT6r43YXdeKZdOf3x9HVt95z1YQNwPN6klL7053bzpzideys/vwdsDfdefwA4G3tOZ8K/P9bDAcBX6c7T/Y6AXgScDTwOmA93fXHIcCRtNZYSZ4IvA/4H3Tnrb8Bzk2y1xTxSpKkRWoxJoqg++L2O0mWzHC7G6rqb9sYNx+muwj7g9ZS51PAj+i+HE44r6o+W1V3AW8EfinJIXQXoDe259peVV8EPgr8Rs+251TVv1bVT+ha0/wMsKVPTFvoLjr7ecgU20z2saq6tHXhOoPuS+6uPBf426q6sqq+D7y5zzq9r+E/AUuq6g+q6kdtTJR3A6t61v9cVf1De3//ju4CfcIG4DkTrTPovghsmCK2HwOPTLJ/VX2vqj4/jdczlanem2cAV1XVx9qydwLfHGA/2r3dQldPfxv43dbi707gj7nnGP8x3Re9h1XVj6vqX6qqX6LoucAfV9W3q2oz3bE12Tur6paqup0uAft4gKq6rao+2loO3kmX2PzV6byAni/gp7VWizcCf07XxXPC16vq3a2Obmiv58DpPL80oIlWRccB1wLfaOVzddyuq6o7quom4J/oc15sSeLv0CWHoKvrF1XVrVM8Z984khwEPB14Wav3P66qf27bPB94X1V9sV1LnEZ3LbGs53nfUlXfraqrgCuBT1XV16rqO8AngSe09X4b+JuquqSq7q6qDcBddAkmSZKkn1qUiaKquhL4BLBmhpv2Xvz9sD3X5LLeFkU39+zze8DtdOMLPQz4xdb0+44kd9BdDP5cv22BbwM/obuonOwg4FtTxHvbFNtM1pvg+MGk1zCVh06K8et91uld/jDgoZNe8xu498X65Djul9YVsKo+B2wDVqYbk+kX6Fpk9XMKXauOa5N8Ickzp/F6pjLVe3Ov19++8G9Gi9XBdOOf7Q1c1nOMn0/Xggi6bq+bgE+l67o61efP5Lp1c591+h6XSfZO8jet+813gc8CD870xgjbH7gv967LX2+vbYf9VtUP2t3pfF5Ig/o7ulaqL6an2xlzd9xO97y4gXta/b2gxbnL55wUxyHA7VX17T7bPJSe19auJW7j3q9v8nXIVNclDwNeM+k8fEjbhyRJ0k8tykRR8ya6X9cmLra+3/7u3bPOzzGYQybutC5p+9G1PLgZ+OeqenDP7YFV9fKebX/a0qC12LkYeE6ffTwXuHCK/V8ILM09Y/0M0xZ6Xh9dN5fJeltL3EzXIqv3NT+oqp4xg31+gO4X5RfS/WLa91fbqrq+qp5H13z/LcBH0jPT3RTxzdQWYOnEg9Z9YOnUq2uhSvILdJ8jf0/3peyInmP8Z6vqgQCttcNrqurhwH8Bfi/JsX2e8l7HFveuZ7vyGuDRwC+2rpdPmQiz/d3ZMf8tulZPD+spO5R7Wm5I86aqvk43qPUzgI/1LBr0uB3kPABdF7iVSR4HPIbuc2Cmbgb2y6SxE5tb6Hlt7Vz2EGZXL28G1k46D+9dVVN1lZMkSYvUok0UVdUmuu5jr2yPt9FdeL0g3SxjL6Ub32YQz0jyK20gyT8ELqmqm+laMz0qyQuT3KfdfiHJY3byXGvoBsx9ZZIHpRu09o+AX6IbQ6ffa7weOB04M8kxSe6b5H5JVu2kNcN0nQ28OMnhSfamS7ztzKXAd5O8Pt1gvXukG/z3F2awzw/QjYX020zd7YwkL0iypHV5u6MV391n1VvpxrSYjfOA/5TkpNbq6VQGTyxqN5Jkn9Za7Szgg9WNl/Vu4G1JDmjrHDwxDlcbRPaRLan4Xbpjst9xeTZwWqvjBwNTjZ/Vz4PoklV3JNmPHevllMd86w5zNrC2fcY8DPg97j0WjDSfTgGe2n48mTDocTvIeYDWPfQLdC2JPlrdmHozfY4tdF3ETm/1/j5JJpK8HwJekuTxbSyhP6a7lrhxFuG+G3hZkl9M5wFJTkzyoFk8lyRJWsAWbaKo+QOgt6XJbwP/i65Z9xF0AzAP4kN0X9Rupxto8vnQtSwAfp1uPINb6JqjvwWYckDJ1vXqeOC/0rU4+DrduAO/0hJCU3kl3WC2f0WXNPkq8Gy6cU1mrao+Cbwd+Axdd5rP7GL9u+laUTye7lfhbwHvoRuAdLr7vJHuf/IAusGup3ICcFWS79ENbL2qqv6jz3rvAH4j3Qwz/caB2Vks36Jr4fUndMfL4XQDpd81k+fRbun/JbmT7tf5NwJv5Z6Bc19PVx8+37p+fZquhQ90g75/GvgeXQvB06vqoj7P/wd03RhvaOt/hOkfV2+nmzb8W3QDAJ8/afmujvnfoWtd+TXgc3SfYe+b5r6lOVVVX62qjX0WDXLczvo80GMD3Th8O+t2tisvpGsZdS2wFXg1QFVdCPz/6MYx3EL3A9aq/k+xc+29+226a4Jv031WvXiAmCVJ0gKV/mOpSpqJNuvNZuD5VfVP8x2PFo4kL6dLdk5rUGpJo9Va/3wQWNZaskqSJO3WFnuLImnWkhyf5MGtO8Ab6MaBGWSGNYkkByV5cpKfSfJounGHPj7fcUnaUZL7AK8C3mOSSJIkLRQmiqTZ+yW6rnzfoutWd9JsxqeQJrkv8DfAnXRdOs+hG2tM0hhp4wreQTe76NvnNRhJkqQhsuuZJEmSJEmSAFsUSZIkSZIkqTFRJEnSiLRxzT6S5Nok1yT5pST7JbkgyfXt774965+WZFOS65IcP5+xS5IkaXEY+65n+++/fy1btmy+w5DmxWWXXfatqloy33H0Y93UYjbbuplkA/AvVfWeJPcF9qYbDP/2qlqXZA2wb1W9PsnhwJnAUcBDgU8Dj6qqu6d6fuulFrtxPm9KkrS72HO+A9iVZcuWsXHjxvkOQ5oXSb4+3zFMxbqpxWw2dTPJPsBTgBcDVNWPgB8lWQkc01bbAFwEvB5YCZxVVXcBNyTZRJc0uniqfVgvtdiN83lTkqTdhV3PJEkajYcD24C/TfKlJO9J8gDgwKraAtD+HtDWPxi4uWf7za3sXpKsTrIxycZt27bN7SuQJEnSgmeiSJKk0dgTeCLwrqp6AvB9YM1O1k+fsh36i1fV+qpaUVUrliyxx40kSZIGY6JIkqTR2AxsrqpL2uOP0CWObk1yEED7u7Vn/UN6tl8K3DKiWCVJkrRImSiSJGkEquqbwM1JHt2KjgWuBs4FTm5lJwPntPvnAquS7JXkMGA5cOkIQ5YkSdIiNPaDWUuStID8DnBGm/Hsa8BL6H60OTvJKcBNwHMAquqqJGfTJZO2A6fubMYzSZIkaRhMFEmSNCJVdTmwos+iY6dYfy2wdi5jkiRJknqZKNJOLVtz3i7XuXHdiSOIRJKGY1efa36maRAeX5IkaXdnokiSJGkapvPjiSRJ0u7OwawlSZIkSZIEmCiSJEmSJElSY6JIkiRJkiRJgIkiSZIkSZIkNSaKJEmSJEmSBJgokiRJkiRJUrPnrlZI8j7gmcDWqjqylf0p8F+AHwFfBV5SVXe0ZacBpwB3A6+sqn9s5U8C3g/cH/gH4FVVVUN+PZK0g+lMaX3juhNHEIkkSZIkjbfptCh6P3DCpLILgCOr6rHAvwOnASQ5HFgFHNG2OT3JHm2bdwGrgeXtNvk5JUmSJEmSNI92mSiqqs8Ct08q+1RVbW8PPw8sbfdXAmdV1V1VdQOwCTgqyUHAPlV1cWtF9AHgpCG9BkmSJEmSJA3BMMYoeinwyXb/YODmnmWbW9nB7f7kckmSJEmSJI2JgRJFSd4IbAfOmCjqs1rtpHyq512dZGOSjdu2bRskRGnBSvK+JFuTXNlTtl+SC5Jc3/7u27PstCSbklyX5Pie8icluaIte2eSfvVVkiRJkrQIzDpRlORkukGun98zKPVm4JCe1ZYCt7TypX3K+6qq9VW1oqpWLFmyZLYhSgvd+9lxrK81wIVVtRy4sD12/DBJkiRJ0rTMKlGU5ATg9cCzquoHPYvOBVYl2SvJYXRfOi+tqi3AnUmObq0VXgScM2Ds0qLWb/wwunHCNrT7G7hnLDDHD5MkSZIk7dKeu1ohyZnAMcD+STYDb6Kb5Wwv4ILWS+XzVfWyqroqydnA1XRd0k6tqrvbU72crgXE/enGNPokkobtwJaYpaq2JDmglR9MN/D8hIlxwn7MDMYPS7KarvURhx566BDDliRJkiSNg10miqrqeX2K37uT9dcCa/uUbwSOnFF0koZlKOOHVdV6YD3AihUrplxPkiRJkrR7GsasZ5LGx62tOxnt79ZWPpTxwyRJkiRJC5uJImlhORc4ud0/mXvGAnP8MEmSJEnSLu2y65mk8TTF+GHrgLOTnALcBDwHwPHDJEmSJEnTYaJI2k1NMX4YwLFTrO/4YdI8S3IjcCdwN7C9qlYk2Q/4MLAMuBF4blV9u61/GnBKW/+VVfWP8xC2JEmSFhG7nkmSNFq/VlWPr6oV7fEa4MKqWg5c2B6T5HBgFXAEcAJwepI95iNgSZIkLR4miiRJml8rgQ3t/gbgpJ7ys6rqrqq6AdgEHDX68CRJkrSYmCiSJGl0CvhUksuSrG5lB7aB5Wl/D2jlBwM392y7uZXdS5LVSTYm2bht27Y5DF2SJEmLgWMUSZI0Ok+uqluSHABckOTanaybPmW1Q0HVemA9wIoVK3ZYLkmSJM2ELYokSRqRqrql/d0KfJyuK9mtSQ4CaH+3ttU3A4f0bL4UuGV00UqSJGkxMlEkSdIIJHlAkgdN3Ad+HbgSOBc4ua12MnBOu38usCrJXkkOA5YDl442akmSJC02dj2TJGk0DgQ+ngS68++Hqur8JF8Azk5yCnAT8ByAqroqydnA1cB24NSqunt+QpckSdJiYaJIkqQRqKqvAY/rU34bcOwU26wF1s5xaJIkSdJP2fVMkiRJkiRJgIkiSZIkSZIkNSaKJEmSJEmSBJgokiRJkiRJUmOiSJIkSZIkSYCJIkmSJEmSJDW7TBQleV+SrUmu7CnbL8kFSa5vf/ftWXZakk1JrktyfE/5k5Jc0Za9M0mG/3IkSZIkSZI0W3tOY533A38JfKCnbA1wYVWtS7KmPX59ksOBVcARwEOBTyd5VFXdDbwLWA18HvgH4ATgk8N6IZIkSbO1bM158x2CJEnSWNhli6Kq+ixw+6TilcCGdn8DcFJP+VlVdVdV3QBsAo5KchCwT1VdXFVFl3Q6CUmSJEmSJI2N2Y5RdGBVbQFofw9o5QcDN/est7mVHdzuTy7vK8nqJBuTbNy2bdssQ5QkSZIkSdJMDHsw637jDtVOyvuqqvVVtaKqVixZsmRowUmSJEmSJGlqs00U3dq6k9H+bm3lm4FDetZbCtzSypf2KZckSZIkSdKYmM5g1v2cC5wMrGt/z+kp/1CSt9INZr0cuLSq7k5yZ5KjgUuAFwF/MVDkkiRJu5npDJp947oTRxCJJElSf7tsUZTkTOBi4NFJNic5hS5BdFyS64Hj2mOq6irgbOBq4Hzg1DbjGcDLgffQDXD9VZzxTJozSX43yVVJrkxyZpL7JdkvyQVJrm9/9+1Z/7Qkm5Jcl+T4+YxdkiRJkjR/dtmiqKqeN8WiY6dYfy2wtk/5RuDIGUUnacaSHAy8Eji8qn6Y5GxgFXA4cGFVrUuyBlgDvD7J4W35EXQtAT+d5FE9SV5JkiRJ0iIx7MGsJY2HPYH7J9kT2JtuTLCVwIa2fANwUru/Ejirqu6qqhvoWv0dNdpwJUmSJEnjwESRtMBU1TeAPwNuArYA36mqTwEHVtWWts4W4IC2ycHAzT1PsbmVSZIkSZIWGRNF0gLTxh5aCRxG15XsAUlesLNN+pTVFM+9OsnGJBu3bds2eLCSJEmSpLFiokhaeJ4G3FBV26rqx8DHgF8Gbk1yEED7u7Wtvxk4pGf7pXRd1XZQVeurakVVrViyZMmcvQBJkiRJ0vwwUSQtPDcBRyfZO0noBp6/BjgXOLmtczJwTrt/LrAqyV5JDgOWA5eOOGZJkiRJ0hgwUSQtMFV1CfAR4IvAFXT1fD2wDjguyfXAce0xVXUVcDZwNXA+cKoznklzJ8keSb6U5BPt8X5JLkhyffu7b8+6pyXZlOS6JMfPX9SSJElaLPac7wAkDV9VvQl406Tiu+haF/Vbfy2wdq7jkgTAq+ha+e3THq8BLqyqdUnWtMevT3I4sAo4gm68sU8neZSJXEmSJM0lWxRJkjQiSZYCJwLv6SleCWxo9zcAJ/WUn1VVd1XVDcAm4KgRhSpJkqRFykSRJEmj83bgdcBPesoOrKotAO3vAa38YODmnvU2t7J7cTZCSZIkDZOJIkmSRiDJM4GtVXXZdDfpU1Y7FDgboSRJkobIMYokSRqNJwPPSvIM4H7APkk+CNya5KCq2pLkIGBrW38zcEjP9kuBW0YasSRJkhYdWxRJkjQCVXVaVS2tqmV0g1R/pqpeAJwLnNxWOxk4p90/F1iVZK8khwHLgUtHHLYkSZIWGVsUSZI0v9YBZyc5BbgJeA5AVV2V5GzgamA7cKoznkmSJGmumSiSJGnEquoi4KJ2/zbg2CnWWwusHVlgkiRJWvTseiZJkiRJkiTARJEkSZIkSZIaE0WSJEmSJEkCBkwUJfndJFcluTLJmUnul2S/JBckub793bdn/dOSbEpyXZLjBw9fkiRJkiRJwzLrRFGSg4FXAiuq6khgD7rpftcAF1bVcuDC9pgkh7flRwAnAKcn2WOw8CVJkiRJkjQsg3Y92xO4f5I9gb2BW4CVwIa2fANwUru/Ejirqu6qqhuATcBRA+5fkiRJkiRJQzLrRFFVfQP4M+AmYAvwnar6FHBgVW1p62wBDmibHAzc3PMUm1vZDpKsTrIxycZt27bNNkRJkiRJkiTNwCBdz/alayV0GPBQ4AFJXrCzTfqUVb8Vq2p9Va2oqhVLliyZbYiSJEmSJEmagT0H2PZpwA1VtQ0gyceAXwZuTXJQVW1JchCwta2/GTikZ/uldF3VBrJszXm7XOfGdScOuhtJkiRJkqQFb5Axim4Cjk6yd5IAxwLXAOcCJ7d1TgbOaffPBVYl2SvJYcBy4NIB9i9JkiRJkqQhmnWLoqq6JMlHgC8C24EvAeuBBwJnJzmFLpn0nLb+VUnOBq5u659aVXcPGL8kSZIkSZKGZJCuZ1TVm4A3TSq+i651Ub/11wJrB9mnJEmSJEmS5sYgXc8kSZIkSZK0gJgokiRJkiRJEmCiSJIkSZIkSY2JImkBSvLgJB9Jcm2Sa5L8UpL9klyQ5Pr2d9+e9U9LsinJdUmOn8/YJUmSJEnzx0SRtDC9Azi/qn4eeBxwDbAGuLCqlgMXtsckORxYBRwBnACcnmSPeYlakiRJkjSvTBRJC0ySfYCnAO8FqKofVdUdwEpgQ1ttA3BSu78SOKuq7qqqG4BNwFGjjFmSJEmSNB5MFEkLz8OBbcDfJvlSkvckeQBwYFVtAWh/D2jrHwzc3LP95la2gySrk2xMsnHbtm1z9wokSZIkSfPCRJG08OwJPBF4V1U9Afg+rZvZFNKnrPqtWFXrq2pFVa1YsmTJ4JFKkiRJksaKiSJp4dkMbK6qS9rjj9Aljm5NchBA+7u1Z/1DerZfCtwyolilRSPJ/ZJcmuTLSa5K8n9auQPNS5IkaWyYKJIWmKr6JnBzkke3omOBq4FzgZNb2cnAOe3+ucCqJHslOQxYDlw6wpClxeIu4KlV9Tjg8cAJSY7GgeYlSZI0Rvac7wAkzYnfAc5Icl/ga8BL6BLDZyc5BbgJeA5AVV2V5Gy6ZNJ24NSqunt+wpYWrqoq4Hvt4X3aregGlD+mlW8ALgJeT89A88ANSSYGmr94dFFLo7FszXm7XOfGdSeOIBJJkmSiaIi8yNG4qKrLgRV9Fh07xfprgbVzGZMkaC2CLgMeCfxVVV2S5F4DzSfpHWj+8z2b9x1oPslqYDXAoYceOpfhS5IkaRGw65kkSSNSVXdX1ePpxgI7KsmRO1l9WgPNO8i8JEmShslEkSRJI1ZVd9B1MTsBB5qXJEnSGDFRJEnSCCRZkuTB7f79gacB1+JA85IkSRojjlEkSdJoHARsaOMU/QxwdlV9IsnFONC8JEmSxoSJIkmSRqCqvgI8oU/5bTjQvCRJksbEQF3Pkjw4yUeSXJvkmiS/lGS/JBckub793bdn/dOSbEpyXZLjBw9fkiRJkiRJwzLoGEXvAM6vqp8HHgdcA6wBLqyq5cCF7TFJDgdWAUfQDd55emt+L0mSJEmSpDEw60RRkn2ApwDvBaiqH7VZXFYCG9pqG4CT2v2VwFlVdVdV3QBsAo6a7f4lSZIkSZI0XIO0KHo4sA342yRfSvKeJA8ADqyqLQDt7wFt/YOBm3u239zKdpBkdZKNSTZu27ZtgBAlSZIkSZI0XYMkivYEngi8q6qeAHyf1s1sCulTVv1WrKr1VbWiqlYsWbJkgBAlSZIkSZI0XYPMerYZ2FxVl7THH6FLFN2a5KCq2pLkIGBrz/qH9Gy/FLhlgP1LkiTt0rI15813CJIkSbuNWbcoqqpvAjcneXQrOha4GjgXOLmVnQyc0+6fC6xKsleSw4DlwKWz3b8kSZIkSZKGa5AWRQC/A5yR5L7A14CX0CWfzk5yCnAT8ByAqroqydl0yaTtwKlVdfeA+5ckSZIkSdKQDJQoqqrLgRV9Fh07xfprgbWD7FOSJEmSJElzY5DBrCVJkiRJkrSAmCiSJEmSJEkSYKJIkiRJkiRJjYkiSZIkSZIkASaKJEmSJEmS1Aw065kkaWaWrTlvl+vcuO7EEUQiSZIkSTuyRZEkSZIkSZIAE0WSJEmSJElqTBRJC1SSPZJ8Kckn2uP9klyQ5Pr2d9+edU9LsinJdUmOn7+oJUmSJEnzyUSRtHC9Crim5/Ea4MKqWg5c2B6T5HBgFXAEcAJwepI9RhyrJEmSJGkMmCiSFqAkS4ETgff0FK8ENrT7G4CTesrPqqq7quoGYBNw1IhClSRJkiSNERNF0sL0duB1wE96yg6sqi0A7e8Brfxg4Oae9Ta3sh0kWZ1kY5KN27ZtG3rQ0kKW5JAk/5TkmiRXJXlVK7dbqCRJksaGiSJpgUnyTGBrVV023U36lFW/FatqfVWtqKoVS5YsmXWM0iK1HXhNVT0GOBo4tXX9tFuoJEmSxoaJImnheTLwrCQ3AmcBT03yQeDWJAcBtL9b2/qbgUN6tl8K3DK6cKXFoaq2VNUX2/076cYQOxi7hUqSJGmMmCiSFpiqOq2qllbVMrrWCJ+pqhcA5wInt9VOBs5p988FViXZK8lhwHLg0hGHLS0qSZYBTwAuYQjdQiVJkqRh2XO+A5A0MuuAs5OcAtwEPAegqq5KcjZwNV3XmFOr6u75C1Na2JI8EPgo8Oqq+m7Sr/dnt2qfsh26hSZZDawGOPTQQ4cVpiRJkhYpE0XSAlZVFwEXtfu3AcdOsd5aYO3IApMWqST3oUsSnVFVH2vFtyY5qKq2zKZbaFWtB9YDrFixou/4YpIkSdJ0Ddz1LMkeSb6U5BPtsbO3SJI0SbqmQ+8Frqmqt/YssluoJEmSxsYwxih6Fd2AnBOcvUWSpB09GXgh3QDzl7fbM+i6hR6X5HrguPaYqroKmOgWej52C5UkSdIIDNT1LMlS4ES6Liu/14pXAse0+xvour28np7ZW4AbkkzM3nLxIDFIkrQ7qKrP0X/cIbBbqCRJksbEoC2K3g68DvhJT9nAs7ckWZ1kY5KN27ZtGzBESZIkSZIkTcesE0VJnglsrarLprtJn7K+g25W1fqqWlFVK5YsWTLbECVJkiRJkjQDg3Q9ezLwrDa+wv2AfZJ8kAFnb5EkSZIkSdL8mHWLoqo6raqWVtUyukGqP1NVL8DZWyRJkiRJknZLAw1mPYV1wNlJTgFuAp4D3ewtSSZmb9mOs7dIkiRJkiSNlaEkiqrqIrrZzaiq23D2FkmSJEmSpN3OoLOeSZIkSZIkaYEwUSRJkiRJkiTARJEkSZIkSZIaE0WSJEmSJEkCTBRJkiRJkiSpGcqsZ5IkSRqOZWvO2+U6N647cQSRSJKkxcgWRZIkSZIkSQJMFEmSJEmSJKmx65kkSdqtTaerliRJkqbHFkWSJEmSJEkCTBRJkiRJkiSpMVEkSZIkSZIkwESRJEmSJEmSGhNF0gKT5JAk/5TkmiRXJXlVK98vyQVJrm9/9+3Z5rQkm5Jcl+T4+YtekiRJkjSfnPVMWni2A6+pqi8meRBwWZILgBcDF1bVuiRrgDXA65McDqwCjgAeCnw6yaOq6u5BgpjOLEQ3rjtxkF1Iu5Uk7wOeCWytqiNb2X7Ah4FlwI3Ac6vq223ZacApwN3AK6vqH+chbEmSJC0ytiiSFpiq2lJVX2z37wSuAQ4GVgIb2mobgJPa/ZXAWVV1V1XdAGwCjhpp0NLi8H7ghElla+gSuMuBC9tjJiVwTwBOT7LH6EKVJEnSYmWiSFrAkiwDngBcAhxYVVugSyYBB7TVDgZu7tlscyvr93yrk2xMsnHbtm1zFre0EFXVZ4HbJxWbwJUkSdJYmXWiyHFQpPGW5IHAR4FXV9V3d7Zqn7Lqt2JVra+qFVW1YsmSJcMIU1rsBk7gSpIkScM0SIuiiXFQHgMcDZzamsrbjF6aZ0nuQ5ckOqOqPtaKb01yUFt+ELC1lW8GDunZfClwy6hildTXtBO4tvSTJEnSMM06UeQ4KNJ4ShLgvcA1VfXWnkXnAie3+ycD5/SUr0qyV5LDgOXApaOKV1rkBk7g2tJPkiRJwzSUMYqGPQ6KpIE8GXgh8NQkl7fbM4B1wHFJrgeOa4+pqquAs4GrgfOBUwed8UzStJnAlSRJ0ljZc9AnmDwOSteYof+qfcqmbEYPrAY49NBDBw1RWlSq6nP0r28Ax06xzVpg7ZwFJYkkZwLHAPsn2Qy8iS5he3aSU4CbgOdAl8BNMpHA3Y4JXEmSJI3IQIminY2DUlVbBmlGD6wHWLFiRd9kkjTulq05b6fLb1x34ogikTQOqup5UywygStJkqSxMcisZ46DIkmSJEmStIAM0qJoYhyUK5Jc3sregM3oJUmSJEmSdkuzThQ5DoqkUdhVFz6wG58kSZIkDctQZj2TJEmSJEnS7s9EkSRJkiRJkgATRZIkSZIkSWpMFEmSJEmSJAkwUSRJkiRJkqTGRJEkSZIkSZIAE0WSJEmSJElqTBRJkiRJkiQJMFEkSZIkSZKkxkSRJEmSJEmSABNFkiRJkiRJakwUSZIkSZIkCTBRJEmSJEmSpMZEkSRJkiRJkgDYc74DkCRJo7NszXm7XOfGdSeOIBJJkiSNI1sUSZIkSZIkCTBRJEmSJEmSpMZEkSRJkiRJkoB5GKMoyQnAO4A9gPdU1bpRxyBpR9ZNaTwt9ro5nTGVJEmSNDwjbVGUZA/gr4CnA4cDz0ty+ChjkLQj66Y0nqybkiRJGrVRdz07CthUVV+rqh8BZwErRxyDpB1ZN6XxZN2UJEnSSKWqRrez5DeAE6rqt9rjFwK/WFWvmLTeamB1e/ho4Loh7H5/4FtDeB73v3vufxximM3+H1ZVS+YimF7zXDd3Zb7/b7syzvEZ2+ztKr6xqZvzVC97jcv/0jjGKwaYnzhGUjclSVrIRj1GUfqU7ZCpqqr1wPqh7jjZWFUrhvmc7n/32f84xDDf+9+FeaubuzLm79tYx2dsszdG8e2ybs5Hvew1Lu+VcYxXDOMUhyRJmplRdz3bDBzS83gpcMuIY5C0I+umNJ6sm5IkSRqpUSeKvgAsT3JYkvsCq4BzRxyDpB1ZN6XxZN2UJEnSSI2061lVbU/yCuAf6ab5fV9VXTWi3c9bs3z3Pxb7h/mPYb73P6V5rpu7MrbvWzPO8Rnb7I1FfGNeNyeMxXuFcfQahxhgfOKQJEkzMNLBrCVJkiRJkjS+Rt31TJIkSZIkSWPKRJEkSZIkSZKABZAoSnJCkuuSbEqyps/yn09ycZK7krx20rLfTXJVkiuTnJnkfiPe/6vavq9K8uqZ7nsGMTw/yVfa7d+SPG66245g/+9LsjXJlbPZ9yD7T3JIkn9Kck37H7xqxPu/X5JLk3y57f//zGb/u7MB68/Ax+4cxnZjkiuSXJ5k47Bjm2Z8c1rv5zC2OX3vphHbyhbX5Uk2JvmV6W4rSPLaJJVk/3na/58mubb9Dz+e5MEj3Pe8Hx/DOq8NKZY9knwpySfmKwZJkjRLVbXb3ugG9vwq8HDgvsCXgcMnrXMA8AvAWuC1PeUHAzcA92+PzwZePML9HwlcCexNN6j4p4Hlc/Qe/DKwb7v/dOCS6W47l/tvj58CPBG4cg6Pgale/0HAE9v9BwH/PsrXDwR4YLt/H+AS4Oj5rlejug1YfwY+ducqtrbsRmD/eX7v5qzez1Vsc/3eTTO2B3LP+H2PBa4dxfu2EG7AIXSDbn99Lo//XcTw68Ce7f5bgLeMaL9jcXwwhPPaEGP5PeBDwCfmY//evHnz5s2bt9nfdvcWRUcBm6rqa1X1I+AsYGXvClW1taq+APy4z/Z7AvdPsiddwuaWEe7/McDnq+oHVbUd+Gfg2TPc/3Rj+Leq+nZ7+Hlg6XS3neP9U1WfBW6f4T6Hsv+q2lJVX2z37wSuoUsgjmr/VVXfa+X3abfFNLr8IPVnGMfuXMU2CvNd7+cqtrk2ndi+V1UT9fAB3FMn5/p9WwjeBryOefwcq6pPtXMqjNmxNQpDOq8NLMlS4ETgPaPetyRJGtzunig6GLi55/FmpnlBVFXfAP4MuAnYAnynqj41qv3TtSZ6SpKHJNkbeAbdr7EzNdMYTgE+Octth73/YRjK/pMsA55A16pnZPtvTfMvB7YCF1TVTPe/Oxvk+BvGsTuXz1/Ap5JclmT1EOOaMN/1fq5ig7l976YVW5JnJ7kWOA946Uy2XaySPAv4RlV9eb5j6fFShnu+2ZmxOz4GOK8Nw9vpkoY/mYd9S5KkAe053wEMKH3KpvVLZpJ96X7tOwy4A/i/SV5QVR8cxf6r6pokbwEuAL5H10x9+863GiyGJL9G96VsYsyNWcc/pP0Pw8D7T/JA4KPAq6vqu6Pcf1XdDTy+jaPx8SRHVtWsx2vazQxy/A3j2J3L539yVd2S5ADggiTXttZzwzLf9X5nBq2Tc/neTSu2qvo4XX18CvCHwNOmu+1CluTTwM/1WfRG4A103b7mNY6qOqet80a6c+oZo4iJMTs+BjyvDbrvZwJbq+qyJMeMct+SJGk4dvdE0Wbu3QpnKdPvPvY04Iaq2gaQ5GN042bMJFE0yP6pqvcC7237/+P2fDM1rRiSPJauCfjTq+q2mWw7h/sfhoH2n+Q+dBfTZ1TVx0a9/wlVdUeSi4AT6FqbLQaDHH/DOHbn7Pmr6pb2d2uSj9N1Sxlmomi+6/1cxTbX792MXntVfTbJI9rAzHP9vo29qnpav/Ik/4nuR5cvJ4HuvflikqOq6pujiqMnnpOBZwLH9nQjnGtjc3wM4bw2qCcDz0ryDOB+wD5JPlhVL5iHWCRJ0izs7l3PvgAsT3JYkvsCq4Bzp7ntTcDRSfZOd2V7LF1f/lHtn/aLOUkOBf4rcOYM9z+tGNrzfwx4YVX9+7DiH8L+h2HW+2//9/cC11TVW+dh/0smZuRJcn+65OW1s4xjdzTI8TeMY3dOnj/JA5I8aOI+XSuLYSf/5rvez0lsI3jvphPbI9tnA0meSDcw8W3T2XaxqqorquqAqlpWVcvokiZPnIsk0a4kOQF4PfCsqvrBCHc9FsfHkM5rA6mq06pqaTsWVgGfMUkkSdLuZbduUVRV25O8gm6WlT2A91XVVUle1pb/dZKfAzYC+wA/STcN/eFVdUmSjwBfpGue/iVg/Qj3/13go0keQjcY7qk9g7sONQbgfwMPAU5v33+2V9WKqbYd1f4BkpwJHAPsn2Qz8KbW0moU+38y8ELginTjBAG8oar+YUT7PwjYkGQPuqTt2VW1aKYRHrT+DHrszlVswP503Zag+4z9UFWdP6zYphsfc1jv5yo24EDm8L2bZmz/DXhRkh8DPwR+s7VKmdP3TUPzl8BedN0WoZs04mVzvdO5rlczMPB5TZIkaWIKYEmSJEmSJC1yu3vXM0mSJEmSJA2JiSJJkiRJkiQBJookSZIkSZLUmCiSJEmSJEkSYKJIkiRJkiRJjYkiSZIkSZIkASaKJEmSJEmS1Px/JtonL29ZLcsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1080 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "test_col = test_df.columns\n",
    "for i, col in enumerate(test_col, 1):\n",
    "    row = int(np.sqrt(len(test_col)))\n",
    "    plt.subplot(row, int(len(test_col)/row)+1, i)\n",
    "    plt.hist(test_df[col], bins=20)\n",
    "    plt.title(col)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb53399",
   "metadata": {},
   "source": [
    "5. 신경망 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "c2fb8b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "TypeofContact               0\n",
       "CityTier                    0\n",
       "DurationOfPitch             0\n",
       "Occupation                  1\n",
       "Gender                      0\n",
       "NumberOfPersonVisiting      1\n",
       "NumberOfFollowups           0\n",
       "ProductPitched              0\n",
       "PreferredPropertyStar       0\n",
       "MaritalStatus               0\n",
       "NumberOfTrips               0\n",
       "Passport                    0\n",
       "PitchSatisfactionScore      0\n",
       "OwnCar                      0\n",
       "NumberOfChildrenVisiting    0\n",
       "Designation                 0\n",
       "MonthlyIncome               0\n",
       "ProdTaken                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "a49bcfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[-train_df['Occupation'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "c215cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[-train_df['NumberOfPersonVisiting'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "b779d69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "TypeofContact               0\n",
       "CityTier                    0\n",
       "DurationOfPitch             0\n",
       "Occupation                  0\n",
       "Gender                      0\n",
       "NumberOfPersonVisiting      0\n",
       "NumberOfFollowups           0\n",
       "ProductPitched              0\n",
       "PreferredPropertyStar       0\n",
       "MaritalStatus               0\n",
       "NumberOfTrips               0\n",
       "Passport                    0\n",
       "PitchSatisfactionScore      0\n",
       "OwnCar                      0\n",
       "NumberOfChildrenVisiting    0\n",
       "Designation                 0\n",
       "MonthlyIncome               0\n",
       "ProdTaken                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b447b",
   "metadata": {},
   "source": [
    "### pytorch 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "720996d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F                       \n",
    "from sklearn.metrics import mean_squared_error   \n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "\n",
    "x = train_df.drop('ProdTaken', axis=1).to_numpy()\n",
    "y = train_df[['ProdTaken']].astype('float64').to_numpy().reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "7153b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.y_data = torch.FloatTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.x_data[index], self.y_data[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "46b8d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "b612c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1757, 18)"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "f5aa6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorData(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=30, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "d532b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class travelNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(travelNet, self).__init__()\n",
    "        self.l1 = nn.Linear(18, 192, bias=True)\n",
    "        self.l2 = nn.Linear(192, 150, bias=True)\n",
    "        self.l3 = nn.Linear(150, 100, bias=True)\n",
    "        self.l4 = nn.Linear(100, 50, bias=True)\n",
    "        self.l5 = nn.Linear(50, 2, bias=True)\n",
    "        self.batchnorm = nn.BatchNorm1d\n",
    "        self.dropout = nn.Dropout(0.05)\n",
    "        self.classifier = torch.softmax\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.l5(x)\n",
    "        x = self.batchnorm(x.size(dim=1))(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.classifier(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "abefc2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = travelNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "lr3 = 0.0001\n",
    "\n",
    "params_ft = []\n",
    "params_ft.append({'params': mlp_model.l1.parameters(), 'lr': lr3})\n",
    "params_ft.append({'params': mlp_model.l2.parameters(), 'lr': lr2})\n",
    "params_ft.append({'params': mlp_model.l3.parameters(), 'lr': lr2})\n",
    "params_ft.append({'params': mlp_model.l4.parameters(), 'lr': lr1})\n",
    "\n",
    "optimizer = optim.Adam(params_ft)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "epoch_num = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "a0c95bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "0bec4fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 is running ---------------\n",
      "loss:0.5802522368472198, accuracy:0.8081957697868347\n",
      "Epoch 2/600 is running ---------------\n",
      "loss:0.519085690892976, accuracy:0.8081957697868347\n",
      "Epoch 3/600 is running ---------------\n",
      "loss:0.5151657775558275, accuracy:0.8081957697868347\n",
      "Epoch 4/600 is running ---------------\n",
      "loss:0.514551739754348, accuracy:0.8081957697868347\n",
      "Epoch 5/600 is running ---------------\n",
      "loss:0.5142690333826788, accuracy:0.8081957697868347\n",
      "Epoch 6/600 is running ---------------\n",
      "loss:0.5141820655814533, accuracy:0.8081957697868347\n",
      "Epoch 7/600 is running ---------------\n",
      "loss:0.5138716867257809, accuracy:0.8076266646385193\n",
      "Epoch 8/600 is running ---------------\n",
      "loss:0.5132855562300518, accuracy:0.8087649345397949\n",
      "Epoch 9/600 is running ---------------\n",
      "loss:0.5130260062628779, accuracy:0.8081957697868347\n",
      "Epoch 10/600 is running ---------------\n",
      "loss:0.5131160323989803, accuracy:0.8076266646385193\n",
      "Epoch 11/600 is running ---------------\n",
      "loss:0.5123766861085234, accuracy:0.8081957697868347\n",
      "Epoch 12/600 is running ---------------\n",
      "loss:0.5095162622887512, accuracy:0.8064883351325989\n",
      "Epoch 13/600 is running ---------------\n",
      "loss:0.5076872716689932, accuracy:0.8093340992927551\n",
      "Epoch 14/600 is running ---------------\n",
      "loss:0.5068118078955288, accuracy:0.8116106986999512\n",
      "Epoch 15/600 is running ---------------\n",
      "loss:0.506275961625165, accuracy:0.8127490282058716\n",
      "Epoch 16/600 is running ---------------\n",
      "loss:0.5035547362319355, accuracy:0.8150256276130676\n",
      "Epoch 17/600 is running ---------------\n",
      "loss:0.503178379658995, accuracy:0.8138872981071472\n",
      "Epoch 18/600 is running ---------------\n",
      "loss:0.5022927065347803, accuracy:0.8155947923660278\n",
      "Epoch 19/600 is running ---------------\n",
      "loss:0.5032282132526924, accuracy:0.8127490282058716\n",
      "Epoch 20/600 is running ---------------\n",
      "loss:0.5042220631550098, accuracy:0.8138872981071472\n",
      "Epoch 21/600 is running ---------------\n",
      "loss:0.5000717650199759, accuracy:0.8212862610816956\n",
      "Epoch 22/600 is running ---------------\n",
      "loss:0.5010407048052755, accuracy:0.8173022270202637\n",
      "Epoch 23/600 is running ---------------\n",
      "loss:0.5000774773030445, accuracy:0.8178713917732239\n",
      "Epoch 24/600 is running ---------------\n",
      "loss:0.5015136656062357, accuracy:0.8184404969215393\n",
      "Epoch 25/600 is running ---------------\n",
      "loss:0.5003043844782072, accuracy:0.8167330622673035\n",
      "Epoch 26/600 is running ---------------\n",
      "loss:0.5011916772044939, accuracy:0.8195788264274597\n",
      "Epoch 27/600 is running ---------------\n",
      "loss:0.5024666950620454, accuracy:0.8116106986999512\n",
      "Epoch 28/600 is running ---------------\n",
      "loss:0.501887861510803, accuracy:0.8173022270202637\n",
      "Epoch 29/600 is running ---------------\n",
      "loss:0.49876957155507184, accuracy:0.8207171559333801\n",
      "Epoch 30/600 is running ---------------\n",
      "loss:0.5014349391748165, accuracy:0.8178713917732239\n",
      "Epoch 31/600 is running ---------------\n",
      "loss:0.49995130454671793, accuracy:0.8195788264274597\n",
      "Epoch 32/600 is running ---------------\n",
      "loss:0.4974772817101972, accuracy:0.8195788264274597\n",
      "Epoch 33/600 is running ---------------\n",
      "loss:0.4975480817515275, accuracy:0.824701189994812\n",
      "Epoch 34/600 is running ---------------\n",
      "loss:0.4999294173100899, accuracy:0.8190096616744995\n",
      "Epoch 35/600 is running ---------------\n",
      "loss:0.4978539640533513, accuracy:0.8229937553405762\n",
      "Epoch 36/600 is running ---------------\n",
      "loss:0.4968511153911722, accuracy:0.8235629200935364\n",
      "Epoch 37/600 is running ---------------\n",
      "loss:0.49368048176683227, accuracy:0.824701189994812\n",
      "Epoch 38/600 is running ---------------\n",
      "loss:0.49768069951698696, accuracy:0.8190096616744995\n",
      "Epoch 39/600 is running ---------------\n",
      "loss:0.49692198531381016, accuracy:0.8212862610816956\n",
      "Epoch 40/600 is running ---------------\n",
      "loss:0.49705107767006446, accuracy:0.8212862610816956\n",
      "Epoch 41/600 is running ---------------\n",
      "loss:0.4929831141027911, accuracy:0.8264086246490479\n",
      "Epoch 42/600 is running ---------------\n",
      "loss:0.4923381445736721, accuracy:0.8275469541549683\n",
      "Epoch 43/600 is running ---------------\n",
      "loss:0.4946962620677619, accuracy:0.8252703547477722\n",
      "Epoch 44/600 is running ---------------\n",
      "loss:0.4966619806043033, accuracy:0.8195788264274597\n",
      "Epoch 45/600 is running ---------------\n",
      "loss:0.4935913646015628, accuracy:0.8275469541549683\n",
      "Epoch 46/600 is running ---------------\n",
      "loss:0.4900697865362825, accuracy:0.8303927183151245\n",
      "Epoch 47/600 is running ---------------\n",
      "loss:0.49558386258010206, accuracy:0.8258395195007324\n",
      "Epoch 48/600 is running ---------------\n",
      "loss:0.4938434449763134, accuracy:0.8264086246490479\n",
      "Epoch 49/600 is running ---------------\n",
      "loss:0.49154481908370706, accuracy:0.8264086246490479\n",
      "Epoch 50/600 is running ---------------\n",
      "loss:0.49472793988112745, accuracy:0.8235629200935364\n",
      "Epoch 51/600 is running ---------------\n",
      "loss:0.4954611376441758, accuracy:0.8218554258346558\n",
      "Epoch 52/600 is running ---------------\n",
      "loss:0.49381383585518807, accuracy:0.8303927183151245\n",
      "Epoch 53/600 is running ---------------\n",
      "loss:0.49428938894436275, accuracy:0.8218554258346558\n",
      "Epoch 54/600 is running ---------------\n",
      "loss:0.4962419805855587, accuracy:0.8201479911804199\n",
      "Epoch 55/600 is running ---------------\n",
      "loss:0.4944046103748782, accuracy:0.822424590587616\n",
      "Epoch 56/600 is running ---------------\n",
      "loss:0.4911855248541668, accuracy:0.8275469541549683\n",
      "Epoch 57/600 is running ---------------\n",
      "loss:0.4933301169296791, accuracy:0.8258395195007324\n",
      "Epoch 58/600 is running ---------------\n",
      "loss:0.48833753528266116, accuracy:0.8315310478210449\n",
      "Epoch 59/600 is running ---------------\n",
      "loss:0.48821276734615193, accuracy:0.8321001529693604\n",
      "Epoch 60/600 is running ---------------\n",
      "loss:0.49401771148730966, accuracy:0.8264086246490479\n",
      "Epoch 61/600 is running ---------------\n",
      "loss:0.4883987569603427, accuracy:0.8303927183151245\n",
      "Epoch 62/600 is running ---------------\n",
      "loss:0.49154807678584395, accuracy:0.8264086246490479\n",
      "Epoch 63/600 is running ---------------\n",
      "loss:0.49447903242604485, accuracy:0.822424590587616\n",
      "Epoch 64/600 is running ---------------\n",
      "loss:0.4939872720118227, accuracy:0.8264086246490479\n",
      "Epoch 65/600 is running ---------------\n",
      "loss:0.49321697652339935, accuracy:0.8258395195007324\n",
      "Epoch 66/600 is running ---------------\n",
      "loss:0.49017367126612826, accuracy:0.8264086246490479\n",
      "Epoch 67/600 is running ---------------\n",
      "loss:0.48786080089108697, accuracy:0.8326693177223206\n",
      "Epoch 68/600 is running ---------------\n",
      "loss:0.4881081123804224, accuracy:0.8315310478210449\n",
      "Epoch 69/600 is running ---------------\n",
      "loss:0.4916237263843931, accuracy:0.8252703547477722\n",
      "Epoch 70/600 is running ---------------\n",
      "loss:0.48612967133522034, accuracy:0.8309618830680847\n",
      "Epoch 71/600 is running ---------------\n",
      "loss:0.489396124050535, accuracy:0.8298235535621643\n",
      "Epoch 72/600 is running ---------------\n",
      "loss:0.48908392673936385, accuracy:0.8321001529693604\n",
      "Epoch 73/600 is running ---------------\n",
      "loss:0.48958806796320553, accuracy:0.8281161189079285\n",
      "Epoch 74/600 is running ---------------\n",
      "loss:0.4881215146903334, accuracy:0.8349459171295166\n",
      "Epoch 75/600 is running ---------------\n",
      "loss:0.49132159506452494, accuracy:0.8258395195007324\n",
      "Epoch 76/600 is running ---------------\n",
      "loss:0.48558778803924035, accuracy:0.8372225165367126\n",
      "Epoch 77/600 is running ---------------\n",
      "loss:0.48839036246825906, accuracy:0.8326693177223206\n",
      "Epoch 78/600 is running ---------------\n",
      "loss:0.48819944724954406, accuracy:0.8292543888092041\n",
      "Epoch 79/600 is running ---------------\n",
      "loss:0.48809890551813717, accuracy:0.8292543888092041\n",
      "Epoch 80/600 is running ---------------\n",
      "loss:0.48621611656813785, accuracy:0.8315310478210449\n",
      "Epoch 81/600 is running ---------------\n",
      "loss:0.48983359645152913, accuracy:0.8264086246490479\n",
      "Epoch 82/600 is running ---------------\n",
      "loss:0.4883211861396658, accuracy:0.8286852836608887\n",
      "Epoch 83/600 is running ---------------\n",
      "loss:0.4868849824214804, accuracy:0.8321001529693604\n",
      "Epoch 84/600 is running ---------------\n",
      "loss:0.4903623996109798, accuracy:0.8292543888092041\n",
      "Epoch 85/600 is running ---------------\n",
      "loss:0.4806498145234996, accuracy:0.8389300107955933\n",
      "Epoch 86/600 is running ---------------\n",
      "loss:0.48506740352203104, accuracy:0.8343767523765564\n",
      "Epoch 87/600 is running ---------------\n",
      "loss:0.48462098477215604, accuracy:0.8349459171295166\n",
      "Epoch 88/600 is running ---------------\n",
      "loss:0.48831991131963404, accuracy:0.8292543888092041\n",
      "Epoch 89/600 is running ---------------\n",
      "loss:0.4852415333534109, accuracy:0.8321001529693604\n",
      "Epoch 90/600 is running ---------------\n",
      "loss:0.48735590895702097, accuracy:0.8321001529693604\n",
      "Epoch 91/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4867829197439654, accuracy:0.8298235535621643\n",
      "Epoch 92/600 is running ---------------\n",
      "loss:0.48411732500997084, accuracy:0.836084246635437\n",
      "Epoch 93/600 is running ---------------\n",
      "loss:0.48803047116460474, accuracy:0.8292543888092041\n",
      "Epoch 94/600 is running ---------------\n",
      "loss:0.48256920123922414, accuracy:0.8394991755485535\n",
      "Epoch 95/600 is running ---------------\n",
      "loss:0.4852413324446514, accuracy:0.8355150818824768\n",
      "Epoch 96/600 is running ---------------\n",
      "loss:0.48178608150317753, accuracy:0.8389300107955933\n",
      "Epoch 97/600 is running ---------------\n",
      "loss:0.4863604898082799, accuracy:0.836084246635437\n",
      "Epoch 98/600 is running ---------------\n",
      "loss:0.4851918050955082, accuracy:0.8315310478210449\n",
      "Epoch 99/600 is running ---------------\n",
      "loss:0.49013446374186154, accuracy:0.8258395195007324\n",
      "Epoch 100/600 is running ---------------\n",
      "loss:0.48023859694086274, accuracy:0.8389300107955933\n",
      "Epoch 101/600 is running ---------------\n",
      "loss:0.4831826301484272, accuracy:0.8343767523765564\n",
      "Epoch 102/600 is running ---------------\n",
      "loss:0.4806697579293415, accuracy:0.836084246635437\n",
      "Epoch 103/600 is running ---------------\n",
      "loss:0.48522198868208916, accuracy:0.8355150818824768\n",
      "Epoch 104/600 is running ---------------\n",
      "loss:0.4769654659361675, accuracy:0.8406374454498291\n",
      "Epoch 105/600 is running ---------------\n",
      "loss:0.4854543162830945, accuracy:0.8315310478210449\n",
      "Epoch 106/600 is running ---------------\n",
      "loss:0.48184385022212717, accuracy:0.8383608460426331\n",
      "Epoch 107/600 is running ---------------\n",
      "loss:0.4844242303535856, accuracy:0.8332384824752808\n",
      "Epoch 108/600 is running ---------------\n",
      "loss:0.481700933185117, accuracy:0.8349459171295166\n",
      "Epoch 109/600 is running ---------------\n",
      "loss:0.4822485837443122, accuracy:0.8377916812896729\n",
      "Epoch 110/600 is running ---------------\n",
      "loss:0.48028372639212114, accuracy:0.8412066102027893\n",
      "Epoch 111/600 is running ---------------\n",
      "loss:0.4777612742678872, accuracy:0.8412066102027893\n",
      "Epoch 112/600 is running ---------------\n",
      "loss:0.4797824266655692, accuracy:0.8355150818824768\n",
      "Epoch 113/600 is running ---------------\n",
      "loss:0.4800575550260215, accuracy:0.8400682806968689\n",
      "Epoch 114/600 is running ---------------\n",
      "loss:0.47525819468087166, accuracy:0.8451906442642212\n",
      "Epoch 115/600 is running ---------------\n",
      "loss:0.48071544334806243, accuracy:0.8349459171295166\n",
      "Epoch 116/600 is running ---------------\n",
      "loss:0.4836254834101118, accuracy:0.8372225165367126\n",
      "Epoch 117/600 is running ---------------\n",
      "loss:0.48339512224855097, accuracy:0.8315310478210449\n",
      "Epoch 118/600 is running ---------------\n",
      "loss:0.47801859985137807, accuracy:0.8423448801040649\n",
      "Epoch 119/600 is running ---------------\n",
      "loss:0.4823450081307313, accuracy:0.8372225165367126\n",
      "Epoch 120/600 is running ---------------\n",
      "loss:0.4774796752066448, accuracy:0.8400682806968689\n",
      "Epoch 121/600 is running ---------------\n",
      "loss:0.4765184881358311, accuracy:0.8463289737701416\n",
      "Epoch 122/600 is running ---------------\n",
      "loss:0.48346071150796166, accuracy:0.8343767523765564\n",
      "Epoch 123/600 is running ---------------\n",
      "loss:0.4761153731880517, accuracy:0.8423448801040649\n",
      "Epoch 124/600 is running ---------------\n",
      "loss:0.48068755710947103, accuracy:0.8372225165367126\n",
      "Epoch 125/600 is running ---------------\n",
      "loss:0.480414568864066, accuracy:0.8366534113883972\n",
      "Epoch 126/600 is running ---------------\n",
      "loss:0.48101114507379206, accuracy:0.8377916812896729\n",
      "Epoch 127/600 is running ---------------\n",
      "loss:0.4779045006324505, accuracy:0.8400682806968689\n",
      "Epoch 128/600 is running ---------------\n",
      "loss:0.47944382552442877, accuracy:0.8389300107955933\n",
      "Epoch 129/600 is running ---------------\n",
      "loss:0.48031311096816226, accuracy:0.8383608460426331\n",
      "Epoch 130/600 is running ---------------\n",
      "loss:0.47728502442096843, accuracy:0.8440523743629456\n",
      "Epoch 131/600 is running ---------------\n",
      "loss:0.4771665344978201, accuracy:0.8429140448570251\n",
      "Epoch 132/600 is running ---------------\n",
      "loss:0.4737766035671892, accuracy:0.8457598090171814\n",
      "Epoch 133/600 is running ---------------\n",
      "loss:0.47979691213574904, accuracy:0.8372225165367126\n",
      "Epoch 134/600 is running ---------------\n",
      "loss:0.4795080844698281, accuracy:0.8394991755485535\n",
      "Epoch 135/600 is running ---------------\n",
      "loss:0.47697528580139426, accuracy:0.8446215391159058\n",
      "Epoch 136/600 is running ---------------\n",
      "loss:0.4742220629905832, accuracy:0.8451906442642212\n",
      "Epoch 137/600 is running ---------------\n",
      "loss:0.47778045668684205, accuracy:0.8440523743629456\n",
      "Epoch 138/600 is running ---------------\n",
      "loss:0.47915573931973554, accuracy:0.8406374454498291\n",
      "Epoch 139/600 is running ---------------\n",
      "loss:0.4771491340522108, accuracy:0.8417757749557495\n",
      "Epoch 140/600 is running ---------------\n",
      "loss:0.47582687951367475, accuracy:0.8451906442642212\n",
      "Epoch 141/600 is running ---------------\n",
      "loss:0.4802188051157984, accuracy:0.8372225165367126\n",
      "Epoch 142/600 is running ---------------\n",
      "loss:0.4753490609341654, accuracy:0.8394991755485535\n",
      "Epoch 143/600 is running ---------------\n",
      "loss:0.47052187446890203, accuracy:0.8503130078315735\n",
      "Epoch 144/600 is running ---------------\n",
      "loss:0.4778060183442872, accuracy:0.8457598090171814\n",
      "Epoch 145/600 is running ---------------\n",
      "loss:0.4715163276113313, accuracy:0.8486055731773376\n",
      "Epoch 146/600 is running ---------------\n",
      "loss:0.477554127573967, accuracy:0.8423448801040649\n",
      "Epoch 147/600 is running ---------------\n",
      "loss:0.4804395406410612, accuracy:0.8377916812896729\n",
      "Epoch 148/600 is running ---------------\n",
      "loss:0.47536864208764046, accuracy:0.8446215391159058\n",
      "Epoch 149/600 is running ---------------\n",
      "loss:0.4769170818657711, accuracy:0.8406374454498291\n",
      "Epoch 150/600 is running ---------------\n",
      "loss:0.4782232020435662, accuracy:0.8412066102027893\n",
      "Epoch 151/600 is running ---------------\n",
      "loss:0.47824111118398865, accuracy:0.8412066102027893\n",
      "Epoch 152/600 is running ---------------\n",
      "loss:0.4749506902078102, accuracy:0.8412066102027893\n",
      "Epoch 153/600 is running ---------------\n",
      "loss:0.4751211543535364, accuracy:0.8457598090171814\n",
      "Epoch 154/600 is running ---------------\n",
      "loss:0.47688144856485826, accuracy:0.8423448801040649\n",
      "Epoch 155/600 is running ---------------\n",
      "loss:0.4772404411743427, accuracy:0.8429140448570251\n",
      "Epoch 156/600 is running ---------------\n",
      "loss:0.4767869274164068, accuracy:0.8434832096099854\n",
      "Epoch 157/600 is running ---------------\n",
      "loss:0.4775865936073764, accuracy:0.8412066102027893\n",
      "Epoch 158/600 is running ---------------\n",
      "loss:0.4754836220165779, accuracy:0.8429140448570251\n",
      "Epoch 159/600 is running ---------------\n",
      "loss:0.4721835245346201, accuracy:0.847467303276062\n",
      "Epoch 160/600 is running ---------------\n",
      "loss:0.47229042443735847, accuracy:0.8457598090171814\n",
      "Epoch 161/600 is running ---------------\n",
      "loss:0.47522809639059266, accuracy:0.8440523743629456\n",
      "Epoch 162/600 is running ---------------\n",
      "loss:0.47433948414079075, accuracy:0.8440523743629456\n",
      "Epoch 163/600 is running ---------------\n",
      "loss:0.4782110087830445, accuracy:0.8383608460426331\n",
      "Epoch 164/600 is running ---------------\n",
      "loss:0.4796229189839856, accuracy:0.8412066102027893\n",
      "Epoch 165/600 is running ---------------\n",
      "loss:0.47617841229356567, accuracy:0.8434832096099854\n",
      "Epoch 166/600 is running ---------------\n",
      "loss:0.47391412895301294, accuracy:0.8480364084243774\n",
      "Epoch 167/600 is running ---------------\n",
      "loss:0.47175597733464736, accuracy:0.8468981385231018\n",
      "Epoch 168/600 is running ---------------\n",
      "loss:0.47210589205396586, accuracy:0.8463289737701416\n",
      "Epoch 169/600 is running ---------------\n",
      "loss:0.47287609453859003, accuracy:0.8468981385231018\n",
      "Epoch 170/600 is running ---------------\n",
      "loss:0.4750306601154393, accuracy:0.8434832096099854\n",
      "Epoch 171/600 is running ---------------\n",
      "loss:0.47372222152249566, accuracy:0.8463289737701416\n",
      "Epoch 172/600 is running ---------------\n",
      "loss:0.47466079171361597, accuracy:0.8457598090171814\n",
      "Epoch 173/600 is running ---------------\n",
      "loss:0.47337009372382327, accuracy:0.8457598090171814\n",
      "Epoch 174/600 is running ---------------\n",
      "loss:0.4736015544883136, accuracy:0.8440523743629456\n",
      "Epoch 175/600 is running ---------------\n",
      "loss:0.47566345745119554, accuracy:0.8440523743629456\n",
      "Epoch 176/600 is running ---------------\n",
      "loss:0.47250905735739346, accuracy:0.8457598090171814\n",
      "Epoch 177/600 is running ---------------\n",
      "loss:0.47278850448542625, accuracy:0.8468981385231018\n",
      "Epoch 178/600 is running ---------------\n",
      "loss:0.4727416542069665, accuracy:0.8463289737701416\n",
      "Epoch 179/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4719471453592695, accuracy:0.847467303276062\n",
      "Epoch 180/600 is running ---------------\n",
      "loss:0.47287996729900095, accuracy:0.8468981385231018\n",
      "Epoch 181/600 is running ---------------\n",
      "loss:0.4763575412076095, accuracy:0.8434832096099854\n",
      "Epoch 182/600 is running ---------------\n",
      "loss:0.4706680019353998, accuracy:0.847467303276062\n",
      "Epoch 183/600 is running ---------------\n",
      "loss:0.4732658899035947, accuracy:0.8497439026832581\n",
      "Epoch 184/600 is running ---------------\n",
      "loss:0.4711189305987851, accuracy:0.8457598090171814\n",
      "Epoch 185/600 is running ---------------\n",
      "loss:0.468126157234455, accuracy:0.8537279367446899\n",
      "Epoch 186/600 is running ---------------\n",
      "loss:0.4761507434063944, accuracy:0.8400682806968689\n",
      "Epoch 187/600 is running ---------------\n",
      "loss:0.4712994196291628, accuracy:0.847467303276062\n",
      "Epoch 188/600 is running ---------------\n",
      "loss:0.4721853085632982, accuracy:0.847467303276062\n",
      "Epoch 189/600 is running ---------------\n",
      "loss:0.47131114427385656, accuracy:0.8486055731773376\n",
      "Epoch 190/600 is running ---------------\n",
      "loss:0.46977430787579766, accuracy:0.8491747379302979\n",
      "Epoch 191/600 is running ---------------\n",
      "loss:0.47429251722220717, accuracy:0.8463289737701416\n",
      "Epoch 192/600 is running ---------------\n",
      "loss:0.4721258493333027, accuracy:0.847467303276062\n",
      "Epoch 193/600 is running ---------------\n",
      "loss:0.47436602721954213, accuracy:0.8440523743629456\n",
      "Epoch 194/600 is running ---------------\n",
      "loss:0.4703856917290852, accuracy:0.8525896668434143\n",
      "Epoch 195/600 is running ---------------\n",
      "loss:0.47002747500764913, accuracy:0.8497439026832581\n",
      "Epoch 196/600 is running ---------------\n",
      "loss:0.46692869529641906, accuracy:0.8514513373374939\n",
      "Epoch 197/600 is running ---------------\n",
      "loss:0.4698188751935959, accuracy:0.8508821725845337\n",
      "Epoch 198/600 is running ---------------\n",
      "loss:0.4719287285517002, accuracy:0.8480364084243774\n",
      "Epoch 199/600 is running ---------------\n",
      "loss:0.47280098395100956, accuracy:0.8417757749557495\n",
      "Epoch 200/600 is running ---------------\n",
      "loss:0.4698542747004279, accuracy:0.8497439026832581\n",
      "Epoch 201/600 is running ---------------\n",
      "loss:0.47069207399055873, accuracy:0.8468981385231018\n",
      "Epoch 202/600 is running ---------------\n",
      "loss:0.4700821879608878, accuracy:0.8480364084243774\n",
      "Epoch 203/600 is running ---------------\n",
      "loss:0.465487116883541, accuracy:0.8531587719917297\n",
      "Epoch 204/600 is running ---------------\n",
      "loss:0.473057309615201, accuracy:0.8423448801040649\n",
      "Epoch 205/600 is running ---------------\n",
      "loss:0.47153089375331486, accuracy:0.8486055731773376\n",
      "Epoch 206/600 is running ---------------\n",
      "loss:0.47192758218995456, accuracy:0.847467303276062\n",
      "Epoch 207/600 is running ---------------\n",
      "loss:0.47567256051918555, accuracy:0.8440523743629456\n",
      "Epoch 208/600 is running ---------------\n",
      "loss:0.4735947122861599, accuracy:0.8451906442642212\n",
      "Epoch 209/600 is running ---------------\n",
      "loss:0.47020834632988634, accuracy:0.8503130078315735\n",
      "Epoch 210/600 is running ---------------\n",
      "loss:0.47043766132716475, accuracy:0.8463289737701416\n",
      "Epoch 211/600 is running ---------------\n",
      "loss:0.4679498328217145, accuracy:0.8514513373374939\n",
      "Epoch 212/600 is running ---------------\n",
      "loss:0.4725410254864857, accuracy:0.8491747379302979\n",
      "Epoch 213/600 is running ---------------\n",
      "loss:0.4706500691586527, accuracy:0.8503130078315735\n",
      "Epoch 214/600 is running ---------------\n",
      "loss:0.47101848649567574, accuracy:0.8480364084243774\n",
      "Epoch 215/600 is running ---------------\n",
      "loss:0.4716137519170498, accuracy:0.8486055731773376\n",
      "Epoch 216/600 is running ---------------\n",
      "loss:0.4677143323010412, accuracy:0.8542971014976501\n",
      "Epoch 217/600 is running ---------------\n",
      "loss:0.46809490693026573, accuracy:0.8491747379302979\n",
      "Epoch 218/600 is running ---------------\n",
      "loss:0.46683045091300174, accuracy:0.8531587719917297\n",
      "Epoch 219/600 is running ---------------\n",
      "loss:0.4707136755359584, accuracy:0.8508821725845337\n",
      "Epoch 220/600 is running ---------------\n",
      "loss:0.474202424287796, accuracy:0.8457598090171814\n",
      "Epoch 221/600 is running ---------------\n",
      "loss:0.4734496371499423, accuracy:0.8434832096099854\n",
      "Epoch 222/600 is running ---------------\n",
      "loss:0.47044856178349465, accuracy:0.8457598090171814\n",
      "Epoch 223/600 is running ---------------\n",
      "loss:0.47115092133653574, accuracy:0.847467303276062\n",
      "Epoch 224/600 is running ---------------\n",
      "loss:0.4698882755534402, accuracy:0.8514513373374939\n",
      "Epoch 225/600 is running ---------------\n",
      "loss:0.47085140028904227, accuracy:0.8486055731773376\n",
      "Epoch 226/600 is running ---------------\n",
      "loss:0.4663077808659652, accuracy:0.8514513373374939\n",
      "Epoch 227/600 is running ---------------\n",
      "loss:0.46524416886526965, accuracy:0.8565737009048462\n",
      "Epoch 228/600 is running ---------------\n",
      "loss:0.4710870010071787, accuracy:0.8497439026832581\n",
      "Epoch 229/600 is running ---------------\n",
      "loss:0.46650989302273455, accuracy:0.8520205020904541\n",
      "Epoch 230/600 is running ---------------\n",
      "loss:0.47334840945128737, accuracy:0.8457598090171814\n",
      "Epoch 231/600 is running ---------------\n",
      "loss:0.4671611580355414, accuracy:0.8491747379302979\n",
      "Epoch 232/600 is running ---------------\n",
      "loss:0.47347391473835915, accuracy:0.8457598090171814\n",
      "Epoch 233/600 is running ---------------\n",
      "loss:0.46783511854451276, accuracy:0.8497439026832581\n",
      "Epoch 234/600 is running ---------------\n",
      "loss:0.46878748669706544, accuracy:0.8480364084243774\n",
      "Epoch 235/600 is running ---------------\n",
      "loss:0.46519406499533816, accuracy:0.856004536151886\n",
      "Epoch 236/600 is running ---------------\n",
      "loss:0.4775765497108986, accuracy:0.8394991755485535\n",
      "Epoch 237/600 is running ---------------\n",
      "loss:0.46835148231736545, accuracy:0.8503130078315735\n",
      "Epoch 238/600 is running ---------------\n",
      "loss:0.4665408530111971, accuracy:0.8520205020904541\n",
      "Epoch 239/600 is running ---------------\n",
      "loss:0.4682558333051616, accuracy:0.8520205020904541\n",
      "Epoch 240/600 is running ---------------\n",
      "loss:0.4706993991958684, accuracy:0.8491747379302979\n",
      "Epoch 241/600 is running ---------------\n",
      "loss:0.47119451596819123, accuracy:0.8486055731773376\n",
      "Epoch 242/600 is running ---------------\n",
      "loss:0.4639845901522143, accuracy:0.8565737009048462\n",
      "Epoch 243/600 is running ---------------\n",
      "loss:0.46961681236480846, accuracy:0.8486055731773376\n",
      "Epoch 244/600 is running ---------------\n",
      "loss:0.4693868715187599, accuracy:0.8497439026832581\n",
      "Epoch 245/600 is running ---------------\n",
      "loss:0.47196697674948596, accuracy:0.8508821725845337\n",
      "Epoch 246/600 is running ---------------\n",
      "loss:0.4656142607845109, accuracy:0.8520205020904541\n",
      "Epoch 247/600 is running ---------------\n",
      "loss:0.46618854999542236, accuracy:0.8520205020904541\n",
      "Epoch 248/600 is running ---------------\n",
      "loss:0.4671112160230505, accuracy:0.8531587719917297\n",
      "Epoch 249/600 is running ---------------\n",
      "loss:0.4694100885555662, accuracy:0.8486055731773376\n",
      "Epoch 250/600 is running ---------------\n",
      "loss:0.4695228050495016, accuracy:0.8457598090171814\n",
      "Epoch 251/600 is running ---------------\n",
      "loss:0.46356166899204254, accuracy:0.856004536151886\n",
      "Epoch 252/600 is running ---------------\n",
      "loss:0.4670796111739915, accuracy:0.8514513373374939\n",
      "Epoch 253/600 is running ---------------\n",
      "loss:0.46847214986538066, accuracy:0.8520205020904541\n",
      "Epoch 254/600 is running ---------------\n",
      "loss:0.4674985568071234, accuracy:0.8497439026832581\n",
      "Epoch 255/600 is running ---------------\n",
      "loss:0.46599070625058536, accuracy:0.8514513373374939\n",
      "Epoch 256/600 is running ---------------\n",
      "loss:0.46790344345158547, accuracy:0.8542971014976501\n",
      "Epoch 257/600 is running ---------------\n",
      "loss:0.466332353394607, accuracy:0.8520205020904541\n",
      "Epoch 258/600 is running ---------------\n",
      "loss:0.4662620471469287, accuracy:0.8520205020904541\n",
      "Epoch 259/600 is running ---------------\n",
      "loss:0.46990836386022894, accuracy:0.8480364084243774\n",
      "Epoch 260/600 is running ---------------\n",
      "loss:0.46649768640255107, accuracy:0.8525896668434143\n",
      "Epoch 261/600 is running ---------------\n",
      "loss:0.4634464460200277, accuracy:0.8537279367446899\n",
      "Epoch 262/600 is running ---------------\n",
      "loss:0.4635002613067627, accuracy:0.8577120304107666\n",
      "Epoch 263/600 is running ---------------\n",
      "loss:0.4693152488305651, accuracy:0.8514513373374939\n",
      "Epoch 264/600 is running ---------------\n",
      "loss:0.4689651213843247, accuracy:0.8503130078315735\n",
      "Epoch 265/600 is running ---------------\n",
      "loss:0.468314414908146, accuracy:0.8508821725845337\n",
      "Epoch 266/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4622325182988726, accuracy:0.858281135559082\n",
      "Epoch 267/600 is running ---------------\n",
      "loss:0.46281746851986855, accuracy:0.8571428656578064\n",
      "Epoch 268/600 is running ---------------\n",
      "loss:0.4641553399891689, accuracy:0.8565737009048462\n",
      "Epoch 269/600 is running ---------------\n",
      "loss:0.467639052148523, accuracy:0.8525896668434143\n",
      "Epoch 270/600 is running ---------------\n",
      "loss:0.46575794651590546, accuracy:0.8542971014976501\n",
      "Epoch 271/600 is running ---------------\n",
      "loss:0.4652497634805482, accuracy:0.8542971014976501\n",
      "Epoch 272/600 is running ---------------\n",
      "loss:0.4646656888312307, accuracy:0.856004536151886\n",
      "Epoch 273/600 is running ---------------\n",
      "loss:0.4653011889293276, accuracy:0.8542971014976501\n",
      "Epoch 274/600 is running ---------------\n",
      "loss:0.4662158915708805, accuracy:0.8537279367446899\n",
      "Epoch 275/600 is running ---------------\n",
      "loss:0.46629223844100687, accuracy:0.8548662662506104\n",
      "Epoch 276/600 is running ---------------\n",
      "loss:0.4670234317409581, accuracy:0.8514513373374939\n",
      "Epoch 277/600 is running ---------------\n",
      "loss:0.465593571292943, accuracy:0.8542971014976501\n",
      "Epoch 278/600 is running ---------------\n",
      "loss:0.4651984392568983, accuracy:0.8514513373374939\n",
      "Epoch 279/600 is running ---------------\n",
      "loss:0.46364386790785295, accuracy:0.856004536151886\n",
      "Epoch 280/600 is running ---------------\n",
      "loss:0.462668364417964, accuracy:0.8588503003120422\n",
      "Epoch 281/600 is running ---------------\n",
      "loss:0.4671680439135124, accuracy:0.8491747379302979\n",
      "Epoch 282/600 is running ---------------\n",
      "loss:0.46326536813686636, accuracy:0.8548662662506104\n",
      "Epoch 283/600 is running ---------------\n",
      "loss:0.461702564153178, accuracy:0.858281135559082\n",
      "Epoch 284/600 is running ---------------\n",
      "loss:0.46118641824557866, accuracy:0.8594194650650024\n",
      "Epoch 285/600 is running ---------------\n",
      "loss:0.46460429964394406, accuracy:0.8514513373374939\n",
      "Epoch 286/600 is running ---------------\n",
      "loss:0.46456195414066315, accuracy:0.856004536151886\n",
      "Epoch 287/600 is running ---------------\n",
      "loss:0.4648541314848538, accuracy:0.8531587719917297\n",
      "Epoch 288/600 is running ---------------\n",
      "loss:0.45919372244127865, accuracy:0.8616960644721985\n",
      "Epoch 289/600 is running ---------------\n",
      "loss:0.4690521529008602, accuracy:0.8468981385231018\n",
      "Epoch 290/600 is running ---------------\n",
      "loss:0.45887810659819633, accuracy:0.8634034991264343\n",
      "Epoch 291/600 is running ---------------\n",
      "loss:0.46414914675827684, accuracy:0.8537279367446899\n",
      "Epoch 292/600 is running ---------------\n",
      "loss:0.46351524108442765, accuracy:0.8565737009048462\n",
      "Epoch 293/600 is running ---------------\n",
      "loss:0.46535164785796196, accuracy:0.856004536151886\n",
      "Epoch 294/600 is running ---------------\n",
      "loss:0.4629104111728997, accuracy:0.8554354310035706\n",
      "Epoch 295/600 is running ---------------\n",
      "loss:0.46440258365252923, accuracy:0.856004536151886\n",
      "Epoch 296/600 is running ---------------\n",
      "loss:0.46215951083035306, accuracy:0.858281135559082\n",
      "Epoch 297/600 is running ---------------\n",
      "loss:0.46521435266938704, accuracy:0.8520205020904541\n",
      "Epoch 298/600 is running ---------------\n",
      "loss:0.4639484841248085, accuracy:0.8554354310035706\n",
      "Epoch 299/600 is running ---------------\n",
      "loss:0.4671190946266569, accuracy:0.8508821725845337\n",
      "Epoch 300/600 is running ---------------\n",
      "loss:0.46632276732346106, accuracy:0.8542971014976501\n",
      "Epoch 301/600 is running ---------------\n",
      "loss:0.4586638417737237, accuracy:0.8605577945709229\n",
      "Epoch 302/600 is running ---------------\n",
      "loss:0.4611637710497297, accuracy:0.8588503003120422\n",
      "Epoch 303/600 is running ---------------\n",
      "loss:0.4551624784181858, accuracy:0.8662492632865906\n",
      "Epoch 304/600 is running ---------------\n",
      "loss:0.45973281870628224, accuracy:0.8639726638793945\n",
      "Epoch 305/600 is running ---------------\n",
      "loss:0.463963995205945, accuracy:0.8554354310035706\n",
      "Epoch 306/600 is running ---------------\n",
      "loss:0.4638602070767304, accuracy:0.8571428656578064\n",
      "Epoch 307/600 is running ---------------\n",
      "loss:0.45996868764532023, accuracy:0.8611268997192383\n",
      "Epoch 308/600 is running ---------------\n",
      "loss:0.45667396023355683, accuracy:0.8605577945709229\n",
      "Epoch 309/600 is running ---------------\n",
      "loss:0.45801285834148014, accuracy:0.8645418286323547\n",
      "Epoch 310/600 is running ---------------\n",
      "loss:0.4597949955997796, accuracy:0.858281135559082\n",
      "Epoch 311/600 is running ---------------\n",
      "loss:0.46151032735561504, accuracy:0.858281135559082\n",
      "Epoch 312/600 is running ---------------\n",
      "loss:0.4589370160267271, accuracy:0.8622652292251587\n",
      "Epoch 313/600 is running ---------------\n",
      "loss:0.45496514799266025, accuracy:0.8645418286323547\n",
      "Epoch 314/600 is running ---------------\n",
      "loss:0.4561742302672616, accuracy:0.8639726638793945\n",
      "Epoch 315/600 is running ---------------\n",
      "loss:0.45983023684600305, accuracy:0.8599886298179626\n",
      "Epoch 316/600 is running ---------------\n",
      "loss:0.4596100624265342, accuracy:0.8571428656578064\n",
      "Epoch 317/600 is running ---------------\n",
      "loss:0.4584397992183422, accuracy:0.8634034991264343\n",
      "Epoch 318/600 is running ---------------\n",
      "loss:0.4596741785263193, accuracy:0.858281135559082\n",
      "Epoch 319/600 is running ---------------\n",
      "loss:0.46072411691320353, accuracy:0.8605577945709229\n",
      "Epoch 320/600 is running ---------------\n",
      "loss:0.4564122989259917, accuracy:0.8645418286323547\n",
      "Epoch 321/600 is running ---------------\n",
      "loss:0.4566082019230415, accuracy:0.867387592792511\n",
      "Epoch 322/600 is running ---------------\n",
      "loss:0.4580196486464862, accuracy:0.8651109933853149\n",
      "Epoch 323/600 is running ---------------\n",
      "loss:0.45636129070972575, accuracy:0.8645418286323547\n",
      "Epoch 324/600 is running ---------------\n",
      "loss:0.45817574344832324, accuracy:0.8616960644721985\n",
      "Epoch 325/600 is running ---------------\n",
      "loss:0.4587825932379427, accuracy:0.8605577945709229\n",
      "Epoch 326/600 is running ---------------\n",
      "loss:0.4607328125115099, accuracy:0.8577120304107666\n",
      "Epoch 327/600 is running ---------------\n",
      "loss:0.4591051000973274, accuracy:0.8599886298179626\n",
      "Epoch 328/600 is running ---------------\n",
      "loss:0.45858872039564724, accuracy:0.8588503003120422\n",
      "Epoch 329/600 is running ---------------\n",
      "loss:0.4641841372539257, accuracy:0.8554354310035706\n",
      "Epoch 330/600 is running ---------------\n",
      "loss:0.45791492636861475, accuracy:0.8645418286323547\n",
      "Epoch 331/600 is running ---------------\n",
      "loss:0.45848821566022674, accuracy:0.8616960644721985\n",
      "Epoch 332/600 is running ---------------\n",
      "loss:0.4584410611925454, accuracy:0.8616960644721985\n",
      "Epoch 333/600 is running ---------------\n",
      "loss:0.45491989470761396, accuracy:0.8668184280395508\n",
      "Epoch 334/600 is running ---------------\n",
      "loss:0.4587124765946947, accuracy:0.8611268997192383\n",
      "Epoch 335/600 is running ---------------\n",
      "loss:0.45480448572800075, accuracy:0.8685259222984314\n",
      "Epoch 336/600 is running ---------------\n",
      "loss:0.4605667955916503, accuracy:0.8622652292251587\n",
      "Epoch 337/600 is running ---------------\n",
      "loss:0.45685682111772996, accuracy:0.8622652292251587\n",
      "Epoch 338/600 is running ---------------\n",
      "loss:0.4573915682989976, accuracy:0.8628343939781189\n",
      "Epoch 339/600 is running ---------------\n",
      "loss:0.4590983986854553, accuracy:0.8616960644721985\n",
      "Epoch 340/600 is running ---------------\n",
      "loss:0.46046444825057326, accuracy:0.8599886298179626\n",
      "Epoch 341/600 is running ---------------\n",
      "loss:0.45858814130569325, accuracy:0.8639726638793945\n",
      "Epoch 342/600 is running ---------------\n",
      "loss:0.45820012174803637, accuracy:0.8594194650650024\n",
      "Epoch 343/600 is running ---------------\n",
      "loss:0.45886928870760163, accuracy:0.8594194650650024\n",
      "Epoch 344/600 is running ---------------\n",
      "loss:0.45947945323483697, accuracy:0.8616960644721985\n",
      "Epoch 345/600 is running ---------------\n",
      "loss:0.4563816451820834, accuracy:0.8639726638793945\n",
      "Epoch 346/600 is running ---------------\n",
      "loss:0.4625886868813942, accuracy:0.8599886298179626\n",
      "Epoch 347/600 is running ---------------\n",
      "loss:0.45996344808874456, accuracy:0.8588503003120422\n",
      "Epoch 348/600 is running ---------------\n",
      "loss:0.46548512783543816, accuracy:0.8525896668434143\n",
      "Epoch 349/600 is running ---------------\n",
      "loss:0.45916052727863704, accuracy:0.8605577945709229\n",
      "Epoch 350/600 is running ---------------\n",
      "loss:0.4608537842487467, accuracy:0.8588503003120422\n",
      "Epoch 351/600 is running ---------------\n",
      "loss:0.4609111028498617, accuracy:0.8611268997192383\n",
      "Epoch 352/600 is running ---------------\n",
      "loss:0.4580879920515521, accuracy:0.8634034991264343\n",
      "Epoch 353/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4580982676867781, accuracy:0.8634034991264343\n",
      "Epoch 354/600 is running ---------------\n",
      "loss:0.4573621117863162, accuracy:0.8634034991264343\n",
      "Epoch 355/600 is running ---------------\n",
      "loss:0.4547818955676309, accuracy:0.8656801581382751\n",
      "Epoch 356/600 is running ---------------\n",
      "loss:0.45625265148179284, accuracy:0.8656801581382751\n",
      "Epoch 357/600 is running ---------------\n",
      "loss:0.459258457710003, accuracy:0.8588503003120422\n",
      "Epoch 358/600 is running ---------------\n",
      "loss:0.4584758595145982, accuracy:0.8616960644721985\n",
      "Epoch 359/600 is running ---------------\n",
      "loss:0.45878404789957505, accuracy:0.8611268997192383\n",
      "Epoch 360/600 is running ---------------\n",
      "loss:0.4563963685570092, accuracy:0.8656801581382751\n",
      "Epoch 361/600 is running ---------------\n",
      "loss:0.459600614062671, accuracy:0.8611268997192383\n",
      "Epoch 362/600 is running ---------------\n",
      "loss:0.4553799880989667, accuracy:0.8639726638793945\n",
      "Epoch 363/600 is running ---------------\n",
      "loss:0.45816376599772224, accuracy:0.8622652292251587\n",
      "Epoch 364/600 is running ---------------\n",
      "loss:0.45715912508553475, accuracy:0.8656801581382751\n",
      "Epoch 365/600 is running ---------------\n",
      "loss:0.46010379904303056, accuracy:0.8577120304107666\n",
      "Epoch 366/600 is running ---------------\n",
      "loss:0.4593429848037917, accuracy:0.8599886298179626\n",
      "Epoch 367/600 is running ---------------\n",
      "loss:0.4551205450090869, accuracy:0.8645418286323547\n",
      "Epoch 368/600 is running ---------------\n",
      "loss:0.45403969904472086, accuracy:0.867387592792511\n",
      "Epoch 369/600 is running ---------------\n",
      "loss:0.4576219654288785, accuracy:0.8616960644721985\n",
      "Epoch 370/600 is running ---------------\n",
      "loss:0.4573592235302103, accuracy:0.8628343939781189\n",
      "Epoch 371/600 is running ---------------\n",
      "loss:0.4549137934528548, accuracy:0.8685259222984314\n",
      "Epoch 372/600 is running ---------------\n",
      "loss:0.4591879053362485, accuracy:0.8599886298179626\n",
      "Epoch 373/600 is running ---------------\n",
      "loss:0.4562347824203557, accuracy:0.8651109933853149\n",
      "Epoch 374/600 is running ---------------\n",
      "loss:0.4571901061411562, accuracy:0.8645418286323547\n",
      "Epoch 375/600 is running ---------------\n",
      "loss:0.4577415446782934, accuracy:0.8639726638793945\n",
      "Epoch 376/600 is running ---------------\n",
      "loss:0.4579232633113861, accuracy:0.8645418286323547\n",
      "Epoch 377/600 is running ---------------\n",
      "loss:0.457231264689873, accuracy:0.8662492632865906\n",
      "Epoch 378/600 is running ---------------\n",
      "loss:0.45707139424208937, accuracy:0.8651109933853149\n",
      "Epoch 379/600 is running ---------------\n",
      "loss:0.4538634994934345, accuracy:0.867387592792511\n",
      "Epoch 380/600 is running ---------------\n",
      "loss:0.45419426056845436, accuracy:0.8690950274467468\n",
      "Epoch 381/600 is running ---------------\n",
      "loss:0.4577999022500268, accuracy:0.8611268997192383\n",
      "Epoch 382/600 is running ---------------\n",
      "loss:0.4609373987748705, accuracy:0.8577120304107666\n",
      "Epoch 383/600 is running ---------------\n",
      "loss:0.4553914753527477, accuracy:0.8662492632865906\n",
      "Epoch 384/600 is running ---------------\n",
      "loss:0.45877663791179657, accuracy:0.8611268997192383\n",
      "Epoch 385/600 is running ---------------\n",
      "loss:0.4577997676257429, accuracy:0.8622652292251587\n",
      "Epoch 386/600 is running ---------------\n",
      "loss:0.4575185672990207, accuracy:0.8645418286323547\n",
      "Epoch 387/600 is running ---------------\n",
      "loss:0.45595235865691613, accuracy:0.8616960644721985\n",
      "Epoch 388/600 is running ---------------\n",
      "loss:0.46042508606252996, accuracy:0.8594194650650024\n",
      "Epoch 389/600 is running ---------------\n",
      "loss:0.4597519677260826, accuracy:0.8605577945709229\n",
      "Epoch 390/600 is running ---------------\n",
      "loss:0.4609219914880292, accuracy:0.8571428656578064\n",
      "Epoch 391/600 is running ---------------\n",
      "loss:0.4553131255610236, accuracy:0.8634034991264343\n",
      "Epoch 392/600 is running ---------------\n",
      "loss:0.46018535618124334, accuracy:0.8605577945709229\n",
      "Epoch 393/600 is running ---------------\n",
      "loss:0.45701746950889455, accuracy:0.8639726638793945\n",
      "Epoch 394/600 is running ---------------\n",
      "loss:0.45935721181589984, accuracy:0.8605577945709229\n",
      "Epoch 395/600 is running ---------------\n",
      "loss:0.46034088422512187, accuracy:0.8571428656578064\n",
      "Epoch 396/600 is running ---------------\n",
      "loss:0.45745525123744174, accuracy:0.8628343939781189\n",
      "Epoch 397/600 is running ---------------\n",
      "loss:0.45513272131311483, accuracy:0.8662492632865906\n",
      "Epoch 398/600 is running ---------------\n",
      "loss:0.4592064423807736, accuracy:0.8605577945709229\n",
      "Epoch 399/600 is running ---------------\n",
      "loss:0.45773740883531244, accuracy:0.8628343939781189\n",
      "Epoch 400/600 is running ---------------\n",
      "loss:0.4584583891876813, accuracy:0.8611268997192383\n",
      "Epoch 401/600 is running ---------------\n",
      "loss:0.4595286661180957, accuracy:0.8622652292251587\n",
      "Epoch 402/600 is running ---------------\n",
      "loss:0.46332355871282777, accuracy:0.8542971014976501\n",
      "Epoch 403/600 is running ---------------\n",
      "loss:0.4568371880671074, accuracy:0.8639726638793945\n",
      "Epoch 404/600 is running ---------------\n",
      "loss:0.4574680909000594, accuracy:0.8599886298179626\n",
      "Epoch 405/600 is running ---------------\n",
      "loss:0.4549369627031787, accuracy:0.8656801581382751\n",
      "Epoch 406/600 is running ---------------\n",
      "loss:0.453923346153621, accuracy:0.8662492632865906\n",
      "Epoch 407/600 is running ---------------\n",
      "loss:0.4586522805279699, accuracy:0.8628343939781189\n",
      "Epoch 408/600 is running ---------------\n",
      "loss:0.45967358607670356, accuracy:0.8588503003120422\n",
      "Epoch 409/600 is running ---------------\n",
      "loss:0.4613507395160609, accuracy:0.8588503003120422\n",
      "Epoch 410/600 is running ---------------\n",
      "loss:0.4562532840103939, accuracy:0.8656801581382751\n",
      "Epoch 411/600 is running ---------------\n",
      "loss:0.45449942093471, accuracy:0.8668184280395508\n",
      "Epoch 412/600 is running ---------------\n",
      "loss:0.4604244792255862, accuracy:0.8599886298179626\n",
      "Epoch 413/600 is running ---------------\n",
      "loss:0.4545767481984763, accuracy:0.8690950274467468\n",
      "Epoch 414/600 is running ---------------\n",
      "loss:0.45748089356669064, accuracy:0.8616960644721985\n",
      "Epoch 415/600 is running ---------------\n",
      "loss:0.45564101328109874, accuracy:0.8651109933853149\n",
      "Epoch 416/600 is running ---------------\n",
      "loss:0.45540362390978584, accuracy:0.8645418286323547\n",
      "Epoch 417/600 is running ---------------\n",
      "loss:0.45820184463057023, accuracy:0.8616960644721985\n",
      "Epoch 418/600 is running ---------------\n",
      "loss:0.45765156375950783, accuracy:0.8599886298179626\n",
      "Epoch 419/600 is running ---------------\n",
      "loss:0.4579452646189722, accuracy:0.8622652292251587\n",
      "Epoch 420/600 is running ---------------\n",
      "loss:0.4557946627510005, accuracy:0.8656801581382751\n",
      "Epoch 421/600 is running ---------------\n",
      "loss:0.45774524427693464, accuracy:0.8628343939781189\n",
      "Epoch 422/600 is running ---------------\n",
      "loss:0.4585512321570824, accuracy:0.8634034991264343\n",
      "Epoch 423/600 is running ---------------\n",
      "loss:0.45636637919935685, accuracy:0.8639726638793945\n",
      "Epoch 424/600 is running ---------------\n",
      "loss:0.46203345214498454, accuracy:0.856004536151886\n",
      "Epoch 425/600 is running ---------------\n",
      "loss:0.4571567778957301, accuracy:0.8611268997192383\n",
      "Epoch 426/600 is running ---------------\n",
      "loss:0.46282241313621914, accuracy:0.856004536151886\n",
      "Epoch 427/600 is running ---------------\n",
      "loss:0.45836853364418295, accuracy:0.8622652292251587\n",
      "Epoch 428/600 is running ---------------\n",
      "loss:0.45969865394049675, accuracy:0.8616960644721985\n",
      "Epoch 429/600 is running ---------------\n",
      "loss:0.4570076943471514, accuracy:0.8599886298179626\n",
      "Epoch 430/600 is running ---------------\n",
      "loss:0.46192886500523006, accuracy:0.8594194650650024\n",
      "Epoch 431/600 is running ---------------\n",
      "loss:0.4580417203492132, accuracy:0.8616960644721985\n",
      "Epoch 432/600 is running ---------------\n",
      "loss:0.4603166965575054, accuracy:0.8594194650650024\n",
      "Epoch 433/600 is running ---------------\n",
      "loss:0.46068618163980285, accuracy:0.8588503003120422\n",
      "Epoch 434/600 is running ---------------\n",
      "loss:0.4587127037089446, accuracy:0.8634034991264343\n",
      "Epoch 435/600 is running ---------------\n",
      "loss:0.4570055665640995, accuracy:0.8668184280395508\n",
      "Epoch 436/600 is running ---------------\n",
      "loss:0.45666666688590213, accuracy:0.8634034991264343\n",
      "Epoch 437/600 is running ---------------\n",
      "loss:0.4590207076278226, accuracy:0.8611268997192383\n",
      "Epoch 438/600 is running ---------------\n",
      "loss:0.45857549741350373, accuracy:0.8628343939781189\n",
      "Epoch 439/600 is running ---------------\n",
      "loss:0.4582091965552034, accuracy:0.8616960644721985\n",
      "Epoch 440/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.4597528951949087, accuracy:0.8616960644721985\n",
      "Epoch 441/600 is running ---------------\n",
      "loss:0.4591089281542548, accuracy:0.8599886298179626\n",
      "Epoch 442/600 is running ---------------\n",
      "loss:0.457993194974702, accuracy:0.8616960644721985\n",
      "Epoch 443/600 is running ---------------\n",
      "loss:0.4557757084739619, accuracy:0.8639726638793945\n",
      "Epoch 444/600 is running ---------------\n",
      "loss:0.45986312473642416, accuracy:0.8611268997192383\n",
      "Epoch 445/600 is running ---------------\n",
      "loss:0.46196034036833666, accuracy:0.8577120304107666\n",
      "Epoch 446/600 is running ---------------\n",
      "loss:0.4593752807584302, accuracy:0.8611268997192383\n",
      "Epoch 447/600 is running ---------------\n",
      "loss:0.4515954004279498, accuracy:0.8713716268539429\n",
      "Epoch 448/600 is running ---------------\n",
      "loss:0.45744538975173027, accuracy:0.8634034991264343\n",
      "Epoch 449/600 is running ---------------\n",
      "loss:0.45460588818994063, accuracy:0.867387592792511\n",
      "Epoch 450/600 is running ---------------\n",
      "loss:0.458476310146266, accuracy:0.8599886298179626\n",
      "Epoch 451/600 is running ---------------\n",
      "loss:0.45764695159320173, accuracy:0.8634034991264343\n",
      "Epoch 452/600 is running ---------------\n",
      "loss:0.4581773080702486, accuracy:0.8616960644721985\n",
      "Epoch 453/600 is running ---------------\n",
      "loss:0.4573921782189402, accuracy:0.8611268997192383\n",
      "Epoch 454/600 is running ---------------\n",
      "loss:0.45635833976597623, accuracy:0.8622652292251587\n",
      "Epoch 455/600 is running ---------------\n",
      "loss:0.4583603010095399, accuracy:0.8605577945709229\n",
      "Epoch 456/600 is running ---------------\n",
      "loss:0.4588291783784998, accuracy:0.8616960644721985\n",
      "Epoch 457/600 is running ---------------\n",
      "loss:0.4557079873208342, accuracy:0.8656801581382751\n",
      "Epoch 458/600 is running ---------------\n",
      "loss:0.4593656402209709, accuracy:0.8616960644721985\n",
      "Epoch 459/600 is running ---------------\n",
      "loss:0.4558397290007821, accuracy:0.8662492632865906\n",
      "Epoch 460/600 is running ---------------\n",
      "loss:0.45664099012983256, accuracy:0.8628343939781189\n",
      "Epoch 461/600 is running ---------------\n",
      "loss:0.45603300990729495, accuracy:0.8651109933853149\n",
      "Epoch 462/600 is running ---------------\n",
      "loss:0.45676797063186253, accuracy:0.8639726638793945\n",
      "Epoch 463/600 is running ---------------\n",
      "loss:0.46015139489338314, accuracy:0.8577120304107666\n",
      "Epoch 464/600 is running ---------------\n",
      "loss:0.45750111135943183, accuracy:0.8645418286323547\n",
      "Epoch 465/600 is running ---------------\n",
      "loss:0.4566903792578599, accuracy:0.8651109933853149\n",
      "Epoch 466/600 is running ---------------\n",
      "loss:0.45931243999251004, accuracy:0.8588503003120422\n",
      "Epoch 467/600 is running ---------------\n",
      "loss:0.45469854669324283, accuracy:0.867387592792511\n",
      "Epoch 468/600 is running ---------------\n",
      "loss:0.4540382906280715, accuracy:0.8662492632865906\n",
      "Epoch 469/600 is running ---------------\n",
      "loss:0.4596884625739065, accuracy:0.8588503003120422\n",
      "Epoch 470/600 is running ---------------\n",
      "loss:0.45479114045356883, accuracy:0.8639726638793945\n",
      "Epoch 471/600 is running ---------------\n",
      "loss:0.4551204114124693, accuracy:0.8662492632865906\n",
      "Epoch 472/600 is running ---------------\n",
      "loss:0.4591937542989336, accuracy:0.8588503003120422\n",
      "Epoch 473/600 is running ---------------\n",
      "loss:0.45765093431390563, accuracy:0.8616960644721985\n",
      "Epoch 474/600 is running ---------------\n",
      "loss:0.45948472670440016, accuracy:0.8599886298179626\n",
      "Epoch 475/600 is running ---------------\n",
      "loss:0.4560558415692428, accuracy:0.8679567575454712\n",
      "Epoch 476/600 is running ---------------\n",
      "loss:0.45721391716907767, accuracy:0.8634034991264343\n",
      "Epoch 477/600 is running ---------------\n",
      "loss:0.4557102219811801, accuracy:0.8651109933853149\n",
      "Epoch 478/600 is running ---------------\n",
      "loss:0.4596284468626154, accuracy:0.8594194650650024\n",
      "Epoch 479/600 is running ---------------\n",
      "loss:0.45610195449714, accuracy:0.8645418286323547\n",
      "Epoch 480/600 is running ---------------\n",
      "loss:0.45400410890579224, accuracy:0.8662492632865906\n",
      "Epoch 481/600 is running ---------------\n",
      "loss:0.4628858021621046, accuracy:0.8571428656578064\n",
      "Epoch 482/600 is running ---------------\n",
      "loss:0.4556164140331334, accuracy:0.8679567575454712\n",
      "Epoch 483/600 is running ---------------\n",
      "loss:0.45442173296007615, accuracy:0.8668184280395508\n",
      "Epoch 484/600 is running ---------------\n",
      "loss:0.4569756979572362, accuracy:0.8645418286323547\n",
      "Epoch 485/600 is running ---------------\n",
      "loss:0.4584371313966554, accuracy:0.8588503003120422\n",
      "Epoch 486/600 is running ---------------\n",
      "loss:0.46086456703728645, accuracy:0.8548662662506104\n",
      "Epoch 487/600 is running ---------------\n",
      "loss:0.45827715150241194, accuracy:0.8599886298179626\n",
      "Epoch 488/600 is running ---------------\n",
      "loss:0.457081178138996, accuracy:0.8639726638793945\n",
      "Epoch 489/600 is running ---------------\n",
      "loss:0.45461612220468195, accuracy:0.8651109933853149\n",
      "Epoch 490/600 is running ---------------\n",
      "loss:0.45846052776122914, accuracy:0.8622652292251587\n",
      "Epoch 491/600 is running ---------------\n",
      "loss:0.4535367550521061, accuracy:0.8679567575454712\n",
      "Epoch 492/600 is running ---------------\n",
      "loss:0.4532373604075662, accuracy:0.867387592792511\n",
      "Epoch 493/600 is running ---------------\n",
      "loss:0.45673425033174714, accuracy:0.8656801581382751\n",
      "Epoch 494/600 is running ---------------\n",
      "loss:0.4578692363253955, accuracy:0.8611268997192383\n",
      "Epoch 495/600 is running ---------------\n",
      "loss:0.4563916340984147, accuracy:0.8668184280395508\n",
      "Epoch 496/600 is running ---------------\n",
      "loss:0.455070475052143, accuracy:0.8645418286323547\n",
      "Epoch 497/600 is running ---------------\n",
      "loss:0.4537953133213109, accuracy:0.8662492632865906\n",
      "Epoch 498/600 is running ---------------\n",
      "loss:0.4551117456164853, accuracy:0.8651109933853149\n",
      "Epoch 499/600 is running ---------------\n",
      "loss:0.45688695589016226, accuracy:0.8651109933853149\n",
      "Epoch 500/600 is running ---------------\n",
      "loss:0.456091760561384, accuracy:0.8634034991264343\n",
      "Epoch 501/600 is running ---------------\n",
      "loss:0.4566311312132868, accuracy:0.8645418286323547\n",
      "Epoch 502/600 is running ---------------\n",
      "loss:0.45746621797824727, accuracy:0.8645418286323547\n",
      "Epoch 503/600 is running ---------------\n",
      "loss:0.45621872051008816, accuracy:0.8651109933853149\n",
      "Epoch 504/600 is running ---------------\n",
      "loss:0.45647909980395746, accuracy:0.8628343939781189\n",
      "Epoch 505/600 is running ---------------\n",
      "loss:0.45939630064471015, accuracy:0.8634034991264343\n",
      "Epoch 506/600 is running ---------------\n",
      "loss:0.4568726245699258, accuracy:0.8639726638793945\n",
      "Epoch 507/600 is running ---------------\n",
      "loss:0.45991830579165754, accuracy:0.8634034991264343\n",
      "Epoch 508/600 is running ---------------\n",
      "loss:0.4559941199319116, accuracy:0.8639726638793945\n",
      "Epoch 509/600 is running ---------------\n",
      "loss:0.45697492258302097, accuracy:0.8645418286323547\n",
      "Epoch 510/600 is running ---------------\n",
      "loss:0.4563789691390662, accuracy:0.8651109933853149\n",
      "Epoch 511/600 is running ---------------\n",
      "loss:0.453094684872134, accuracy:0.869664192199707\n",
      "Epoch 512/600 is running ---------------\n",
      "loss:0.45832228403666925, accuracy:0.8622652292251587\n",
      "Epoch 513/600 is running ---------------\n",
      "loss:0.45650246893537455, accuracy:0.8616960644721985\n",
      "Epoch 514/600 is running ---------------\n",
      "loss:0.45321277811609467, accuracy:0.8679567575454712\n",
      "Epoch 515/600 is running ---------------\n",
      "loss:0.45350527249533557, accuracy:0.8690950274467468\n",
      "Epoch 516/600 is running ---------------\n",
      "loss:0.4573726314922859, accuracy:0.8634034991264343\n",
      "Epoch 517/600 is running ---------------\n",
      "loss:0.45311037990553626, accuracy:0.8679567575454712\n",
      "Epoch 518/600 is running ---------------\n",
      "loss:0.4542203191025504, accuracy:0.8662492632865906\n",
      "Epoch 519/600 is running ---------------\n",
      "loss:0.4563567977527092, accuracy:0.8628343939781189\n",
      "Epoch 520/600 is running ---------------\n",
      "loss:0.45796819031238556, accuracy:0.8611268997192383\n",
      "Epoch 521/600 is running ---------------\n",
      "loss:0.4574430703089155, accuracy:0.867387592792511\n",
      "Epoch 522/600 is running ---------------\n",
      "loss:0.4594658890674854, accuracy:0.8577120304107666\n",
      "Epoch 523/600 is running ---------------\n",
      "loss:0.45617827362027663, accuracy:0.8639726638793945\n",
      "Epoch 524/600 is running ---------------\n",
      "loss:0.4578963040277876, accuracy:0.8628343939781189\n",
      "Epoch 525/600 is running ---------------\n",
      "loss:0.4559584167496911, accuracy:0.8679567575454712\n",
      "Epoch 526/600 is running ---------------\n",
      "loss:0.4583274547396035, accuracy:0.8639726638793945\n",
      "Epoch 527/600 is running ---------------\n",
      "loss:0.4550009724395028, accuracy:0.867387592792511\n",
      "Epoch 528/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.45813177680147105, accuracy:0.8622652292251587\n",
      "Epoch 529/600 is running ---------------\n",
      "loss:0.45480807331101647, accuracy:0.8662492632865906\n",
      "Epoch 530/600 is running ---------------\n",
      "loss:0.4565629244878374, accuracy:0.8622652292251587\n",
      "Epoch 531/600 is running ---------------\n",
      "loss:0.4534185955236698, accuracy:0.8656801581382751\n",
      "Epoch 532/600 is running ---------------\n",
      "loss:0.45235647672209245, accuracy:0.8685259222984314\n",
      "Epoch 533/600 is running ---------------\n",
      "loss:0.45572393978464193, accuracy:0.8651109933853149\n",
      "Epoch 534/600 is running ---------------\n",
      "loss:0.45351787117020836, accuracy:0.8679567575454712\n",
      "Epoch 535/600 is running ---------------\n",
      "loss:0.4599294277100727, accuracy:0.8622652292251587\n",
      "Epoch 536/600 is running ---------------\n",
      "loss:0.4533102178368075, accuracy:0.8685259222984314\n",
      "Epoch 537/600 is running ---------------\n",
      "loss:0.46027926586825274, accuracy:0.8605577945709229\n",
      "Epoch 538/600 is running ---------------\n",
      "loss:0.46035153804154233, accuracy:0.8605577945709229\n",
      "Epoch 539/600 is running ---------------\n",
      "loss:0.4562669838296956, accuracy:0.8628343939781189\n",
      "Epoch 540/600 is running ---------------\n",
      "loss:0.4513616582442974, accuracy:0.8708025217056274\n",
      "Epoch 541/600 is running ---------------\n",
      "loss:0.4538811969346014, accuracy:0.867387592792511\n",
      "Epoch 542/600 is running ---------------\n",
      "loss:0.4529595647392602, accuracy:0.867387592792511\n",
      "Epoch 543/600 is running ---------------\n",
      "loss:0.45543359220027924, accuracy:0.8656801581382751\n",
      "Epoch 544/600 is running ---------------\n",
      "loss:0.45541692704989994, accuracy:0.8679567575454712\n",
      "Epoch 545/600 is running ---------------\n",
      "loss:0.4538056655176755, accuracy:0.8679567575454712\n",
      "Epoch 546/600 is running ---------------\n",
      "loss:0.4592513118324609, accuracy:0.858281135559082\n",
      "Epoch 547/600 is running ---------------\n",
      "loss:0.4547390871006867, accuracy:0.8662492632865906\n",
      "Epoch 548/600 is running ---------------\n",
      "loss:0.4548281616178052, accuracy:0.8634034991264343\n",
      "Epoch 549/600 is running ---------------\n",
      "loss:0.45587441839020826, accuracy:0.8656801581382751\n",
      "Epoch 550/600 is running ---------------\n",
      "loss:0.45506485885587233, accuracy:0.8639726638793945\n",
      "Epoch 551/600 is running ---------------\n",
      "loss:0.4549820474509535, accuracy:0.867387592792511\n",
      "Epoch 552/600 is running ---------------\n",
      "loss:0.45614165273205987, accuracy:0.8634034991264343\n",
      "Epoch 553/600 is running ---------------\n",
      "loss:0.4569261310429409, accuracy:0.8639726638793945\n",
      "Epoch 554/600 is running ---------------\n",
      "loss:0.45955514188470514, accuracy:0.8616960644721985\n",
      "Epoch 555/600 is running ---------------\n",
      "loss:0.45122074875338325, accuracy:0.8702333569526672\n",
      "Epoch 556/600 is running ---------------\n",
      "loss:0.4559136510922991, accuracy:0.8628343939781189\n",
      "Epoch 557/600 is running ---------------\n",
      "loss:0.45546658234349613, accuracy:0.867387592792511\n",
      "Epoch 558/600 is running ---------------\n",
      "loss:0.4545492293505833, accuracy:0.8668184280395508\n",
      "Epoch 559/600 is running ---------------\n",
      "loss:0.4571113221604249, accuracy:0.8639726638793945\n",
      "Epoch 560/600 is running ---------------\n",
      "loss:0.45761747658252716, accuracy:0.8611268997192383\n",
      "Epoch 561/600 is running ---------------\n",
      "loss:0.457313248823429, accuracy:0.8656801581382751\n",
      "Epoch 562/600 is running ---------------\n",
      "loss:0.4567921747421396, accuracy:0.8634034991264343\n",
      "Epoch 563/600 is running ---------------\n",
      "loss:0.4559521325703325, accuracy:0.8668184280395508\n",
      "Epoch 564/600 is running ---------------\n",
      "loss:0.45637519143778704, accuracy:0.8656801581382751\n",
      "Epoch 565/600 is running ---------------\n",
      "loss:0.4592110612269106, accuracy:0.8634034991264343\n",
      "Epoch 566/600 is running ---------------\n",
      "loss:0.4569619642249469, accuracy:0.8628343939781189\n",
      "Epoch 567/600 is running ---------------\n",
      "loss:0.45856662762576134, accuracy:0.8628343939781189\n",
      "Epoch 568/600 is running ---------------\n",
      "loss:0.45770368082769985, accuracy:0.8616960644721985\n",
      "Epoch 569/600 is running ---------------\n",
      "loss:0.455285320508069, accuracy:0.8656801581382751\n",
      "Epoch 570/600 is running ---------------\n",
      "loss:0.4590274886838321, accuracy:0.8599886298179626\n",
      "Epoch 571/600 is running ---------------\n",
      "loss:0.4554284656869954, accuracy:0.8662492632865906\n",
      "Epoch 572/600 is running ---------------\n",
      "loss:0.4534710712473968, accuracy:0.8656801581382751\n",
      "Epoch 573/600 is running ---------------\n",
      "loss:0.4548196129757783, accuracy:0.8702333569526672\n",
      "Epoch 574/600 is running ---------------\n",
      "loss:0.4533782421514906, accuracy:0.8656801581382751\n",
      "Epoch 575/600 is running ---------------\n",
      "loss:0.45701360034531563, accuracy:0.8616960644721985\n",
      "Epoch 576/600 is running ---------------\n",
      "loss:0.453695496608471, accuracy:0.8679567575454712\n",
      "Epoch 577/600 is running ---------------\n",
      "loss:0.4561653306771969, accuracy:0.8645418286323547\n",
      "Epoch 578/600 is running ---------------\n",
      "loss:0.45714961506169416, accuracy:0.8628343939781189\n",
      "Epoch 579/600 is running ---------------\n",
      "loss:0.4573172202398037, accuracy:0.8616960644721985\n",
      "Epoch 580/600 is running ---------------\n",
      "loss:0.45790320223775405, accuracy:0.8628343939781189\n",
      "Epoch 581/600 is running ---------------\n",
      "loss:0.45446764395154754, accuracy:0.8690950274467468\n",
      "Epoch 582/600 is running ---------------\n",
      "loss:0.45703478876886694, accuracy:0.8628343939781189\n",
      "Epoch 583/600 is running ---------------\n",
      "loss:0.45505578610403785, accuracy:0.867387592792511\n",
      "Epoch 584/600 is running ---------------\n",
      "loss:0.4588348053652665, accuracy:0.8639726638793945\n",
      "Epoch 585/600 is running ---------------\n",
      "loss:0.45494123625344246, accuracy:0.867387592792511\n",
      "Epoch 586/600 is running ---------------\n",
      "loss:0.4546463756725706, accuracy:0.8662492632865906\n",
      "Epoch 587/600 is running ---------------\n",
      "loss:0.45613064128777076, accuracy:0.8656801581382751\n",
      "Epoch 588/600 is running ---------------\n",
      "loss:0.4557713835403837, accuracy:0.8651109933853149\n",
      "Epoch 589/600 is running ---------------\n",
      "loss:0.4564369171857834, accuracy:0.8639726638793945\n",
      "Epoch 590/600 is running ---------------\n",
      "loss:0.45569508240140716, accuracy:0.8628343939781189\n",
      "Epoch 591/600 is running ---------------\n",
      "loss:0.4537026697191699, accuracy:0.8679567575454712\n",
      "Epoch 592/600 is running ---------------\n",
      "loss:0.45414540120239916, accuracy:0.8668184280395508\n",
      "Epoch 593/600 is running ---------------\n",
      "loss:0.4550422815413311, accuracy:0.8656801581382751\n",
      "Epoch 594/600 is running ---------------\n",
      "loss:0.45208975724105177, accuracy:0.8685259222984314\n",
      "Epoch 595/600 is running ---------------\n",
      "loss:0.453162279108475, accuracy:0.8702333569526672\n",
      "Epoch 596/600 is running ---------------\n",
      "loss:0.45566359367863885, accuracy:0.8651109933853149\n",
      "Epoch 597/600 is running ---------------\n",
      "loss:0.4536661973287319, accuracy:0.8656801581382751\n",
      "Epoch 598/600 is running ---------------\n",
      "loss:0.45596185480726176, accuracy:0.8622652292251587\n",
      "Epoch 599/600 is running ---------------\n",
      "loss:0.45597664806349525, accuracy:0.8651109933853149\n",
      "Epoch 600/600 is running ---------------\n",
      "loss:0.4603509147619379, accuracy:0.8599886298179626\n"
     ]
    }
   ],
   "source": [
    "mlp_model.train(True)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f'Epoch {epoch+1}/{epoch_num} is running ---------------')\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model.forward(data)\n",
    "        \n",
    "        # view([r, c])는 크기 -1이 아닌 [r, c]만큼의 행렬로 만들어준다.\n",
    "        label = label.view([1,-1]).squeeze()\n",
    "        label = label.long()\n",
    "        \n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "    \n",
    "    # print(running_loss)\n",
    "    epoch_loss = running_loss / i\n",
    "    epoch_corrects = running_corrects / train_x.shape[0]\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'loss:{epoch_loss}, accuracy:{epoch_corrects}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "23dec18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TensorData(valid_x, valid_y)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "d82a9e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.7602040767669678\n"
     ]
    }
   ],
   "source": [
    "mlp_model.train(False)\n",
    "#mlp_model.dropout = nn.Sequential()\n",
    "\n",
    "for i, (data, label) in enumerate(valid_dataloader):\n",
    "    \n",
    "    output = mlp_model.forward(data)\n",
    "    label = label.view([1,-1]).squeeze()\n",
    "    label = label.long()\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    \n",
    "accuracy = torch.sum(preds == label) / len(valid_dataset)\n",
    "print(f'accuracy:{accuracy}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "9ee5b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_dataset = TensorData(x, y)\n",
    "total_train_dataloader = DataLoader(total_train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "bd03d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = travelNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "lr3 = 0.0001\n",
    "best_lr = 7.9094022518724e-05\n",
    "\n",
    "params_ft = []\n",
    "params_ft.append({'params': mlp_model.l1.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l2.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l3.parameters(), 'lr': best_lr})\n",
    "params_ft.append({'params': mlp_model.l4.parameters(), 'lr': best_lr})\n",
    "\n",
    "optimizer = optim.Adam(params_ft)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
    "epoch_num = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5efea",
   "metadata": {},
   "source": [
    "'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.05, 'optimizer': {'lr': 7.9094022518724e-05, 'type': 'adam'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "c1d3ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600 is running ---------------\n",
      "loss:0.012158763971800959, accuracy:0.5788275599479675\n",
      "Epoch 2/600 is running ---------------\n",
      "loss:0.011748273771597569, accuracy:0.6528173089027405\n",
      "Epoch 3/600 is running ---------------\n",
      "loss:0.011505400347451832, accuracy:0.689812183380127\n",
      "Epoch 4/600 is running ---------------\n",
      "loss:0.011372335267867564, accuracy:0.7199772596359253\n",
      "Epoch 5/600 is running ---------------\n",
      "loss:0.011257108430259935, accuracy:0.7478656768798828\n",
      "Epoch 6/600 is running ---------------\n",
      "loss:0.01118114907835177, accuracy:0.7507114410400391\n",
      "Epoch 7/600 is running ---------------\n",
      "loss:0.011063705979251156, accuracy:0.7689242959022522\n",
      "Epoch 8/600 is running ---------------\n",
      "loss:0.011088790205573927, accuracy:0.7666476964950562\n",
      "Epoch 9/600 is running ---------------\n",
      "loss:0.011055985634206166, accuracy:0.7689242959022522\n",
      "Epoch 10/600 is running ---------------\n",
      "loss:0.010953250023609003, accuracy:0.7848605513572693\n",
      "Epoch 11/600 is running ---------------\n",
      "loss:0.010904431105609909, accuracy:0.7973818778991699\n",
      "Epoch 12/600 is running ---------------\n",
      "loss:0.010907095233079176, accuracy:0.7859988808631897\n",
      "Epoch 13/600 is running ---------------\n",
      "loss:0.01087798631930799, accuracy:0.7831531167030334\n",
      "Epoch 14/600 is running ---------------\n",
      "loss:0.010822878415841031, accuracy:0.7956744432449341\n",
      "Epoch 15/600 is running ---------------\n",
      "loss:0.010886929628180046, accuracy:0.7859988808631897\n",
      "Epoch 16/600 is running ---------------\n",
      "loss:0.010818914215064009, accuracy:0.7905520796775818\n",
      "Epoch 17/600 is running ---------------\n",
      "loss:0.010807201105149279, accuracy:0.7899829149246216\n",
      "Epoch 18/600 is running ---------------\n",
      "loss:0.010808066983893302, accuracy:0.8036425709724426\n",
      "Epoch 19/600 is running ---------------\n",
      "loss:0.010766533130804246, accuracy:0.8059191703796387\n",
      "Epoch 20/600 is running ---------------\n",
      "loss:0.010737190547828044, accuracy:0.8093340992927551\n",
      "Epoch 21/600 is running ---------------\n",
      "loss:0.010715322224470995, accuracy:0.8104723691940308\n",
      "Epoch 22/600 is running ---------------\n",
      "loss:0.0106816234138111, accuracy:0.8093340992927551\n",
      "Epoch 23/600 is running ---------------\n",
      "loss:0.01073684279187267, accuracy:0.7985202074050903\n",
      "Epoch 24/600 is running ---------------\n",
      "loss:0.010662911456623196, accuracy:0.8167330622673035\n",
      "Epoch 25/600 is running ---------------\n",
      "loss:0.01066848857876382, accuracy:0.8161638975143433\n",
      "Epoch 26/600 is running ---------------\n",
      "loss:0.010670491321967554, accuracy:0.8144564628601074\n",
      "Epoch 27/600 is running ---------------\n",
      "loss:0.010679243197139855, accuracy:0.8087649345397949\n",
      "Epoch 28/600 is running ---------------\n",
      "loss:0.010635041145689458, accuracy:0.8178713917732239\n",
      "Epoch 29/600 is running ---------------\n",
      "loss:0.01063124958466045, accuracy:0.8207171559333801\n",
      "Epoch 30/600 is running ---------------\n",
      "loss:0.010592143475907596, accuracy:0.8252703547477722\n",
      "Epoch 31/600 is running ---------------\n",
      "loss:0.010604256958057749, accuracy:0.8269777894020081\n",
      "Epoch 32/600 is running ---------------\n",
      "loss:0.01058783928779424, accuracy:0.8264086246490479\n",
      "Epoch 33/600 is running ---------------\n",
      "loss:0.010644419531151864, accuracy:0.8138872981071472\n",
      "Epoch 34/600 is running ---------------\n",
      "loss:0.01057232769633398, accuracy:0.833807647228241\n",
      "Epoch 35/600 is running ---------------\n",
      "loss:0.010582561545841519, accuracy:0.8292543888092041\n",
      "Epoch 36/600 is running ---------------\n",
      "loss:0.010574671376748864, accuracy:0.8309618830680847\n",
      "Epoch 37/600 is running ---------------\n",
      "loss:0.010559797558380109, accuracy:0.8252703547477722\n",
      "Epoch 38/600 is running ---------------\n",
      "loss:0.010503206900323598, accuracy:0.8383608460426331\n",
      "Epoch 39/600 is running ---------------\n",
      "loss:0.010520485191095533, accuracy:0.8326693177223206\n",
      "Epoch 40/600 is running ---------------\n",
      "loss:0.01052447347798399, accuracy:0.8332384824752808\n",
      "Epoch 41/600 is running ---------------\n",
      "loss:0.010450574777719306, accuracy:0.8497439026832581\n",
      "Epoch 42/600 is running ---------------\n",
      "loss:0.010485076164751481, accuracy:0.8372225165367126\n",
      "Epoch 43/600 is running ---------------\n",
      "loss:0.01051450454177813, accuracy:0.8520205020904541\n",
      "Epoch 44/600 is running ---------------\n",
      "loss:0.010526727971938366, accuracy:0.8326693177223206\n",
      "Epoch 45/600 is running ---------------\n",
      "loss:0.010501372013569692, accuracy:0.8417757749557495\n",
      "Epoch 46/600 is running ---------------\n",
      "loss:0.010442327794936413, accuracy:0.8412066102027893\n",
      "Epoch 47/600 is running ---------------\n",
      "loss:0.010426192678506902, accuracy:0.8611268997192383\n",
      "Epoch 48/600 is running ---------------\n",
      "loss:0.010436727638331474, accuracy:0.8537279367446899\n",
      "Epoch 49/600 is running ---------------\n",
      "loss:0.010442626462718202, accuracy:0.8565737009048462\n",
      "Epoch 50/600 is running ---------------\n",
      "loss:0.010447913058861183, accuracy:0.8406374454498291\n",
      "Epoch 51/600 is running ---------------\n",
      "loss:0.010449487781687495, accuracy:0.8383608460426331\n",
      "Epoch 52/600 is running ---------------\n",
      "loss:0.010472312900378205, accuracy:0.8554354310035706\n",
      "Epoch 53/600 is running ---------------\n",
      "loss:0.010475696727641957, accuracy:0.8372225165367126\n",
      "Epoch 54/600 is running ---------------\n",
      "loss:0.010390645513225157, accuracy:0.8634034991264343\n",
      "Epoch 55/600 is running ---------------\n",
      "loss:0.010397592626244899, accuracy:0.8571428656578064\n",
      "Epoch 56/600 is running ---------------\n",
      "loss:0.01035379952982738, accuracy:0.8531587719917297\n",
      "Epoch 57/600 is running ---------------\n",
      "loss:0.010431349684585546, accuracy:0.8412066102027893\n",
      "Epoch 58/600 is running ---------------\n",
      "loss:0.01041501200735128, accuracy:0.8634034991264343\n",
      "Epoch 59/600 is running ---------------\n",
      "loss:0.010377318260679207, accuracy:0.8468981385231018\n",
      "Epoch 60/600 is running ---------------\n",
      "loss:0.010367183605241857, accuracy:0.8486055731773376\n",
      "Epoch 61/600 is running ---------------\n",
      "loss:0.010404374020033148, accuracy:0.8486055731773376\n",
      "Epoch 62/600 is running ---------------\n",
      "loss:0.010330854353744602, accuracy:0.8656801581382751\n",
      "Epoch 63/600 is running ---------------\n",
      "loss:0.010355071988917238, accuracy:0.8412066102027893\n",
      "Epoch 64/600 is running ---------------\n",
      "loss:0.010343877341032164, accuracy:0.8565737009048462\n",
      "Epoch 65/600 is running ---------------\n",
      "loss:0.010352489855999151, accuracy:0.847467303276062\n",
      "Epoch 66/600 is running ---------------\n",
      "loss:0.010288589436287356, accuracy:0.8651109933853149\n",
      "Epoch 67/600 is running ---------------\n",
      "loss:0.01039737262845243, accuracy:0.8446215391159058\n",
      "Epoch 68/600 is running ---------------\n",
      "loss:0.010336802944786927, accuracy:0.8594194650650024\n",
      "Epoch 69/600 is running ---------------\n",
      "loss:0.010369232485222424, accuracy:0.8605577945709229\n",
      "Epoch 70/600 is running ---------------\n",
      "loss:0.010315349513180362, accuracy:0.856004536151886\n",
      "Epoch 71/600 is running ---------------\n",
      "loss:0.010352662427898965, accuracy:0.8514513373374939\n",
      "Epoch 72/600 is running ---------------\n",
      "loss:0.010306336320661047, accuracy:0.8548662662506104\n",
      "Epoch 73/600 is running ---------------\n",
      "loss:0.010268669577573062, accuracy:0.8520205020904541\n",
      "Epoch 74/600 is running ---------------\n",
      "loss:0.010359352294463765, accuracy:0.8520205020904541\n",
      "Epoch 75/600 is running ---------------\n",
      "loss:0.010335948634161214, accuracy:0.8616960644721985\n",
      "Epoch 76/600 is running ---------------\n",
      "loss:0.010343107942430282, accuracy:0.8565737009048462\n",
      "Epoch 77/600 is running ---------------\n",
      "loss:0.010304596896326264, accuracy:0.8542971014976501\n",
      "Epoch 78/600 is running ---------------\n",
      "loss:0.010204077072284679, accuracy:0.8690950274467468\n",
      "Epoch 79/600 is running ---------------\n",
      "loss:0.010243945862667494, accuracy:0.869664192199707\n",
      "Epoch 80/600 is running ---------------\n",
      "loss:0.010253150285341961, accuracy:0.8639726638793945\n",
      "Epoch 81/600 is running ---------------\n",
      "loss:0.01032687851700107, accuracy:0.858281135559082\n",
      "Epoch 82/600 is running ---------------\n",
      "loss:0.010241707821901916, accuracy:0.8656801581382751\n",
      "Epoch 83/600 is running ---------------\n",
      "loss:0.010268810803603909, accuracy:0.8605577945709229\n",
      "Epoch 84/600 is running ---------------\n",
      "loss:0.010244163723241625, accuracy:0.8719407916069031\n",
      "Epoch 85/600 is running ---------------\n",
      "loss:0.010208661269924488, accuracy:0.8708025217056274\n",
      "Epoch 86/600 is running ---------------\n",
      "loss:0.01021258170861172, accuracy:0.8725099563598633\n",
      "Epoch 87/600 is running ---------------\n",
      "loss:0.0102465623606995, accuracy:0.8599886298179626\n",
      "Epoch 88/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.010211239739039708, accuracy:0.8776323199272156\n",
      "Epoch 89/600 is running ---------------\n",
      "loss:0.010214476539251408, accuracy:0.8793397545814514\n",
      "Epoch 90/600 is running ---------------\n",
      "loss:0.010142416321695292, accuracy:0.8759248852729797\n",
      "Epoch 91/600 is running ---------------\n",
      "loss:0.010291878852779103, accuracy:0.8491747379302979\n",
      "Epoch 92/600 is running ---------------\n",
      "loss:0.0102046657232785, accuracy:0.867387592792511\n",
      "Epoch 93/600 is running ---------------\n",
      "loss:0.010171341332917059, accuracy:0.878770649433136\n",
      "Epoch 94/600 is running ---------------\n",
      "loss:0.010234895998195416, accuracy:0.8679567575454712\n",
      "Epoch 95/600 is running ---------------\n",
      "loss:0.01014027238638481, accuracy:0.8764940500259399\n",
      "Epoch 96/600 is running ---------------\n",
      "loss:0.010166308319426153, accuracy:0.8759248852729797\n",
      "Epoch 97/600 is running ---------------\n",
      "loss:0.010171695161286442, accuracy:0.8713716268539429\n",
      "Epoch 98/600 is running ---------------\n",
      "loss:0.010188412008193249, accuracy:0.8719407916069031\n",
      "Epoch 99/600 is running ---------------\n",
      "loss:0.01013102888314644, accuracy:0.8861696124076843\n",
      "Epoch 100/600 is running ---------------\n",
      "loss:0.010192888802130519, accuracy:0.8764940500259399\n",
      "Epoch 101/600 is running ---------------\n",
      "loss:0.01017339597999473, accuracy:0.8725099563598633\n",
      "Epoch 102/600 is running ---------------\n",
      "loss:0.01016292666330484, accuracy:0.8770631551742554\n",
      "Epoch 103/600 is running ---------------\n",
      "loss:0.01014485604123901, accuracy:0.8719407916069031\n",
      "Epoch 104/600 is running ---------------\n",
      "loss:0.01013961887251062, accuracy:0.8690950274467468\n",
      "Epoch 105/600 is running ---------------\n",
      "loss:0.010103818972679316, accuracy:0.88730788230896\n",
      "Epoch 106/600 is running ---------------\n",
      "loss:0.010099827021987993, accuracy:0.8935685753822327\n",
      "Epoch 107/600 is running ---------------\n",
      "loss:0.010147725952302589, accuracy:0.8713716268539429\n",
      "Epoch 108/600 is running ---------------\n",
      "loss:0.01011419214846807, accuracy:0.8884462118148804\n",
      "Epoch 109/600 is running ---------------\n",
      "loss:0.010216503911352185, accuracy:0.869664192199707\n",
      "Epoch 110/600 is running ---------------\n",
      "loss:0.010129834686956694, accuracy:0.8907228112220764\n",
      "Epoch 111/600 is running ---------------\n",
      "loss:0.010133477015867108, accuracy:0.8776323199272156\n",
      "Epoch 112/600 is running ---------------\n",
      "loss:0.010077742703882347, accuracy:0.8850312829017639\n",
      "Epoch 113/600 is running ---------------\n",
      "loss:0.010081763049045882, accuracy:0.8793397545814514\n",
      "Epoch 114/600 is running ---------------\n",
      "loss:0.010146145326682772, accuracy:0.8759248852729797\n",
      "Epoch 115/600 is running ---------------\n",
      "loss:0.010118351311051309, accuracy:0.8844621777534485\n",
      "Epoch 116/600 is running ---------------\n",
      "loss:0.0100778398964306, accuracy:0.8884462118148804\n",
      "Epoch 117/600 is running ---------------\n",
      "loss:0.01014863884985006, accuracy:0.8833238482475281\n",
      "Epoch 118/600 is running ---------------\n",
      "loss:0.010111825874602177, accuracy:0.8782014846801758\n",
      "Epoch 119/600 is running ---------------\n",
      "loss:0.010112687852074628, accuracy:0.8844621777534485\n",
      "Epoch 120/600 is running ---------------\n",
      "loss:0.010139737165849455, accuracy:0.8634034991264343\n",
      "Epoch 121/600 is running ---------------\n",
      "loss:0.010102861193546735, accuracy:0.8816164135932922\n",
      "Epoch 122/600 is running ---------------\n",
      "loss:0.010224464846118716, accuracy:0.8719407916069031\n",
      "Epoch 123/600 is running ---------------\n",
      "loss:0.010133357501260652, accuracy:0.8799089193344116\n",
      "Epoch 124/600 is running ---------------\n",
      "loss:0.010121626513344901, accuracy:0.8838930130004883\n",
      "Epoch 125/600 is running ---------------\n",
      "loss:0.010124177062925318, accuracy:0.878770649433136\n",
      "Epoch 126/600 is running ---------------\n",
      "loss:0.010035073886083214, accuracy:0.8929994106292725\n",
      "Epoch 127/600 is running ---------------\n",
      "loss:0.010079910284290955, accuracy:0.8747865557670593\n",
      "Epoch 128/600 is running ---------------\n",
      "loss:0.01005797743729454, accuracy:0.8764940500259399\n",
      "Epoch 129/600 is running ---------------\n",
      "loss:0.010076043309986286, accuracy:0.8856004476547241\n",
      "Epoch 130/600 is running ---------------\n",
      "loss:0.010047165758581276, accuracy:0.8833238482475281\n",
      "Epoch 131/600 is running ---------------\n",
      "loss:0.01008672634851003, accuracy:0.8907228112220764\n",
      "Epoch 132/600 is running ---------------\n",
      "loss:0.010125207168240403, accuracy:0.8901536464691162\n",
      "Epoch 133/600 is running ---------------\n",
      "loss:0.010063994792488805, accuracy:0.878770649433136\n",
      "Epoch 134/600 is running ---------------\n",
      "loss:0.010081227217876715, accuracy:0.8895845413208008\n",
      "Epoch 135/600 is running ---------------\n",
      "loss:0.01014940434718037, accuracy:0.8747865557670593\n",
      "Epoch 136/600 is running ---------------\n",
      "loss:0.010081705208454339, accuracy:0.8861696124076843\n",
      "Epoch 137/600 is running ---------------\n",
      "loss:0.010075353056311133, accuracy:0.8947069048881531\n",
      "Epoch 138/600 is running ---------------\n",
      "loss:0.010063710406753376, accuracy:0.8958451747894287\n",
      "Epoch 139/600 is running ---------------\n",
      "loss:0.010013709573088957, accuracy:0.8884462118148804\n",
      "Epoch 140/600 is running ---------------\n",
      "loss:0.01003531980188855, accuracy:0.8947069048881531\n",
      "Epoch 141/600 is running ---------------\n",
      "loss:0.009988410369740608, accuracy:0.8958451747894287\n",
      "Epoch 142/600 is running ---------------\n",
      "loss:0.010068478303397998, accuracy:0.8838930130004883\n",
      "Epoch 143/600 is running ---------------\n",
      "loss:0.010074833915799466, accuracy:0.8793397545814514\n",
      "Epoch 144/600 is running ---------------\n",
      "loss:0.010017378498494251, accuracy:0.8907228112220764\n",
      "Epoch 145/600 is running ---------------\n",
      "loss:0.010048832619264848, accuracy:0.8804780840873718\n",
      "Epoch 146/600 is running ---------------\n",
      "loss:0.010010443292833826, accuracy:0.8838930130004883\n",
      "Epoch 147/600 is running ---------------\n",
      "loss:0.010067444703900549, accuracy:0.8901536464691162\n",
      "Epoch 148/600 is running ---------------\n",
      "loss:0.010041244544288145, accuracy:0.8895845413208008\n",
      "Epoch 149/600 is running ---------------\n",
      "loss:0.010059136488116053, accuracy:0.878770649433136\n",
      "Epoch 150/600 is running ---------------\n",
      "loss:0.009990779832471958, accuracy:0.8981217741966248\n",
      "Epoch 151/600 is running ---------------\n",
      "loss:0.010052397804770472, accuracy:0.8907228112220764\n",
      "Epoch 152/600 is running ---------------\n",
      "loss:0.010035875851821682, accuracy:0.8804780840873718\n",
      "Epoch 153/600 is running ---------------\n",
      "loss:0.009936453709903603, accuracy:0.8947069048881531\n",
      "Epoch 154/600 is running ---------------\n",
      "loss:0.010039593492235457, accuracy:0.8884462118148804\n",
      "Epoch 155/600 is running ---------------\n",
      "loss:0.010059244095363193, accuracy:0.8833238482475281\n",
      "Epoch 156/600 is running ---------------\n",
      "loss:0.010050323753107252, accuracy:0.8907228112220764\n",
      "Epoch 157/600 is running ---------------\n",
      "loss:0.010041331084668806, accuracy:0.8907228112220764\n",
      "Epoch 158/600 is running ---------------\n",
      "loss:0.009977550484880372, accuracy:0.9038133025169373\n",
      "Epoch 159/600 is running ---------------\n",
      "loss:0.009985084688385305, accuracy:0.8884462118148804\n",
      "Epoch 160/600 is running ---------------\n",
      "loss:0.00998919869799063, accuracy:0.8890153765678406\n",
      "Epoch 161/600 is running ---------------\n",
      "loss:0.010019875414071601, accuracy:0.8958451747894287\n",
      "Epoch 162/600 is running ---------------\n",
      "loss:0.01003702740541696, accuracy:0.8890153765678406\n",
      "Epoch 163/600 is running ---------------\n",
      "loss:0.010080559455880267, accuracy:0.8901536464691162\n",
      "Epoch 164/600 is running ---------------\n",
      "loss:0.009992088386804866, accuracy:0.9026750326156616\n",
      "Epoch 165/600 is running ---------------\n",
      "loss:0.0099436312693388, accuracy:0.9060899019241333\n",
      "Epoch 166/600 is running ---------------\n",
      "loss:0.009983442320902373, accuracy:0.8975526690483093\n",
      "Epoch 167/600 is running ---------------\n",
      "loss:0.009994877524584889, accuracy:0.9021058678627014\n",
      "Epoch 168/600 is running ---------------\n",
      "loss:0.009964660208674406, accuracy:0.900967538356781\n",
      "Epoch 169/600 is running ---------------\n",
      "loss:0.009947655549698006, accuracy:0.9112122654914856\n",
      "Epoch 170/600 is running ---------------\n",
      "loss:0.009959362621122826, accuracy:0.9049516320228577\n",
      "Epoch 171/600 is running ---------------\n",
      "loss:0.010025632333945468, accuracy:0.8844621777534485\n",
      "Epoch 172/600 is running ---------------\n",
      "loss:0.009963596925588921, accuracy:0.8952760100364685\n",
      "Epoch 173/600 is running ---------------\n",
      "loss:0.009910735408216291, accuracy:0.9060899019241333\n",
      "Epoch 174/600 is running ---------------\n",
      "loss:0.009982009807905827, accuracy:0.8935685753822327\n",
      "Epoch 175/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.010053651910921899, accuracy:0.8850312829017639\n",
      "Epoch 176/600 is running ---------------\n",
      "loss:0.010043571059108119, accuracy:0.8952760100364685\n",
      "Epoch 177/600 is running ---------------\n",
      "loss:0.009997281657747064, accuracy:0.8890153765678406\n",
      "Epoch 178/600 is running ---------------\n",
      "loss:0.009986704123176223, accuracy:0.88730788230896\n",
      "Epoch 179/600 is running ---------------\n",
      "loss:0.0099986914415962, accuracy:0.8856004476547241\n",
      "Epoch 180/600 is running ---------------\n",
      "loss:0.00999132869049576, accuracy:0.8901536464691162\n",
      "Epoch 181/600 is running ---------------\n",
      "loss:0.009962365616433107, accuracy:0.9003984332084656\n",
      "Epoch 182/600 is running ---------------\n",
      "loss:0.01005454092590254, accuracy:0.8850312829017639\n",
      "Epoch 183/600 is running ---------------\n",
      "loss:0.010029633376295755, accuracy:0.8844621777534485\n",
      "Epoch 184/600 is running ---------------\n",
      "loss:0.009976862402347658, accuracy:0.900967538356781\n",
      "Epoch 185/600 is running ---------------\n",
      "loss:0.010017899097742245, accuracy:0.8918611407279968\n",
      "Epoch 186/600 is running ---------------\n",
      "loss:0.009976806156188843, accuracy:0.8901536464691162\n",
      "Epoch 187/600 is running ---------------\n",
      "loss:0.009938841322028779, accuracy:0.8901536464691162\n",
      "Epoch 188/600 is running ---------------\n",
      "loss:0.009970538780373068, accuracy:0.8947069048881531\n",
      "Epoch 189/600 is running ---------------\n",
      "loss:0.009912003321476667, accuracy:0.9077973961830139\n",
      "Epoch 190/600 is running ---------------\n",
      "loss:0.009909208043434434, accuracy:0.9049516320228577\n",
      "Epoch 191/600 is running ---------------\n",
      "loss:0.009912778181858747, accuracy:0.9129197597503662\n",
      "Epoch 192/600 is running ---------------\n",
      "loss:0.009932706623370497, accuracy:0.8992601037025452\n",
      "Epoch 193/600 is running ---------------\n",
      "loss:0.009957807743895373, accuracy:0.8947069048881531\n",
      "Epoch 194/600 is running ---------------\n",
      "loss:0.009935530804746451, accuracy:0.8992601037025452\n",
      "Epoch 195/600 is running ---------------\n",
      "loss:0.009897831121356771, accuracy:0.9066590666770935\n",
      "Epoch 196/600 is running ---------------\n",
      "loss:0.009990380308339049, accuracy:0.8918611407279968\n",
      "Epoch 197/600 is running ---------------\n",
      "loss:0.009929729037211576, accuracy:0.8941377401351929\n",
      "Epoch 198/600 is running ---------------\n",
      "loss:0.010014326720327291, accuracy:0.8884462118148804\n",
      "Epoch 199/600 is running ---------------\n",
      "loss:0.00991500590969774, accuracy:0.8998292684555054\n",
      "Epoch 200/600 is running ---------------\n",
      "loss:0.009908976477523656, accuracy:0.9060899019241333\n",
      "Epoch 201/600 is running ---------------\n",
      "loss:0.009962118343663744, accuracy:0.8941377401351929\n",
      "Epoch 202/600 is running ---------------\n",
      "loss:0.009957168037130131, accuracy:0.8941377401351929\n",
      "Epoch 203/600 is running ---------------\n",
      "loss:0.009912949261098133, accuracy:0.9021058678627014\n",
      "Epoch 204/600 is running ---------------\n",
      "loss:0.009980422159997183, accuracy:0.8964143395423889\n",
      "Epoch 205/600 is running ---------------\n",
      "loss:0.00998452409262269, accuracy:0.8947069048881531\n",
      "Epoch 206/600 is running ---------------\n",
      "loss:0.009941801030186678, accuracy:0.8935685753822327\n",
      "Epoch 207/600 is running ---------------\n",
      "loss:0.009968971826165796, accuracy:0.8861696124076843\n",
      "Epoch 208/600 is running ---------------\n",
      "loss:0.009926042844439069, accuracy:0.8981217741966248\n",
      "Epoch 209/600 is running ---------------\n",
      "loss:0.009976386006202763, accuracy:0.8969835042953491\n",
      "Epoch 210/600 is running ---------------\n",
      "loss:0.00995461233108642, accuracy:0.9021058678627014\n",
      "Epoch 211/600 is running ---------------\n",
      "loss:0.009939220288156718, accuracy:0.9038133025169373\n",
      "Epoch 212/600 is running ---------------\n",
      "loss:0.009917425376553391, accuracy:0.9060899019241333\n",
      "Epoch 213/600 is running ---------------\n",
      "loss:0.009977291949309608, accuracy:0.8964143395423889\n",
      "Epoch 214/600 is running ---------------\n",
      "loss:0.009903275328871195, accuracy:0.9077973961830139\n",
      "Epoch 215/600 is running ---------------\n",
      "loss:0.009944234914785091, accuracy:0.9066590666770935\n",
      "Epoch 216/600 is running ---------------\n",
      "loss:0.009962101890474948, accuracy:0.8964143395423889\n",
      "Epoch 217/600 is running ---------------\n",
      "loss:0.009913752414180124, accuracy:0.9112122654914856\n",
      "Epoch 218/600 is running ---------------\n",
      "loss:0.009905611919149052, accuracy:0.9049516320228577\n",
      "Epoch 219/600 is running ---------------\n",
      "loss:0.009893612896914365, accuracy:0.8992601037025452\n",
      "Epoch 220/600 is running ---------------\n",
      "loss:0.009909893649507803, accuracy:0.9066590666770935\n",
      "Epoch 221/600 is running ---------------\n",
      "loss:0.009928669248308455, accuracy:0.8918611407279968\n",
      "Epoch 222/600 is running ---------------\n",
      "loss:0.009885449876280488, accuracy:0.9077973961830139\n",
      "Epoch 223/600 is running ---------------\n",
      "loss:0.009959641511153958, accuracy:0.9015367031097412\n",
      "Epoch 224/600 is running ---------------\n",
      "loss:0.009936491162110678, accuracy:0.9077973961830139\n",
      "Epoch 225/600 is running ---------------\n",
      "loss:0.009885253116496937, accuracy:0.9021058678627014\n",
      "Epoch 226/600 is running ---------------\n",
      "loss:0.009875469507652722, accuracy:0.898690938949585\n",
      "Epoch 227/600 is running ---------------\n",
      "loss:0.00986860713703631, accuracy:0.903244137763977\n",
      "Epoch 228/600 is running ---------------\n",
      "loss:0.009855473251863847, accuracy:0.914627194404602\n",
      "Epoch 229/600 is running ---------------\n",
      "loss:0.009887987500778572, accuracy:0.912350594997406\n",
      "Epoch 230/600 is running ---------------\n",
      "loss:0.009873375169377335, accuracy:0.9049516320228577\n",
      "Epoch 231/600 is running ---------------\n",
      "loss:0.009847196890809824, accuracy:0.9026750326156616\n",
      "Epoch 232/600 is running ---------------\n",
      "loss:0.009917636146990495, accuracy:0.9095048308372498\n",
      "Epoch 233/600 is running ---------------\n",
      "loss:0.0098650149479466, accuracy:0.9015367031097412\n",
      "Epoch 234/600 is running ---------------\n",
      "loss:0.009871987673662302, accuracy:0.9026750326156616\n",
      "Epoch 235/600 is running ---------------\n",
      "loss:0.009901571999779464, accuracy:0.8992601037025452\n",
      "Epoch 236/600 is running ---------------\n",
      "loss:0.009905353553198791, accuracy:0.9015367031097412\n",
      "Epoch 237/600 is running ---------------\n",
      "loss:0.009913409271902433, accuracy:0.9169037938117981\n",
      "Epoch 238/600 is running ---------------\n",
      "loss:0.009829586483092897, accuracy:0.9271485209465027\n",
      "Epoch 239/600 is running ---------------\n",
      "loss:0.009915903303931231, accuracy:0.9095048308372498\n",
      "Epoch 240/600 is running ---------------\n",
      "loss:0.009898269149343874, accuracy:0.9003984332084656\n",
      "Epoch 241/600 is running ---------------\n",
      "loss:0.009894045225652565, accuracy:0.91007399559021\n",
      "Epoch 242/600 is running ---------------\n",
      "loss:0.009858190029537264, accuracy:0.9077973961830139\n",
      "Epoch 243/600 is running ---------------\n",
      "loss:0.009827324424064084, accuracy:0.9089356660842896\n",
      "Epoch 244/600 is running ---------------\n",
      "loss:0.009960359005881903, accuracy:0.8850312829017639\n",
      "Epoch 245/600 is running ---------------\n",
      "loss:0.009911300481960187, accuracy:0.9072282314300537\n",
      "Epoch 246/600 is running ---------------\n",
      "loss:0.009892569730820545, accuracy:0.8981217741966248\n",
      "Epoch 247/600 is running ---------------\n",
      "loss:0.009911743395017778, accuracy:0.9021058678627014\n",
      "Epoch 248/600 is running ---------------\n",
      "loss:0.009857867648809148, accuracy:0.921457052230835\n",
      "Epoch 249/600 is running ---------------\n",
      "loss:0.00987431272974595, accuracy:0.9083665609359741\n",
      "Epoch 250/600 is running ---------------\n",
      "loss:0.009861362509654203, accuracy:0.9049516320228577\n",
      "Epoch 251/600 is running ---------------\n",
      "loss:0.009853969532180117, accuracy:0.9163346886634827\n",
      "Epoch 252/600 is running ---------------\n",
      "loss:0.009845786801643785, accuracy:0.9049516320228577\n",
      "Epoch 253/600 is running ---------------\n",
      "loss:0.009960646919723797, accuracy:0.8969835042953491\n",
      "Epoch 254/600 is running ---------------\n",
      "loss:0.00990012571359807, accuracy:0.8935685753822327\n",
      "Epoch 255/600 is running ---------------\n",
      "loss:0.009930385604254821, accuracy:0.9083665609359741\n",
      "Epoch 256/600 is running ---------------\n",
      "loss:0.00989631352671589, accuracy:0.8964143395423889\n",
      "Epoch 257/600 is running ---------------\n",
      "loss:0.009881507692244763, accuracy:0.9134889245033264\n",
      "Epoch 258/600 is running ---------------\n",
      "loss:0.009829950929705773, accuracy:0.9134889245033264\n",
      "Epoch 259/600 is running ---------------\n",
      "loss:0.009890219808371147, accuracy:0.9026750326156616\n",
      "Epoch 260/600 is running ---------------\n",
      "loss:0.009856077338323447, accuracy:0.912350594997406\n",
      "Epoch 261/600 is running ---------------\n",
      "loss:0.00985882000008563, accuracy:0.9129197597503662\n",
      "Epoch 262/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.009885261360053386, accuracy:0.8992601037025452\n",
      "Epoch 263/600 is running ---------------\n",
      "loss:0.009859392978144968, accuracy:0.9072282314300537\n",
      "Epoch 264/600 is running ---------------\n",
      "loss:0.00988685555531345, accuracy:0.9083665609359741\n",
      "Epoch 265/600 is running ---------------\n",
      "loss:0.009819375362732767, accuracy:0.9191803932189941\n",
      "Epoch 266/600 is running ---------------\n",
      "loss:0.009787262062763178, accuracy:0.923733651638031\n",
      "Epoch 267/600 is running ---------------\n",
      "loss:0.009858198476638316, accuracy:0.9077973961830139\n",
      "Epoch 268/600 is running ---------------\n",
      "loss:0.009805291467046765, accuracy:0.9077973961830139\n",
      "Epoch 269/600 is running ---------------\n",
      "loss:0.009872036727911787, accuracy:0.9060899019241333\n",
      "Epoch 270/600 is running ---------------\n",
      "loss:0.009888508235722966, accuracy:0.8981217741966248\n",
      "Epoch 271/600 is running ---------------\n",
      "loss:0.009825462872967098, accuracy:0.9140580296516418\n",
      "Epoch 272/600 is running ---------------\n",
      "loss:0.009814820238047183, accuracy:0.91007399559021\n",
      "Epoch 273/600 is running ---------------\n",
      "loss:0.009824552892892281, accuracy:0.9157655239105225\n",
      "Epoch 274/600 is running ---------------\n",
      "loss:0.009901740127622098, accuracy:0.9112122654914856\n",
      "Epoch 275/600 is running ---------------\n",
      "loss:0.009833931380126988, accuracy:0.9163346886634827\n",
      "Epoch 276/600 is running ---------------\n",
      "loss:0.00984717606141205, accuracy:0.9208878874778748\n",
      "Epoch 277/600 is running ---------------\n",
      "loss:0.009863878829817102, accuracy:0.9231644868850708\n",
      "Epoch 278/600 is running ---------------\n",
      "loss:0.009847037311840546, accuracy:0.9174729585647583\n",
      "Epoch 279/600 is running ---------------\n",
      "loss:0.009839437092035823, accuracy:0.9066590666770935\n",
      "Epoch 280/600 is running ---------------\n",
      "loss:0.009848908904471314, accuracy:0.903244137763977\n",
      "Epoch 281/600 is running ---------------\n",
      "loss:0.009872856300258474, accuracy:0.9021058678627014\n",
      "Epoch 282/600 is running ---------------\n",
      "loss:0.009849697266645438, accuracy:0.91007399559021\n",
      "Epoch 283/600 is running ---------------\n",
      "loss:0.009846516983985358, accuracy:0.9043824672698975\n",
      "Epoch 284/600 is running ---------------\n",
      "loss:0.009877989559466688, accuracy:0.9095048308372498\n",
      "Epoch 285/600 is running ---------------\n",
      "loss:0.009895296516103642, accuracy:0.892430305480957\n",
      "Epoch 286/600 is running ---------------\n",
      "loss:0.009847665246942874, accuracy:0.9180421233177185\n",
      "Epoch 287/600 is running ---------------\n",
      "loss:0.009816053446952633, accuracy:0.9060899019241333\n",
      "Epoch 288/600 is running ---------------\n",
      "loss:0.009865130425585083, accuracy:0.8975526690483093\n",
      "Epoch 289/600 is running ---------------\n",
      "loss:0.009813601684353199, accuracy:0.9117814302444458\n",
      "Epoch 290/600 is running ---------------\n",
      "loss:0.009822468528302474, accuracy:0.9072282314300537\n",
      "Epoch 291/600 is running ---------------\n",
      "loss:0.009844958782196045, accuracy:0.9049516320228577\n",
      "Epoch 292/600 is running ---------------\n",
      "loss:0.009806616745961276, accuracy:0.9197495579719543\n",
      "Epoch 293/600 is running ---------------\n",
      "loss:0.009798908274351498, accuracy:0.9072282314300537\n",
      "Epoch 294/600 is running ---------------\n",
      "loss:0.009873248293241045, accuracy:0.9106431603431702\n",
      "Epoch 295/600 is running ---------------\n",
      "loss:0.009810917643436872, accuracy:0.9129197597503662\n",
      "Epoch 296/600 is running ---------------\n",
      "loss:0.009896026087811404, accuracy:0.8918611407279968\n",
      "Epoch 297/600 is running ---------------\n",
      "loss:0.009891292759825305, accuracy:0.9060899019241333\n",
      "Epoch 298/600 is running ---------------\n",
      "loss:0.00982875303578906, accuracy:0.9134889245033264\n",
      "Epoch 299/600 is running ---------------\n",
      "loss:0.009921881612485691, accuracy:0.8935685753822327\n",
      "Epoch 300/600 is running ---------------\n",
      "loss:0.00981032892459485, accuracy:0.9095048308372498\n",
      "Epoch 301/600 is running ---------------\n",
      "loss:0.009914073234399612, accuracy:0.8935685753822327\n",
      "Epoch 302/600 is running ---------------\n",
      "loss:0.00983087539537155, accuracy:0.91007399559021\n",
      "Epoch 303/600 is running ---------------\n",
      "loss:0.009775195090554965, accuracy:0.9169037938117981\n",
      "Epoch 304/600 is running ---------------\n",
      "loss:0.009795430714797757, accuracy:0.9180421233177185\n",
      "Epoch 305/600 is running ---------------\n",
      "loss:0.009787825406377942, accuracy:0.9151963591575623\n",
      "Epoch 306/600 is running ---------------\n",
      "loss:0.009701693370115642, accuracy:0.9288560152053833\n",
      "Epoch 307/600 is running ---------------\n",
      "loss:0.00975085780507947, accuracy:0.9186112880706787\n",
      "Epoch 308/600 is running ---------------\n",
      "loss:0.00974434247801232, accuracy:0.914627194404602\n",
      "Epoch 309/600 is running ---------------\n",
      "loss:0.009765375895329206, accuracy:0.914627194404602\n",
      "Epoch 310/600 is running ---------------\n",
      "loss:0.009825668181624003, accuracy:0.9203187227249146\n",
      "Epoch 311/600 is running ---------------\n",
      "loss:0.009840727700519507, accuracy:0.9077973961830139\n",
      "Epoch 312/600 is running ---------------\n",
      "loss:0.009772803543234116, accuracy:0.9254410862922668\n",
      "Epoch 313/600 is running ---------------\n",
      "loss:0.009753553617658707, accuracy:0.923733651638031\n",
      "Epoch 314/600 is running ---------------\n",
      "loss:0.009743285199492644, accuracy:0.9208878874778748\n",
      "Epoch 315/600 is running ---------------\n",
      "loss:0.00974898278611454, accuracy:0.9243028163909912\n",
      "Epoch 316/600 is running ---------------\n",
      "loss:0.009758516272564945, accuracy:0.912350594997406\n",
      "Epoch 317/600 is running ---------------\n",
      "loss:0.009777876790708347, accuracy:0.914627194404602\n",
      "Epoch 318/600 is running ---------------\n",
      "loss:0.009722428560664046, accuracy:0.926010251045227\n",
      "Epoch 319/600 is running ---------------\n",
      "loss:0.009731546001944544, accuracy:0.9328400492668152\n",
      "Epoch 320/600 is running ---------------\n",
      "loss:0.009779337121467394, accuracy:0.9066590666770935\n",
      "Epoch 321/600 is running ---------------\n",
      "loss:0.009755518196249252, accuracy:0.9203187227249146\n",
      "Epoch 322/600 is running ---------------\n",
      "loss:0.009752404574446876, accuracy:0.9134889245033264\n",
      "Epoch 323/600 is running ---------------\n",
      "loss:0.009724499321689508, accuracy:0.9265794157981873\n",
      "Epoch 324/600 is running ---------------\n",
      "loss:0.00980883405910138, accuracy:0.914627194404602\n",
      "Epoch 325/600 is running ---------------\n",
      "loss:0.009721964614664076, accuracy:0.923733651638031\n",
      "Epoch 326/600 is running ---------------\n",
      "loss:0.009769472467946816, accuracy:0.9282868504524231\n",
      "Epoch 327/600 is running ---------------\n",
      "loss:0.009788951109808552, accuracy:0.923733651638031\n",
      "Epoch 328/600 is running ---------------\n",
      "loss:0.009746989304190273, accuracy:0.9112122654914856\n",
      "Epoch 329/600 is running ---------------\n",
      "loss:0.009765275615687796, accuracy:0.9254410862922668\n",
      "Epoch 330/600 is running ---------------\n",
      "loss:0.009792580513636634, accuracy:0.9129197597503662\n",
      "Epoch 331/600 is running ---------------\n",
      "loss:0.0097459155724818, accuracy:0.9174729585647583\n",
      "Epoch 332/600 is running ---------------\n",
      "loss:0.009772400490994744, accuracy:0.9129197597503662\n",
      "Epoch 333/600 is running ---------------\n",
      "loss:0.009721454837204178, accuracy:0.9186112880706787\n",
      "Epoch 334/600 is running ---------------\n",
      "loss:0.00973347126250248, accuracy:0.9157655239105225\n",
      "Epoch 335/600 is running ---------------\n",
      "loss:0.009790878337964323, accuracy:0.9169037938117981\n",
      "Epoch 336/600 is running ---------------\n",
      "loss:0.009743903839391405, accuracy:0.9254410862922668\n",
      "Epoch 337/600 is running ---------------\n",
      "loss:0.00974157454279518, accuracy:0.9299942851066589\n",
      "Epoch 338/600 is running ---------------\n",
      "loss:0.00973095382284426, accuracy:0.9243028163909912\n",
      "Epoch 339/600 is running ---------------\n",
      "loss:0.009779916375485346, accuracy:0.9163346886634827\n",
      "Epoch 340/600 is running ---------------\n",
      "loss:0.00978596412667376, accuracy:0.9243028163909912\n",
      "Epoch 341/600 is running ---------------\n",
      "loss:0.009751022845828947, accuracy:0.923733651638031\n",
      "Epoch 342/600 is running ---------------\n",
      "loss:0.009731310500838095, accuracy:0.9305634498596191\n",
      "Epoch 343/600 is running ---------------\n",
      "loss:0.009750518190908242, accuracy:0.9277176856994629\n",
      "Epoch 344/600 is running ---------------\n",
      "loss:0.009755413947488072, accuracy:0.9191803932189941\n",
      "Epoch 345/600 is running ---------------\n",
      "loss:0.009793881367198703, accuracy:0.9117814302444458\n",
      "Epoch 346/600 is running ---------------\n",
      "loss:0.009703326849484214, accuracy:0.921457052230835\n",
      "Epoch 347/600 is running ---------------\n",
      "loss:0.009775076491899225, accuracy:0.9089356660842896\n",
      "Epoch 348/600 is running ---------------\n",
      "loss:0.009754306563071791, accuracy:0.9277176856994629\n",
      "Epoch 349/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.009720678552009874, accuracy:0.9174729585647583\n",
      "Epoch 350/600 is running ---------------\n",
      "loss:0.009730391327332012, accuracy:0.9243028163909912\n",
      "Epoch 351/600 is running ---------------\n",
      "loss:0.009693941170340175, accuracy:0.9345475435256958\n",
      "Epoch 352/600 is running ---------------\n",
      "loss:0.009720683403156261, accuracy:0.9174729585647583\n",
      "Epoch 353/600 is running ---------------\n",
      "loss:0.009779142329281681, accuracy:0.9191803932189941\n",
      "Epoch 354/600 is running ---------------\n",
      "loss:0.009698558104737074, accuracy:0.9271485209465027\n",
      "Epoch 355/600 is running ---------------\n",
      "loss:0.00975186718276908, accuracy:0.9288560152053833\n",
      "Epoch 356/600 is running ---------------\n",
      "loss:0.009786617878016655, accuracy:0.9191803932189941\n",
      "Epoch 357/600 is running ---------------\n",
      "loss:0.009710723728229868, accuracy:0.9186112880706787\n",
      "Epoch 358/600 is running ---------------\n",
      "loss:0.00972843078570944, accuracy:0.9089356660842896\n",
      "Epoch 359/600 is running ---------------\n",
      "loss:0.009755667394443744, accuracy:0.9169037938117981\n",
      "Epoch 360/600 is running ---------------\n",
      "loss:0.009686374467546856, accuracy:0.9299942851066589\n",
      "Epoch 361/600 is running ---------------\n",
      "loss:0.009703575614914, accuracy:0.9271485209465027\n",
      "Epoch 362/600 is running ---------------\n",
      "loss:0.009713989330002986, accuracy:0.923733651638031\n",
      "Epoch 363/600 is running ---------------\n",
      "loss:0.00975197631660075, accuracy:0.9191803932189941\n",
      "Epoch 364/600 is running ---------------\n",
      "loss:0.009715152485640671, accuracy:0.9134889245033264\n",
      "Epoch 365/600 is running ---------------\n",
      "loss:0.009742519668238232, accuracy:0.9254410862922668\n",
      "Epoch 366/600 is running ---------------\n",
      "loss:0.009707829357889747, accuracy:0.9248719215393066\n",
      "Epoch 367/600 is running ---------------\n",
      "loss:0.009742060675156951, accuracy:0.9254410862922668\n",
      "Epoch 368/600 is running ---------------\n",
      "loss:0.00973997217182687, accuracy:0.9288560152053833\n",
      "Epoch 369/600 is running ---------------\n",
      "loss:0.009728881060296863, accuracy:0.9248719215393066\n",
      "Epoch 370/600 is running ---------------\n",
      "loss:0.00972633078210925, accuracy:0.9191803932189941\n",
      "Epoch 371/600 is running ---------------\n",
      "loss:0.009650397110745251, accuracy:0.9311326146125793\n",
      "Epoch 372/600 is running ---------------\n",
      "loss:0.009741011300952718, accuracy:0.9328400492668152\n",
      "Epoch 373/600 is running ---------------\n",
      "loss:0.009736496817339667, accuracy:0.9294251799583435\n",
      "Epoch 374/600 is running ---------------\n",
      "loss:0.009719310698349094, accuracy:0.9248719215393066\n",
      "Epoch 375/600 is running ---------------\n",
      "loss:0.009752449422107885, accuracy:0.9203187227249146\n",
      "Epoch 376/600 is running ---------------\n",
      "loss:0.009735548536955258, accuracy:0.9271485209465027\n",
      "Epoch 377/600 is running ---------------\n",
      "loss:0.009727177290191824, accuracy:0.9140580296516418\n",
      "Epoch 378/600 is running ---------------\n",
      "loss:0.0097499893481038, accuracy:0.9220261573791504\n",
      "Epoch 379/600 is running ---------------\n",
      "loss:0.009756689663291587, accuracy:0.9117814302444458\n",
      "Epoch 380/600 is running ---------------\n",
      "loss:0.009744614549299228, accuracy:0.9157655239105225\n",
      "Epoch 381/600 is running ---------------\n",
      "loss:0.009784357752661578, accuracy:0.91007399559021\n",
      "Epoch 382/600 is running ---------------\n",
      "loss:0.009690990587765342, accuracy:0.9294251799583435\n",
      "Epoch 383/600 is running ---------------\n",
      "loss:0.009706867473940993, accuracy:0.9271485209465027\n",
      "Epoch 384/600 is running ---------------\n",
      "loss:0.009711733173767681, accuracy:0.9163346886634827\n",
      "Epoch 385/600 is running ---------------\n",
      "loss:0.009767418702895762, accuracy:0.903244137763977\n",
      "Epoch 386/600 is running ---------------\n",
      "loss:0.00971164721009673, accuracy:0.9203187227249146\n",
      "Epoch 387/600 is running ---------------\n",
      "loss:0.009746459036573606, accuracy:0.9317017793655396\n",
      "Epoch 388/600 is running ---------------\n",
      "loss:0.009706167925062271, accuracy:0.9191803932189941\n",
      "Epoch 389/600 is running ---------------\n",
      "loss:0.009683035962881524, accuracy:0.9311326146125793\n",
      "Epoch 390/600 is running ---------------\n",
      "loss:0.009773149195895251, accuracy:0.9151963591575623\n",
      "Epoch 391/600 is running ---------------\n",
      "loss:0.009734862150627575, accuracy:0.9169037938117981\n",
      "Epoch 392/600 is running ---------------\n",
      "loss:0.009717766608985635, accuracy:0.9317017793655396\n",
      "Epoch 393/600 is running ---------------\n",
      "loss:0.009748558259919481, accuracy:0.9129197597503662\n",
      "Epoch 394/600 is running ---------------\n",
      "loss:0.009689315279980716, accuracy:0.9151963591575623\n",
      "Epoch 395/600 is running ---------------\n",
      "loss:0.009707489099160608, accuracy:0.9197495579719543\n",
      "Epoch 396/600 is running ---------------\n",
      "loss:0.009726781090620775, accuracy:0.9043824672698975\n",
      "Epoch 397/600 is running ---------------\n",
      "loss:0.009741982174788135, accuracy:0.9248719215393066\n",
      "Epoch 398/600 is running ---------------\n",
      "loss:0.00968844309135398, accuracy:0.9134889245033264\n",
      "Epoch 399/600 is running ---------------\n",
      "loss:0.009698879773059079, accuracy:0.9305634498596191\n",
      "Epoch 400/600 is running ---------------\n",
      "loss:0.009737340713266493, accuracy:0.9248719215393066\n",
      "Epoch 401/600 is running ---------------\n",
      "loss:0.009727811976190175, accuracy:0.923733651638031\n",
      "Epoch 402/600 is running ---------------\n",
      "loss:0.009744545547678583, accuracy:0.9157655239105225\n",
      "Epoch 403/600 is running ---------------\n",
      "loss:0.00969413477518237, accuracy:0.9334092140197754\n",
      "Epoch 404/600 is running ---------------\n",
      "loss:0.00969978771768786, accuracy:0.9197495579719543\n",
      "Epoch 405/600 is running ---------------\n",
      "loss:0.009717112654098137, accuracy:0.9163346886634827\n",
      "Epoch 406/600 is running ---------------\n",
      "loss:0.009705824952405084, accuracy:0.9220261573791504\n",
      "Epoch 407/600 is running ---------------\n",
      "loss:0.009712092022903955, accuracy:0.9248719215393066\n",
      "Epoch 408/600 is running ---------------\n",
      "loss:0.0097190892757444, accuracy:0.9208878874778748\n",
      "Epoch 409/600 is running ---------------\n",
      "loss:0.009686481837325294, accuracy:0.921457052230835\n",
      "Epoch 410/600 is running ---------------\n",
      "loss:0.00971538846168453, accuracy:0.9197495579719543\n",
      "Epoch 411/600 is running ---------------\n",
      "loss:0.00973565726369772, accuracy:0.9305634498596191\n",
      "Epoch 412/600 is running ---------------\n",
      "loss:0.009736554895399915, accuracy:0.9197495579719543\n",
      "Epoch 413/600 is running ---------------\n",
      "loss:0.009708358811328, accuracy:0.9294251799583435\n",
      "Epoch 414/600 is running ---------------\n",
      "loss:0.00978650012746343, accuracy:0.9169037938117981\n",
      "Epoch 415/600 is running ---------------\n",
      "loss:0.00969066735893471, accuracy:0.9271485209465027\n",
      "Epoch 416/600 is running ---------------\n",
      "loss:0.009688514772978572, accuracy:0.9305634498596191\n",
      "Epoch 417/600 is running ---------------\n",
      "loss:0.009703301813497961, accuracy:0.9334092140197754\n",
      "Epoch 418/600 is running ---------------\n",
      "loss:0.00970447731750827, accuracy:0.9203187227249146\n",
      "Epoch 419/600 is running ---------------\n",
      "loss:0.009734324555405176, accuracy:0.9271485209465027\n",
      "Epoch 420/600 is running ---------------\n",
      "loss:0.009766044132263061, accuracy:0.9225953221321106\n",
      "Epoch 421/600 is running ---------------\n",
      "loss:0.00972492625649571, accuracy:0.921457052230835\n",
      "Epoch 422/600 is running ---------------\n",
      "loss:0.009719973948440161, accuracy:0.9077973961830139\n",
      "Epoch 423/600 is running ---------------\n",
      "loss:0.00971176638546218, accuracy:0.9140580296516418\n",
      "Epoch 424/600 is running ---------------\n",
      "loss:0.0097012199253675, accuracy:0.9351166486740112\n",
      "Epoch 425/600 is running ---------------\n",
      "loss:0.009728198846633555, accuracy:0.921457052230835\n",
      "Epoch 426/600 is running ---------------\n",
      "loss:0.009659907766275943, accuracy:0.9271485209465027\n",
      "Epoch 427/600 is running ---------------\n",
      "loss:0.009739014596238894, accuracy:0.9163346886634827\n",
      "Epoch 428/600 is running ---------------\n",
      "loss:0.009714425390392254, accuracy:0.9334092140197754\n",
      "Epoch 429/600 is running ---------------\n",
      "loss:0.009752695439685525, accuracy:0.9106431603431702\n",
      "Epoch 430/600 is running ---------------\n",
      "loss:0.009688297251645448, accuracy:0.9231644868850708\n",
      "Epoch 431/600 is running ---------------\n",
      "loss:0.009744582050010843, accuracy:0.9095048308372498\n",
      "Epoch 432/600 is running ---------------\n",
      "loss:0.009733413455835038, accuracy:0.9174729585647583\n",
      "Epoch 433/600 is running ---------------\n",
      "loss:0.009710221889009513, accuracy:0.9254410862922668\n",
      "Epoch 434/600 is running ---------------\n",
      "loss:0.009665403741967902, accuracy:0.921457052230835\n",
      "Epoch 435/600 is running ---------------\n",
      "loss:0.009681880847255683, accuracy:0.9288560152053833\n",
      "Epoch 436/600 is running ---------------\n",
      "loss:0.009727236657367895, accuracy:0.9225953221321106\n",
      "Epoch 437/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.009642142562887923, accuracy:0.9385315775871277\n",
      "Epoch 438/600 is running ---------------\n",
      "loss:0.00976131677491867, accuracy:0.9225953221321106\n",
      "Epoch 439/600 is running ---------------\n",
      "loss:0.009715303210419692, accuracy:0.9208878874778748\n",
      "Epoch 440/600 is running ---------------\n",
      "loss:0.009735114240619224, accuracy:0.9254410862922668\n",
      "Epoch 441/600 is running ---------------\n",
      "loss:0.009727844645099064, accuracy:0.923733651638031\n",
      "Epoch 442/600 is running ---------------\n",
      "loss:0.009756819049811323, accuracy:0.9095048308372498\n",
      "Epoch 443/600 is running ---------------\n",
      "loss:0.009716537945909669, accuracy:0.9203187227249146\n",
      "Epoch 444/600 is running ---------------\n",
      "loss:0.009664232003532761, accuracy:0.926010251045227\n",
      "Epoch 445/600 is running ---------------\n",
      "loss:0.009727522671460155, accuracy:0.9220261573791504\n",
      "Epoch 446/600 is running ---------------\n",
      "loss:0.009703686241406098, accuracy:0.923733651638031\n",
      "Epoch 447/600 is running ---------------\n",
      "loss:0.009719714734387385, accuracy:0.9225953221321106\n",
      "Epoch 448/600 is running ---------------\n",
      "loss:0.009730904293657366, accuracy:0.9186112880706787\n",
      "Epoch 449/600 is running ---------------\n",
      "loss:0.009685630274551731, accuracy:0.9265794157981873\n",
      "Epoch 450/600 is running ---------------\n",
      "loss:0.00972954152861168, accuracy:0.9254410862922668\n",
      "Epoch 451/600 is running ---------------\n",
      "loss:0.009687776414928751, accuracy:0.9151963591575623\n",
      "Epoch 452/600 is running ---------------\n",
      "loss:0.009703748288586117, accuracy:0.9368241429328918\n",
      "Epoch 453/600 is running ---------------\n",
      "loss:0.009688403094839359, accuracy:0.9208878874778748\n",
      "Epoch 454/600 is running ---------------\n",
      "loss:0.009666122898976778, accuracy:0.9203187227249146\n",
      "Epoch 455/600 is running ---------------\n",
      "loss:0.009699810412911168, accuracy:0.9197495579719543\n",
      "Epoch 456/600 is running ---------------\n",
      "loss:0.009690941872756863, accuracy:0.9265794157981873\n",
      "Epoch 457/600 is running ---------------\n",
      "loss:0.009707265166172468, accuracy:0.9299942851066589\n",
      "Epoch 458/600 is running ---------------\n",
      "loss:0.009710790253391168, accuracy:0.9180421233177185\n",
      "Epoch 459/600 is running ---------------\n",
      "loss:0.009691733084555438, accuracy:0.9169037938117981\n",
      "Epoch 460/600 is running ---------------\n",
      "loss:0.009677954573623145, accuracy:0.9425156712532043\n",
      "Epoch 461/600 is running ---------------\n",
      "loss:0.009713456246685969, accuracy:0.9151963591575623\n",
      "Epoch 462/600 is running ---------------\n",
      "loss:0.009663984425446494, accuracy:0.9299942851066589\n",
      "Epoch 463/600 is running ---------------\n",
      "loss:0.009697646123140322, accuracy:0.937393307685852\n",
      "Epoch 464/600 is running ---------------\n",
      "loss:0.00970765929637338, accuracy:0.9271485209465027\n",
      "Epoch 465/600 is running ---------------\n",
      "loss:0.009692051699708387, accuracy:0.9180421233177185\n",
      "Epoch 466/600 is running ---------------\n",
      "loss:0.009655409057369677, accuracy:0.9288560152053833\n",
      "Epoch 467/600 is running ---------------\n",
      "loss:0.009679271710753508, accuracy:0.9299942851066589\n",
      "Epoch 468/600 is running ---------------\n",
      "loss:0.009729969718609606, accuracy:0.9265794157981873\n",
      "Epoch 469/600 is running ---------------\n",
      "loss:0.009690581666656563, accuracy:0.9248719215393066\n",
      "Epoch 470/600 is running ---------------\n",
      "loss:0.009641720411379905, accuracy:0.9385315775871277\n",
      "Epoch 471/600 is running ---------------\n",
      "loss:0.009686804048432907, accuracy:0.9117814302444458\n",
      "Epoch 472/600 is running ---------------\n",
      "loss:0.009693275104548758, accuracy:0.9197495579719543\n",
      "Epoch 473/600 is running ---------------\n",
      "loss:0.009737110979257148, accuracy:0.9277176856994629\n",
      "Epoch 474/600 is running ---------------\n",
      "loss:0.009668742959039349, accuracy:0.9282868504524231\n",
      "Epoch 475/600 is running ---------------\n",
      "loss:0.00971756340362297, accuracy:0.9288560152053833\n",
      "Epoch 476/600 is running ---------------\n",
      "loss:0.009696926151953032, accuracy:0.9174729585647583\n",
      "Epoch 477/600 is running ---------------\n",
      "loss:0.00972185788944355, accuracy:0.9203187227249146\n",
      "Epoch 478/600 is running ---------------\n",
      "loss:0.009674369610366865, accuracy:0.9322709441184998\n",
      "Epoch 479/600 is running ---------------\n",
      "loss:0.009706768822656413, accuracy:0.9208878874778748\n",
      "Epoch 480/600 is running ---------------\n",
      "loss:0.009719750388617127, accuracy:0.9220261573791504\n",
      "Epoch 481/600 is running ---------------\n",
      "loss:0.009670861044185164, accuracy:0.9305634498596191\n",
      "Epoch 482/600 is running ---------------\n",
      "loss:0.009739940486716898, accuracy:0.9180421233177185\n",
      "Epoch 483/600 is running ---------------\n",
      "loss:0.009697809908698077, accuracy:0.9277176856994629\n",
      "Epoch 484/600 is running ---------------\n",
      "loss:0.00970455218799832, accuracy:0.9271485209465027\n",
      "Epoch 485/600 is running ---------------\n",
      "loss:0.009681584554160938, accuracy:0.937393307685852\n",
      "Epoch 486/600 is running ---------------\n",
      "loss:0.009709467688404706, accuracy:0.9169037938117981\n",
      "Epoch 487/600 is running ---------------\n",
      "loss:0.0097275528639097, accuracy:0.912350594997406\n",
      "Epoch 488/600 is running ---------------\n",
      "loss:0.009657554044327277, accuracy:0.9311326146125793\n",
      "Epoch 489/600 is running ---------------\n",
      "loss:0.009697061271645768, accuracy:0.926010251045227\n",
      "Epoch 490/600 is running ---------------\n",
      "loss:0.009694320272164516, accuracy:0.9243028163909912\n",
      "Epoch 491/600 is running ---------------\n",
      "loss:0.009713721634925057, accuracy:0.9311326146125793\n",
      "Epoch 492/600 is running ---------------\n",
      "loss:0.009694242823442818, accuracy:0.9288560152053833\n",
      "Epoch 493/600 is running ---------------\n",
      "loss:0.009730918609627826, accuracy:0.9277176856994629\n",
      "Epoch 494/600 is running ---------------\n",
      "loss:0.009659431845068457, accuracy:0.9248719215393066\n",
      "Epoch 495/600 is running ---------------\n",
      "loss:0.009686767444328347, accuracy:0.9186112880706787\n",
      "Epoch 496/600 is running ---------------\n",
      "loss:0.009699015876550735, accuracy:0.9254410862922668\n",
      "Epoch 497/600 is running ---------------\n",
      "loss:0.009705974082751374, accuracy:0.9220261573791504\n",
      "Epoch 498/600 is running ---------------\n",
      "loss:0.00971082577192451, accuracy:0.9288560152053833\n",
      "Epoch 499/600 is running ---------------\n",
      "loss:0.009717293774871304, accuracy:0.9248719215393066\n",
      "Epoch 500/600 is running ---------------\n",
      "loss:0.009701191293426583, accuracy:0.921457052230835\n",
      "Epoch 501/600 is running ---------------\n",
      "loss:0.009690295754336605, accuracy:0.9220261573791504\n",
      "Epoch 502/600 is running ---------------\n",
      "loss:0.009704333445397574, accuracy:0.9299942851066589\n",
      "Epoch 503/600 is running ---------------\n",
      "loss:0.009696329596643767, accuracy:0.9186112880706787\n",
      "Epoch 504/600 is running ---------------\n",
      "loss:0.00971715380403218, accuracy:0.9254410862922668\n",
      "Epoch 505/600 is running ---------------\n",
      "loss:0.009691030753900466, accuracy:0.9328400492668152\n",
      "Epoch 506/600 is running ---------------\n",
      "loss:0.009710853725383413, accuracy:0.9294251799583435\n",
      "Epoch 507/600 is running ---------------\n",
      "loss:0.009686733350607232, accuracy:0.9317017793655396\n",
      "Epoch 508/600 is running ---------------\n",
      "loss:0.00970144497785096, accuracy:0.9362549781799316\n",
      "Epoch 509/600 is running ---------------\n",
      "loss:0.009710279967069762, accuracy:0.9254410862922668\n",
      "Epoch 510/600 is running ---------------\n",
      "loss:0.009708650253276357, accuracy:0.9294251799583435\n",
      "Epoch 511/600 is running ---------------\n",
      "loss:0.009660100014154112, accuracy:0.9305634498596191\n",
      "Epoch 512/600 is running ---------------\n",
      "loss:0.009696410946637035, accuracy:0.9186112880706787\n",
      "Epoch 513/600 is running ---------------\n",
      "loss:0.009683031620596645, accuracy:0.9311326146125793\n",
      "Epoch 514/600 is running ---------------\n",
      "loss:0.009701025201029986, accuracy:0.9265794157981873\n",
      "Epoch 515/600 is running ---------------\n",
      "loss:0.009741806990732575, accuracy:0.9134889245033264\n",
      "Epoch 516/600 is running ---------------\n",
      "loss:0.009708034700470752, accuracy:0.9248719215393066\n",
      "Epoch 517/600 is running ---------------\n",
      "loss:0.0096892368474601, accuracy:0.9282868504524231\n",
      "Epoch 518/600 is running ---------------\n",
      "loss:0.009687806403833692, accuracy:0.9208878874778748\n",
      "Epoch 519/600 is running ---------------\n",
      "loss:0.009720455297503745, accuracy:0.9174729585647583\n",
      "Epoch 520/600 is running ---------------\n",
      "loss:0.009662087966449979, accuracy:0.9328400492668152\n",
      "Epoch 521/600 is running ---------------\n",
      "loss:0.009748822562587349, accuracy:0.9112122654914856\n",
      "Epoch 522/600 is running ---------------\n",
      "loss:0.009686019655378556, accuracy:0.9191803932189941\n",
      "Epoch 523/600 is running ---------------\n",
      "loss:0.009647182937908661, accuracy:0.9299942851066589\n",
      "Epoch 524/600 is running ---------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.009687361964191574, accuracy:0.9248719215393066\n",
      "Epoch 525/600 is running ---------------\n",
      "loss:0.0096958824430736, accuracy:0.926010251045227\n",
      "Epoch 526/600 is running ---------------\n",
      "loss:0.009707126891538374, accuracy:0.9191803932189941\n",
      "Epoch 527/600 is running ---------------\n",
      "loss:0.009682958276691122, accuracy:0.9197495579719543\n",
      "Epoch 528/600 is running ---------------\n",
      "loss:0.009669370656672973, accuracy:0.9265794157981873\n",
      "Epoch 529/600 is running ---------------\n",
      "loss:0.00974907254928476, accuracy:0.9220261573791504\n",
      "Epoch 530/600 is running ---------------\n",
      "loss:0.009755800342994043, accuracy:0.914627194404602\n",
      "Epoch 531/600 is running ---------------\n",
      "loss:0.009647358902218536, accuracy:0.9277176856994629\n",
      "Epoch 532/600 is running ---------------\n",
      "loss:0.009693779114911552, accuracy:0.9271485209465027\n",
      "Epoch 533/600 is running ---------------\n",
      "loss:0.009707744344093614, accuracy:0.9134889245033264\n",
      "Epoch 534/600 is running ---------------\n",
      "loss:0.009714865148508488, accuracy:0.9140580296516418\n",
      "Epoch 535/600 is running ---------------\n",
      "loss:0.009661318364303494, accuracy:0.9305634498596191\n",
      "Epoch 536/600 is running ---------------\n",
      "loss:0.009629316912093622, accuracy:0.9396699070930481\n",
      "Epoch 537/600 is running ---------------\n",
      "loss:0.009706040099051166, accuracy:0.9282868504524231\n",
      "Epoch 538/600 is running ---------------\n",
      "loss:0.00972774904698354, accuracy:0.921457052230835\n",
      "Epoch 539/600 is running ---------------\n",
      "loss:0.009675721825017261, accuracy:0.9271485209465027\n",
      "Epoch 540/600 is running ---------------\n",
      "loss:0.009671002168443708, accuracy:0.9254410862922668\n",
      "Epoch 541/600 is running ---------------\n",
      "loss:0.009690315769555966, accuracy:0.9277176856994629\n",
      "Epoch 542/600 is running ---------------\n",
      "loss:0.009715659990185829, accuracy:0.9282868504524231\n",
      "Epoch 543/600 is running ---------------\n",
      "loss:0.009722495425066353, accuracy:0.9157655239105225\n",
      "Epoch 544/600 is running ---------------\n",
      "loss:0.009696763485890596, accuracy:0.926010251045227\n",
      "Epoch 545/600 is running ---------------\n",
      "loss:0.009715842841088127, accuracy:0.9134889245033264\n",
      "Epoch 546/600 is running ---------------\n",
      "loss:0.00965659975937706, accuracy:0.9254410862922668\n",
      "Epoch 547/600 is running ---------------\n",
      "loss:0.00971788435953886, accuracy:0.9169037938117981\n",
      "Epoch 548/600 is running ---------------\n",
      "loss:0.009664585662281643, accuracy:0.9294251799583435\n",
      "Epoch 549/600 is running ---------------\n",
      "loss:0.00969042737984698, accuracy:0.9282868504524231\n",
      "Epoch 550/600 is running ---------------\n",
      "loss:0.009691937918274933, accuracy:0.9317017793655396\n",
      "Epoch 551/600 is running ---------------\n",
      "loss:0.00971320687062237, accuracy:0.926010251045227\n",
      "Epoch 552/600 is running ---------------\n",
      "loss:0.009699520904636546, accuracy:0.9140580296516418\n",
      "Epoch 553/600 is running ---------------\n",
      "loss:0.009697998662393882, accuracy:0.9282868504524231\n",
      "Epoch 554/600 is running ---------------\n",
      "loss:0.009685658160162433, accuracy:0.9203187227249146\n",
      "Epoch 555/600 is running ---------------\n",
      "loss:0.009706828223756585, accuracy:0.9282868504524231\n",
      "Epoch 556/600 is running ---------------\n",
      "loss:0.009684887133203722, accuracy:0.914627194404602\n",
      "Epoch 557/600 is running ---------------\n",
      "loss:0.009663459789230529, accuracy:0.9208878874778748\n",
      "Epoch 558/600 is running ---------------\n",
      "loss:0.009658574142032682, accuracy:0.9277176856994629\n",
      "Epoch 559/600 is running ---------------\n",
      "loss:0.009678131928821145, accuracy:0.923733651638031\n",
      "Epoch 560/600 is running ---------------\n",
      "loss:0.009721258314889333, accuracy:0.914627194404602\n",
      "Epoch 561/600 is running ---------------\n",
      "loss:0.00971679190172685, accuracy:0.9243028163909912\n",
      "Epoch 562/600 is running ---------------\n",
      "loss:0.0096578158366117, accuracy:0.9225953221321106\n",
      "Epoch 563/600 is running ---------------\n",
      "loss:0.009674227163068394, accuracy:0.9322709441184998\n",
      "Epoch 564/600 is running ---------------\n",
      "loss:0.009671755894111109, accuracy:0.9271485209465027\n",
      "Epoch 565/600 is running ---------------\n",
      "loss:0.009722868488400784, accuracy:0.9180421233177185\n",
      "Epoch 566/600 is running ---------------\n",
      "loss:0.009668005957953552, accuracy:0.9271485209465027\n",
      "Epoch 567/600 is running ---------------\n",
      "loss:0.0096449292920568, accuracy:0.923733651638031\n",
      "Epoch 568/600 is running ---------------\n",
      "loss:0.009725116638548345, accuracy:0.9197495579719543\n",
      "Epoch 569/600 is running ---------------\n",
      "loss:0.00965590200847567, accuracy:0.9231644868850708\n",
      "Epoch 570/600 is running ---------------\n",
      "loss:0.00973754564875829, accuracy:0.9089356660842896\n",
      "Epoch 571/600 is running ---------------\n",
      "loss:0.009695720523341379, accuracy:0.923733651638031\n",
      "Epoch 572/600 is running ---------------\n",
      "loss:0.00968449178173519, accuracy:0.9248719215393066\n",
      "Epoch 573/600 is running ---------------\n",
      "loss:0.009685584612712307, accuracy:0.9317017793655396\n",
      "Epoch 574/600 is running ---------------\n",
      "loss:0.009718890989376321, accuracy:0.9134889245033264\n",
      "Epoch 575/600 is running ---------------\n",
      "loss:0.00970783559992426, accuracy:0.9248719215393066\n",
      "Epoch 576/600 is running ---------------\n",
      "loss:0.009735154745995353, accuracy:0.9186112880706787\n",
      "Epoch 577/600 is running ---------------\n",
      "loss:0.009720735612347103, accuracy:0.9231644868850708\n",
      "Epoch 578/600 is running ---------------\n",
      "loss:0.009705813316438574, accuracy:0.9231644868850708\n",
      "Epoch 579/600 is running ---------------\n",
      "loss:0.009718073045386464, accuracy:0.9248719215393066\n",
      "Epoch 580/600 is running ---------------\n",
      "loss:0.009677714289218509, accuracy:0.9231644868850708\n",
      "Epoch 581/600 is running ---------------\n",
      "loss:0.009680027132625939, accuracy:0.9311326146125793\n",
      "Epoch 582/600 is running ---------------\n",
      "loss:0.009695863343804956, accuracy:0.9362549781799316\n",
      "Epoch 583/600 is running ---------------\n",
      "loss:0.009699257009057884, accuracy:0.9225953221321106\n",
      "Epoch 584/600 is running ---------------\n",
      "loss:0.009715230171831074, accuracy:0.9282868504524231\n",
      "Epoch 585/600 is running ---------------\n",
      "loss:0.009659192442690437, accuracy:0.9277176856994629\n",
      "Epoch 586/600 is running ---------------\n",
      "loss:0.009673713755729731, accuracy:0.9208878874778748\n",
      "Epoch 587/600 is running ---------------\n",
      "loss:0.009689928593795681, accuracy:0.9288560152053833\n",
      "Epoch 588/600 is running ---------------\n",
      "loss:0.009668973710771711, accuracy:0.9271485209465027\n",
      "Epoch 589/600 is running ---------------\n",
      "loss:0.009627248593603404, accuracy:0.9299942851066589\n",
      "Epoch 590/600 is running ---------------\n",
      "loss:0.009691129303412744, accuracy:0.9294251799583435\n",
      "Epoch 591/600 is running ---------------\n",
      "loss:0.009667248975572494, accuracy:0.9402390718460083\n",
      "Epoch 592/600 is running ---------------\n",
      "loss:0.009657061873475597, accuracy:0.9186112880706787\n",
      "Epoch 593/600 is running ---------------\n",
      "loss:0.009604356305193481, accuracy:0.9322709441184998\n",
      "Epoch 594/600 is running ---------------\n",
      "loss:0.00968881608684021, accuracy:0.9243028163909912\n",
      "Epoch 595/600 is running ---------------\n",
      "loss:0.00965947526791724, accuracy:0.9322709441184998\n",
      "Epoch 596/600 is running ---------------\n",
      "loss:0.009720994453234773, accuracy:0.9299942851066589\n",
      "Epoch 597/600 is running ---------------\n",
      "loss:0.009713649546211256, accuracy:0.9191803932189941\n",
      "Epoch 598/600 is running ---------------\n",
      "loss:0.0096983156831141, accuracy:0.9254410862922668\n",
      "Epoch 599/600 is running ---------------\n",
      "loss:0.00969179028058907, accuracy:0.923733651638031\n",
      "Epoch 600/600 is running ---------------\n",
      "loss:0.009652584061815309, accuracy:0.9339783787727356\n"
     ]
    }
   ],
   "source": [
    "mlp_model.train(True)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print(f'Epoch {epoch+1}/{epoch_num} is running ---------------')\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for i, (data, label) in enumerate(total_train_dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = mlp_model.forward(data)\n",
    "        \n",
    "        # view([r, c])는 크기 -1이 아닌 [r, c]만큼의 행렬로 만들어준다.\n",
    "        label = label.view([1,-1]).squeeze()\n",
    "        label = label.long()\n",
    "        \n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "    \n",
    "    # print(running_loss)\n",
    "    epoch_loss = running_loss / train_x.shape[0]\n",
    "    epoch_corrects = running_corrects / train_x.shape[0]\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'loss:{epoch_loss}, accuracy:{epoch_corrects}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "e654363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_torch = np.zeros(test_df.shape[0], dtype=float)\n",
    "test_dataset = TensorData(test_df.to_numpy(), empty_torch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "7bddb645",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.train(False)\n",
    "\n",
    "for i, (data,label) in enumerate(test_dataloader):\n",
    "    \n",
    "    output = mlp_model.forward(data)\n",
    "    _, preds = torch.max(output.data, 1)\n",
    "    \n",
    "sub_file['ProdTaken'] = preds.detach().numpy()\n",
    "file_name = 'mlp_result.csv'\n",
    "sub_file.to_csv(os.path.join(file_path, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "54ca6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2933"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bf619",
   "metadata": {},
   "source": [
    "### keras 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ce54af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200                                                                     \n",
      "\n",
      "  0%|                                    | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 5s - loss: 0.7592 - accuracy: 0.3359\n",
      "14/14 [==============================] - 1s 12ms/step - loss: 0.7511 - accuracy: 0.3375 - val_loss: 0.7231 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.7534 - accuracy: 0.3516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7279 - accuracy: 0.4007 - val_loss: 0.7029 - val_accuracy: 0.4949\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.7053 - accuracy: 0.4531\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7021 - accuracy: 0.4713 - val_loss: 0.6843 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.7047 - accuracy: 0.4609\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5487 - val_loss: 0.6676 - val_accuracy: 0.6122\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6687 - accuracy: 0.6016\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6621 - accuracy: 0.6153 - val_loss: 0.6526 - val_accuracy: 0.6888\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6374 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6439 - accuracy: 0.6904 - val_loss: 0.6391 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6391 - accuracy: 0.7031\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6303 - accuracy: 0.7262 - val_loss: 0.6270 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6149 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6187 - accuracy: 0.7450 - val_loss: 0.6163 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6237 - accuracy: 0.7344\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6034 - accuracy: 0.7775 - val_loss: 0.6065 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6140 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5905 - accuracy: 0.7945 - val_loss: 0.5979 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5841 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5809 - accuracy: 0.7997 - val_loss: 0.5901 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5587 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5693 - accuracy: 0.8071 - val_loss: 0.5830 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5685 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5642 - accuracy: 0.8065 - val_loss: 0.5768 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5532 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5545 - accuracy: 0.8065 - val_loss: 0.5712 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5326 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5459 - accuracy: 0.8071 - val_loss: 0.5661 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5366 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.8082 - val_loss: 0.5614 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5904 - accuracy: 0.7422\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5336 - accuracy: 0.8076 - val_loss: 0.5572 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5493 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5300 - accuracy: 0.8076 - val_loss: 0.5535 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5661 - accuracy: 0.7266\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5191 - accuracy: 0.8082 - val_loss: 0.5500 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5843 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.8082 - val_loss: 0.5467 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5127 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.8082 - val_loss: 0.5438 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5149 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.8082 - val_loss: 0.5410 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5306 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.8082 - val_loss: 0.5385 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4998 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.8082 - val_loss: 0.5362 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5281 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.8082 - val_loss: 0.5339 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5112 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.8082 - val_loss: 0.5318 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4854 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4914 - accuracy: 0.8082 - val_loss: 0.5297 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5130 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4905 - accuracy: 0.8082 - val_loss: 0.5278 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.8082 - val_loss: 0.5258 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4259 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.8082 - val_loss: 0.5239 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5095 - accuracy: 0.7578\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4834 - accuracy: 0.8082 - val_loss: 0.5221 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4483 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.8082 - val_loss: 0.5203 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4522 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4781 - accuracy: 0.8082 - val_loss: 0.5185 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4288 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.8082 - val_loss: 0.5168 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4485 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.8082 - val_loss: 0.5152 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4717 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - accuracy: 0.8082 - val_loss: 0.5136 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4980 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.8082 - val_loss: 0.5120 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4700 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.8082 - val_loss: 0.5103 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4623 - accuracy: 0.8082 - val_loss: 0.5087 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4599 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4624 - accuracy: 0.8082 - val_loss: 0.5072 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4988 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.8082 - val_loss: 0.5057 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5531 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4599 - accuracy: 0.8093 - val_loss: 0.5041 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4569 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4572 - accuracy: 0.8082 - val_loss: 0.5026 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4964 - accuracy: 0.7422\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8082 - val_loss: 0.5010 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3928 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8076 - val_loss: 0.4995 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4714 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4534 - accuracy: 0.8088 - val_loss: 0.4980 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4802 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.8082 - val_loss: 0.4966 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4929 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.8088 - val_loss: 0.4952 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4234 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4492 - accuracy: 0.8099 - val_loss: 0.4937 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4200 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.8093 - val_loss: 0.4923 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4018 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.8093 - val_loss: 0.4908 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3973 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8099 - val_loss: 0.4895 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4236 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8088 - val_loss: 0.4881 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4356 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.8105 - val_loss: 0.4868 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4070 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4413 - accuracy: 0.8093 - val_loss: 0.4856 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4244 - accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4390 - accuracy: 0.8093 - val_loss: 0.4842 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4798 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8105 - val_loss: 0.4828 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4281 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8116 - val_loss: 0.4815 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4703 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4346 - accuracy: 0.8116 - val_loss: 0.4802 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4615 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4305 - accuracy: 0.8145 - val_loss: 0.4790 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4328 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8173 - val_loss: 0.4778 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4099 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8162 - val_loss: 0.4766 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5005 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8133 - val_loss: 0.4755 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3981 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8127 - val_loss: 0.4743 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4006 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8173 - val_loss: 0.4733 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4701 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8179 - val_loss: 0.4722 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4366 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8167 - val_loss: 0.4711 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4175 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8190 - val_loss: 0.4699 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4646 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.8179 - val_loss: 0.4689 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4088 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8241 - val_loss: 0.4677 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3689 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4204 - accuracy: 0.8219 - val_loss: 0.4667 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4539 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8207 - val_loss: 0.4656 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4316 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.8230 - val_loss: 0.4646 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4704 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.8213 - val_loss: 0.4636 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4651 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8196 - val_loss: 0.4626 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4197 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4154 - accuracy: 0.8230 - val_loss: 0.4618 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4697 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4159 - accuracy: 0.8253 - val_loss: 0.4609 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4984 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.8253 - val_loss: 0.4599 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4476 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4155 - accuracy: 0.8258 - val_loss: 0.4591 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4506 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4150 - accuracy: 0.8253 - val_loss: 0.4583 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3894 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8270 - val_loss: 0.4575 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4127 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4154 - accuracy: 0.8310 - val_loss: 0.4566 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3677 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4155 - accuracy: 0.8258 - val_loss: 0.4559 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3656 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.8304 - val_loss: 0.4551 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4331 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8275 - val_loss: 0.4543 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3773 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4051 - accuracy: 0.8298 - val_loss: 0.4536 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4376 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8287 - val_loss: 0.4529 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3574 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8287 - val_loss: 0.4522 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3455 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8293 - val_loss: 0.4514 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5077 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4090 - accuracy: 0.8344 - val_loss: 0.4505 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4282 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8355 - val_loss: 0.4499 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3752 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4054 - accuracy: 0.8321 - val_loss: 0.4491 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4056 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4027 - accuracy: 0.8378 - val_loss: 0.4485 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4460 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4019 - accuracy: 0.8355 - val_loss: 0.4478 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3968 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8332 - val_loss: 0.4474 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4535 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8332 - val_loss: 0.4467 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3849 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8367 - val_loss: 0.4461 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3758 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8361 - val_loss: 0.4455 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3994 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4040 - accuracy: 0.8344 - val_loss: 0.4449 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4627 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4017 - accuracy: 0.8389 - val_loss: 0.4444 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4035 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8321 - val_loss: 0.4439 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4584 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8378 - val_loss: 0.4435 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3986 - accuracy: 0.8389 - val_loss: 0.4430 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4165 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3969 - accuracy: 0.8372 - val_loss: 0.4427 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3801 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8418 - val_loss: 0.4424 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3308 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8355 - val_loss: 0.4419 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3805 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8378 - val_loss: 0.4414 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4389 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3956 - accuracy: 0.8423 - val_loss: 0.4409 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3759 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8423 - val_loss: 0.4406 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3697 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4008 - accuracy: 0.8389 - val_loss: 0.4403 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3262 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8412 - val_loss: 0.4398 - val_accuracy: 0.8061\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4813 - accuracy: 0.7578\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8384 - val_loss: 0.4395 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3658 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8423 - val_loss: 0.4391 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4405 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3957 - accuracy: 0.8406 - val_loss: 0.4387 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.2686 - accuracy: 0.9375\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8372 - val_loss: 0.4384 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4550 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8423 - val_loss: 0.4379 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4189 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8418 - val_loss: 0.4375 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4655 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.8441 - val_loss: 0.4374 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3908 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8412 - val_loss: 0.4372 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4510 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8463 - val_loss: 0.4367 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3771 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3878 - accuracy: 0.8441 - val_loss: 0.4364 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3292 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8458 - val_loss: 0.4360 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3880 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8458 - val_loss: 0.4358 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4645 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3876 - accuracy: 0.8389 - val_loss: 0.4353 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3992 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8469 - val_loss: 0.4351 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4825 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8463 - val_loss: 0.4349 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4441 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8418 - val_loss: 0.4347 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5357 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3908 - accuracy: 0.8452 - val_loss: 0.4343 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4143 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3878 - accuracy: 0.8412 - val_loss: 0.4341 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3197 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8418 - val_loss: 0.4341 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3904 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3884 - accuracy: 0.8486 - val_loss: 0.4337 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3715 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8497 - val_loss: 0.4335 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4251 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.8458 - val_loss: 0.4332 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3880 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8435 - val_loss: 0.4330 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3872 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3867 - accuracy: 0.8469 - val_loss: 0.4329 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3986 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8423 - val_loss: 0.4327 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4061 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8509 - val_loss: 0.4325 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3890 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3842 - accuracy: 0.8435 - val_loss: 0.4324 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4608 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8480 - val_loss: 0.4322 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4934 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8463 - val_loss: 0.4320 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4843 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8429 - val_loss: 0.4319 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3452 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8520 - val_loss: 0.4317 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.2912 - accuracy: 0.8984\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8469 - val_loss: 0.4315 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4459 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3883 - accuracy: 0.8492 - val_loss: 0.4313 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4326 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8480 - val_loss: 0.4312 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4302 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8458 - val_loss: 0.4310 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3827 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3800 - accuracy: 0.8515 - val_loss: 0.4308 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3966 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3804 - accuracy: 0.8497 - val_loss: 0.4309 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3663 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3813 - accuracy: 0.8446 - val_loss: 0.4307 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3778 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8480 - val_loss: 0.4306 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4076 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8503 - val_loss: 0.4303 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3660 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3793 - accuracy: 0.8509 - val_loss: 0.4300 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3260 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8469 - val_loss: 0.4300 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4938 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8480 - val_loss: 0.4297 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3834 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3854 - accuracy: 0.8492 - val_loss: 0.4295 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3510 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.8486 - val_loss: 0.4296 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3931 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3841 - accuracy: 0.8469 - val_loss: 0.4295 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4124 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3810 - accuracy: 0.8532 - val_loss: 0.4294 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3394 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8480 - val_loss: 0.4292 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3281 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8486 - val_loss: 0.4292 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3750 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3806 - accuracy: 0.8537 - val_loss: 0.4291 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4603 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3837 - accuracy: 0.8475 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4039 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8463 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3256 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8492 - val_loss: 0.4289 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3212 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8429 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3974 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8452 - val_loss: 0.4286 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3198 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.8560 - val_loss: 0.4284 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3929 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8520 - val_loss: 0.4285 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3977 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3864 - accuracy: 0.8435 - val_loss: 0.4284 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3841 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8543 - val_loss: 0.4283 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3894 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8480 - val_loss: 0.4282 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3590 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3793 - accuracy: 0.8497 - val_loss: 0.4282 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4217 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8560 - val_loss: 0.4282 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4268 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8526 - val_loss: 0.4281 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3901 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.8532 - val_loss: 0.4281 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3877 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8480 - val_loss: 0.4281 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5400 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8492 - val_loss: 0.4281 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4725 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8492 - val_loss: 0.4280 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3970 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8503 - val_loss: 0.4278 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3744 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8532 - val_loss: 0.4277 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3985 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8475 - val_loss: 0.4276 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3952 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8520 - val_loss: 0.4277 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4009 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8503 - val_loss: 0.4275 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4291 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8515 - val_loss: 0.4273 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4538 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3796 - accuracy: 0.8469 - val_loss: 0.4272 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3014 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8526 - val_loss: 0.4272 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3353 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8560 - val_loss: 0.4272 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3760 - accuracy: 0.8549 - val_loss: 0.4269 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3608 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3739 - accuracy: 0.8554 - val_loss: 0.4268 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3455 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8532 - val_loss: 0.4267 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.2802 - accuracy: 0.9062\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8554 - val_loss: 0.4266 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3696 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8526 - val_loss: 0.4265 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3961 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8509 - val_loss: 0.4265 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3610 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8549 - val_loss: 0.4264 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3434 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3791 - accuracy: 0.8537 - val_loss: 0.4265 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3634 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8532 - val_loss: 0.4264 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3805 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3698 - accuracy: 0.8503 - val_loss: 0.4264 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3802 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3723 - accuracy: 0.8554 - val_loss: 0.4263 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3175 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8497 - val_loss: 0.4263 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3424 - accuracy: 0.8984\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8560 - val_loss: 0.4263 - val_accuracy: 0.8112\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 2.0, 'hidden_units': 224.0, 'input_dropout': 0.05, 'optimizer': {'lr': 1.5104340473265163e-05, 'type': 'adam'}}, logloss: 0.4263\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 10%|█          | 1/10 [00:10<01:38, 10.91s/trial, best loss: 0.426301786262656]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 4s - loss: 0.6904 - accuracy: 0.5234\n",
      "14/14 [==============================] - 1s 11ms/step - loss: 0.6661 - accuracy: 0.6249 - val_loss: 0.6534 - val_accuracy: 0.7041\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6376 - accuracy: 0.7344\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.7445 - val_loss: 0.6240 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6152 - accuracy: 0.7266\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5935 - accuracy: 0.7940 - val_loss: 0.6005 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5877 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5661 - accuracy: 0.8105 - val_loss: 0.5829 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5673 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5491 - accuracy: 0.8082 - val_loss: 0.5698 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5662 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5313 - accuracy: 0.8088 - val_loss: 0.5603 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4987 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5181 - accuracy: 0.8082 - val_loss: 0.5530 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5211 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.8082 - val_loss: 0.5472 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4984 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.8082 - val_loss: 0.5428 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4923 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.8082 - val_loss: 0.5389 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5211 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4945 - accuracy: 0.8082 - val_loss: 0.5358 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4874 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4945 - accuracy: 0.8082 - val_loss: 0.5330 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5429 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.8082 - val_loss: 0.5305 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5792 - accuracy: 0.7188\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.8082 - val_loss: 0.5281 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.8082 - val_loss: 0.5259 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4905 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.8082 - val_loss: 0.5239 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4814 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4768 - accuracy: 0.8082 - val_loss: 0.5219 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4970 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - accuracy: 0.8082 - val_loss: 0.5201 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5188 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4741 - accuracy: 0.8082 - val_loss: 0.5183 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4780 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - accuracy: 0.8082 - val_loss: 0.5164 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5527 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.8082 - val_loss: 0.5147 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4563 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4702 - accuracy: 0.8082 - val_loss: 0.5130 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5071 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4726 - accuracy: 0.8082 - val_loss: 0.5113 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4358 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4672 - accuracy: 0.8088 - val_loss: 0.5097 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4460 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.8082 - val_loss: 0.5080 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5916 - accuracy: 0.6953\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.8082 - val_loss: 0.5064 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4894 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.8076 - val_loss: 0.5048 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4218 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4655 - accuracy: 0.8082 - val_loss: 0.5033 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5221 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4668 - accuracy: 0.8082 - val_loss: 0.5017 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3862 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.8088 - val_loss: 0.5002 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4526 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.8082 - val_loss: 0.4988 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4888 - accuracy: 0.7500\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.8082 - val_loss: 0.4973 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8088 - val_loss: 0.4960 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4267 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.8093 - val_loss: 0.4946 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4790 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4516 - accuracy: 0.8082 - val_loss: 0.4932 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5487 - accuracy: 0.7266\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8088 - val_loss: 0.4919 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5040 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8105 - val_loss: 0.4906 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4213 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8088 - val_loss: 0.4893 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4567 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4476 - accuracy: 0.8088 - val_loss: 0.4880 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4908 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4510 - accuracy: 0.8105 - val_loss: 0.4868 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4876 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.8116 - val_loss: 0.4856 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4793 - accuracy: 0.7578\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.8093 - val_loss: 0.4844 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3654 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4426 - accuracy: 0.8122 - val_loss: 0.4834 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4612 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.8099 - val_loss: 0.4822 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5038 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4468 - accuracy: 0.8099 - val_loss: 0.4810 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4135 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8127 - val_loss: 0.4800 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3946 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4461 - accuracy: 0.8133 - val_loss: 0.4790 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4214 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8099 - val_loss: 0.4780 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4269 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8122 - val_loss: 0.4769 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4872 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.8122 - val_loss: 0.4759 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4167 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.8173 - val_loss: 0.4749 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5336 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8110 - val_loss: 0.4740 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4919 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4379 - accuracy: 0.8156 - val_loss: 0.4730 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4412 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8150 - val_loss: 0.4720 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4197 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4394 - accuracy: 0.8179 - val_loss: 0.4710 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4452 - accuracy: 0.8359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.8162 - val_loss: 0.4701 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4544 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.8162 - val_loss: 0.4693 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4491 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4303 - accuracy: 0.8184 - val_loss: 0.4684 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4422 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4353 - accuracy: 0.8184 - val_loss: 0.4676 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4247 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.8167 - val_loss: 0.4667 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3693 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8179 - val_loss: 0.4658 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8190 - val_loss: 0.4649 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4558 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4301 - accuracy: 0.8167 - val_loss: 0.4642 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5046 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.8207 - val_loss: 0.4634 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4191 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8213 - val_loss: 0.4625 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4976 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4302 - accuracy: 0.8201 - val_loss: 0.4616 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3725 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8236 - val_loss: 0.4609 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3524 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4260 - accuracy: 0.8230 - val_loss: 0.4602 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3145 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4294 - accuracy: 0.8196 - val_loss: 0.4595 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4307 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8219 - val_loss: 0.4588 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4228 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4238 - accuracy: 0.8264 - val_loss: 0.4581 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4679 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.8207 - val_loss: 0.4574 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4064 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8230 - val_loss: 0.4568 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3879 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4273 - accuracy: 0.8196 - val_loss: 0.4561 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4277 - accuracy: 0.8258 - val_loss: 0.4555 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4321 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4261 - accuracy: 0.8241 - val_loss: 0.4549 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4081 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4224 - accuracy: 0.8236 - val_loss: 0.4542 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3888 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4196 - accuracy: 0.8247 - val_loss: 0.4535 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3829 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4233 - accuracy: 0.8264 - val_loss: 0.4528 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4328 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4271 - accuracy: 0.8213 - val_loss: 0.4523 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4470 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8287 - val_loss: 0.4517 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4194 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4202 - accuracy: 0.8270 - val_loss: 0.4511 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4601 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8264 - val_loss: 0.4505 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3551 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4202 - accuracy: 0.8264 - val_loss: 0.4499 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4021 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8196 - val_loss: 0.4493 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3412 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8275 - val_loss: 0.4488 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3744 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8264 - val_loss: 0.4482 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4599 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8287 - val_loss: 0.4477 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3767 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4135 - accuracy: 0.8304 - val_loss: 0.4472 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4023 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4165 - accuracy: 0.8315 - val_loss: 0.4466 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4431 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4167 - accuracy: 0.8304 - val_loss: 0.4461 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4385 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4157 - accuracy: 0.8230 - val_loss: 0.4457 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3803 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4222 - accuracy: 0.8287 - val_loss: 0.4453 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4504 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4155 - accuracy: 0.8258 - val_loss: 0.4448 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4434 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.8270 - val_loss: 0.4444 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4471 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4121 - accuracy: 0.8338 - val_loss: 0.4439 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3778 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.8293 - val_loss: 0.4435 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4942 - accuracy: 0.7578\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4168 - accuracy: 0.8321 - val_loss: 0.4430 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3668 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4139 - accuracy: 0.8275 - val_loss: 0.4426 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4682 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4158 - accuracy: 0.8293 - val_loss: 0.4421 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4293 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4158 - accuracy: 0.8281 - val_loss: 0.4418 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4340 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4182 - accuracy: 0.8287 - val_loss: 0.4414 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4053 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.8344 - val_loss: 0.4410 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4023 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4110 - accuracy: 0.8315 - val_loss: 0.4406 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3756 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4140 - accuracy: 0.8349 - val_loss: 0.4403 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3824 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8293 - val_loss: 0.4398 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4273 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4126 - accuracy: 0.8315 - val_loss: 0.4395 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4153 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8298 - val_loss: 0.4391 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4070 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8298 - val_loss: 0.4388 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8310 - val_loss: 0.4384 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4037 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8389 - val_loss: 0.4381 - val_accuracy: 0.8112\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4287 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4100 - accuracy: 0.8361 - val_loss: 0.4377 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3312 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4133 - accuracy: 0.8355 - val_loss: 0.4374 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3667 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4142 - accuracy: 0.8298 - val_loss: 0.4371 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4157 - accuracy: 0.8304 - val_loss: 0.4368 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4444 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8361 - val_loss: 0.4365 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5347 - accuracy: 0.7656\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4168 - accuracy: 0.8281 - val_loss: 0.4362 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4121 - accuracy: 0.8293 - val_loss: 0.4359 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4296 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4112 - accuracy: 0.8315 - val_loss: 0.4357 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3538 - accuracy: 0.9062\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4064 - accuracy: 0.8412 - val_loss: 0.4354 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4807 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4087 - accuracy: 0.8344 - val_loss: 0.4352 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3872 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4171 - accuracy: 0.8338 - val_loss: 0.4350 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3465 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4085 - accuracy: 0.8281 - val_loss: 0.4347 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4322 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4021 - accuracy: 0.8372 - val_loss: 0.4344 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4261 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4115 - accuracy: 0.8315 - val_loss: 0.4342 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4791 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4149 - accuracy: 0.8332 - val_loss: 0.4339 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4140 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8389 - val_loss: 0.4337 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3254 - accuracy: 0.8906\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4062 - accuracy: 0.8344 - val_loss: 0.4335 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4312 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4112 - accuracy: 0.8310 - val_loss: 0.4332 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3936 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4046 - accuracy: 0.8349 - val_loss: 0.4330 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3593 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4033 - accuracy: 0.8361 - val_loss: 0.4327 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3832 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 0.8355 - val_loss: 0.4324 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3493 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8355 - val_loss: 0.4322 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4738 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.8355 - val_loss: 0.4320 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3343 - accuracy: 0.8984\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4146 - accuracy: 0.8349 - val_loss: 0.4318 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3958 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8321 - val_loss: 0.4317 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4077 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8372 - val_loss: 0.4314 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4105 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4168 - accuracy: 0.8230 - val_loss: 0.4312 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3959 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4033 - accuracy: 0.8327 - val_loss: 0.4311 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.7734\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.8327 - val_loss: 0.4309 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3308 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4019 - accuracy: 0.8372 - val_loss: 0.4307 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5302 - accuracy: 0.7188\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4067 - accuracy: 0.8321 - val_loss: 0.4304 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3976 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4044 - accuracy: 0.8327 - val_loss: 0.4303 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3534 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8378 - val_loss: 0.4300 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3608 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8310 - val_loss: 0.4299 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3391 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8389 - val_loss: 0.4297 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4601 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 0.8384 - val_loss: 0.4295 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4124 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4062 - accuracy: 0.8389 - val_loss: 0.4293 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3561 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8378 - val_loss: 0.4292 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3937 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8372 - val_loss: 0.4290 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3155 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8406 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3529 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4038 - accuracy: 0.8418 - val_loss: 0.4288 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4426 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8361 - val_loss: 0.4286 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3842 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3995 - accuracy: 0.8395 - val_loss: 0.4284 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3878 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8321 - val_loss: 0.4282 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4240 - accuracy: 0.8047\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4025 - accuracy: 0.8361 - val_loss: 0.4279 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3846 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4078 - accuracy: 0.8287 - val_loss: 0.4278 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3595 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3988 - accuracy: 0.8412 - val_loss: 0.4277 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4257 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4053 - accuracy: 0.8338 - val_loss: 0.4275 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3922 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8384 - val_loss: 0.4274 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4108 - accuracy: 0.8338 - val_loss: 0.4272 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4229 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8435 - val_loss: 0.4270 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4009 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4102 - accuracy: 0.8315 - val_loss: 0.4268 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3223 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4041 - accuracy: 0.8332 - val_loss: 0.4267 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3450 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8332 - val_loss: 0.4266 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4162 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8406 - val_loss: 0.4264 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4097 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4073 - accuracy: 0.8321 - val_loss: 0.4263 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4508 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4067 - accuracy: 0.8332 - val_loss: 0.4262 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4133 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4058 - accuracy: 0.8378 - val_loss: 0.4260 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4553 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8349 - val_loss: 0.4259 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3301 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8355 - val_loss: 0.4258 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3613 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3986 - accuracy: 0.8338 - val_loss: 0.4257 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4119 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4047 - accuracy: 0.8349 - val_loss: 0.4255 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4655 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8412 - val_loss: 0.4253 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3757 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8429 - val_loss: 0.4252 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3634 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8355 - val_loss: 0.4250 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3844 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8441 - val_loss: 0.4248 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4606 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3982 - accuracy: 0.8423 - val_loss: 0.4248 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3542 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8310 - val_loss: 0.4246 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4567 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8338 - val_loss: 0.4245 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4397 - accuracy: 0.7969\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8446 - val_loss: 0.4244 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3038 - accuracy: 0.9375\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.8355 - val_loss: 0.4243 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4387 - accuracy: 0.7812\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4013 - accuracy: 0.8367 - val_loss: 0.4242 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3982 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8423 - val_loss: 0.4241 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3656 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4098 - accuracy: 0.8361 - val_loss: 0.4241 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3378 - accuracy: 0.8750\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3950 - accuracy: 0.8401 - val_loss: 0.4239 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4253 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8338 - val_loss: 0.4238 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4070 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4038 - accuracy: 0.8395 - val_loss: 0.4237 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3750 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.8344 - val_loss: 0.4236 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4500 - accuracy: 0.7891\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3995 - accuracy: 0.8389 - val_loss: 0.4236 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3933 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8378 - val_loss: 0.4236 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3660 - accuracy: 0.8594\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4030 - accuracy: 0.8270 - val_loss: 0.4235 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3662 - accuracy: 0.8672\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8395 - val_loss: 0.4233 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3581 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3914 - accuracy: 0.8361 - val_loss: 0.4232 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4320 - accuracy: 0.8203\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8452 - val_loss: 0.4230 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4300 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8321 - val_loss: 0.4230 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4333 - accuracy: 0.8125\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8361 - val_loss: 0.4229 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4417 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8389 - val_loss: 0.4228 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3494 - accuracy: 0.8828\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8446 - val_loss: 0.4228 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4435 - accuracy: 0.8281\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8446 - val_loss: 0.4226 - val_accuracy: 0.8214\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 2.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00044169778741522055, 'type': 'sgd'}}, logloss: 0.4226\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 20%|██        | 2/10 [00:22<01:29, 11.17s/trial, best loss: 0.4225908487807123]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/19 [>.............................] - ETA: 16s - loss: 1.2566 - accuracy: 0.2083\n",
      "19/19 [==============================] - 1s 11ms/step - loss: 1.1542 - accuracy: 0.2891 - val_loss: 0.7290 - val_accuracy: 0.3520\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 1.1833 - accuracy: 0.2292\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.1108 - accuracy: 0.2965 - val_loss: 0.7468 - val_accuracy: 0.2704\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 1.0801 - accuracy: 0.2812\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0659 - accuracy: 0.3182 - val_loss: 0.7566 - val_accuracy: 0.2857\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 1.0890 - accuracy: 0.2917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0630 - accuracy: 0.3187 - val_loss: 0.7612 - val_accuracy: 0.2857\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 1.0794 - accuracy: 0.3333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.0506 - accuracy: 0.3210 - val_loss: 0.7634 - val_accuracy: 0.3010\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 1.0117 - accuracy: 0.2812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.0167 - accuracy: 0.3358 - val_loss: 0.7648 - val_accuracy: 0.3061\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9885 - accuracy: 0.3229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 1.0162 - accuracy: 0.3307 - val_loss: 0.7637 - val_accuracy: 0.3214\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9926 - accuracy: 0.3542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.9800 - accuracy: 0.3688 - val_loss: 0.7604 - val_accuracy: 0.3316\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9825 - accuracy: 0.3958\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.9776 - accuracy: 0.3768 - val_loss: 0.7555 - val_accuracy: 0.3469\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9562 - accuracy: 0.3854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.9510 - accuracy: 0.3722 - val_loss: 0.7519 - val_accuracy: 0.3571\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9483 - accuracy: 0.3021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.9489 - accuracy: 0.3819 - val_loss: 0.7477 - val_accuracy: 0.3776\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9411 - accuracy: 0.4271\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.9433 - accuracy: 0.3847 - val_loss: 0.7415 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9598 - accuracy: 0.3750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.8930 - accuracy: 0.4200 - val_loss: 0.7343 - val_accuracy: 0.4337\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9201 - accuracy: 0.4271\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.8792 - accuracy: 0.4381\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8787 - accuracy: 0.4377 - val_loss: 0.7250 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.8386 - accuracy: 0.4583\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8681 - accuracy: 0.4263 - val_loss: 0.7196 - val_accuracy: 0.4592\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.9057 - accuracy: 0.4479\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8461 - accuracy: 0.4599 - val_loss: 0.7138 - val_accuracy: 0.4796\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.8323 - accuracy: 0.5312\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.8413 - accuracy: 0.4570 - val_loss: 0.7068 - val_accuracy: 0.4949\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.8441 - accuracy: 0.4583\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.8268 - accuracy: 0.4906 - val_loss: 0.6975 - val_accuracy: 0.5153\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7932 - accuracy: 0.5417\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7942 - accuracy: 0.5065 - val_loss: 0.6887 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7823 - accuracy: 0.5104\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.8024 - accuracy: 0.5139 - val_loss: 0.6795 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.8664 - accuracy: 0.4375\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7931 - accuracy: 0.5054 - val_loss: 0.6702 - val_accuracy: 0.5765\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7985 - accuracy: 0.5208\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7786 - accuracy: 0.5208 - val_loss: 0.6622 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7738 - accuracy: 0.5312\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.7543 - accuracy: 0.5336\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7536 - accuracy: 0.5356 - val_loss: 0.6549 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7427 - accuracy: 0.5104\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7602 - accuracy: 0.5213 - val_loss: 0.6483 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7733 - accuracy: 0.5208\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7542 - accuracy: 0.5464 - val_loss: 0.6375 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7666 - accuracy: 0.5521\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.7471 - accuracy: 0.5561 - val_loss: 0.6318 - val_accuracy: 0.6531\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7060 - accuracy: 0.5938\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7167 - accuracy: 0.5754 - val_loss: 0.6251 - val_accuracy: 0.6582\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6953 - accuracy: 0.5833\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7117 - accuracy: 0.5931\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7117 - accuracy: 0.5931 - val_loss: 0.6193 - val_accuracy: 0.6633\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7640 - accuracy: 0.5000\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7086 - accuracy: 0.5896\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.7086 - accuracy: 0.5896 - val_loss: 0.6123 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7159 - accuracy: 0.5938\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.6039 - val_loss: 0.6057 - val_accuracy: 0.6939\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6841 - accuracy: 0.6146\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5828 - val_loss: 0.5991 - val_accuracy: 0.7092\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6773 - accuracy: 0.6250\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.6084 - val_loss: 0.5958 - val_accuracy: 0.7194\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6528 - accuracy: 0.6667\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.6295 - val_loss: 0.5912 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6421 - accuracy: 0.6771\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.6414\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6414 - val_loss: 0.5865 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7260 - accuracy: 0.5833\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.6170 - val_loss: 0.5815 - val_accuracy: 0.7398\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6265 - accuracy: 0.6562\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6543 - accuracy: 0.6483 - val_loss: 0.5760 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5867 - accuracy: 0.7188\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6304 - accuracy: 0.6638\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6310 - accuracy: 0.6642 - val_loss: 0.5715 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6231 - accuracy: 0.6875\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.6375 - accuracy: 0.6620\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6402 - accuracy: 0.6602 - val_loss: 0.5677 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7284 - accuracy: 0.6354\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6391 - accuracy: 0.6631 - val_loss: 0.5611 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.7110 - accuracy: 0.5521\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6394 - accuracy: 0.6443 - val_loss: 0.5564 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5597 - accuracy: 0.7292\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6054 - accuracy: 0.6915 - val_loss: 0.5531 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5739 - accuracy: 0.7500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5976 - accuracy: 0.6892 - val_loss: 0.5499 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6384 - accuracy: 0.6771\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6131 - accuracy: 0.6847 - val_loss: 0.5465 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6251 - accuracy: 0.6875\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7137\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5850 - accuracy: 0.7137 - val_loss: 0.5426 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6513 - accuracy: 0.6562\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6027 - accuracy: 0.6887 - val_loss: 0.5388 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5347 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5905 - accuracy: 0.7183 - val_loss: 0.5348 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6016 - accuracy: 0.6562\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.6938\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.6938 - val_loss: 0.5313 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5619 - accuracy: 0.7292\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5757 - accuracy: 0.7240 - val_loss: 0.5277 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5673 - accuracy: 0.7500\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5687 - accuracy: 0.7326\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5673 - accuracy: 0.7342 - val_loss: 0.5244 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5561 - accuracy: 0.7396\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5631 - accuracy: 0.7302 - val_loss: 0.5208 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5534 - accuracy: 0.7083\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5574 - accuracy: 0.7308 - val_loss: 0.5190 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5237 - accuracy: 0.7812\n",
      "16/19 [========================>.....] - ETA: 0s - loss: 0.5583 - accuracy: 0.7279\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5583 - accuracy: 0.7291 - val_loss: 0.5162 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5290 - accuracy: 0.7708\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5603 - accuracy: 0.7199\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5610 - accuracy: 0.7177 - val_loss: 0.5142 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4991 - accuracy: 0.7812\n",
      "16/19 [========================>.....] - ETA: 0s - loss: 0.5541 - accuracy: 0.7305\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.7285 - val_loss: 0.5094 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5420 - accuracy: 0.7708\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.5420 - accuracy: 0.7555\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5413 - accuracy: 0.7564 - val_loss: 0.5060 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5793 - accuracy: 0.6979\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7416\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5532 - accuracy: 0.7416 - val_loss: 0.5032 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5299 - accuracy: 0.7604\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5521 - accuracy: 0.7350\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5533 - accuracy: 0.7348 - val_loss: 0.5010 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6224 - accuracy: 0.6562\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5370 - accuracy: 0.7465\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.7450 - val_loss: 0.4995 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5362 - accuracy: 0.7708\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5368 - accuracy: 0.7512\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5379 - accuracy: 0.7496 - val_loss: 0.4968 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5338 - accuracy: 0.7396\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5287 - accuracy: 0.7477\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5288 - accuracy: 0.7479 - val_loss: 0.4937 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4733 - accuracy: 0.7812\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.5357 - accuracy: 0.7506\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5361 - accuracy: 0.7524 - val_loss: 0.4916 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.7917\n",
      "16/19 [========================>.....] - ETA: 0s - loss: 0.5172 - accuracy: 0.7682\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5234 - accuracy: 0.7666 - val_loss: 0.4890 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4564 - accuracy: 0.8542\n",
      "14/19 [=====================>........] - ETA: 0s - loss: 0.5245 - accuracy: 0.7582\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.5190 - accuracy: 0.7621 - val_loss: 0.4868 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5594 - accuracy: 0.7500\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5265 - accuracy: 0.7703\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5272 - accuracy: 0.7712 - val_loss: 0.4843 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5204 - accuracy: 0.7917\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5124 - accuracy: 0.7685\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5116 - accuracy: 0.7701 - val_loss: 0.4823 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4719 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5197 - accuracy: 0.7684 - val_loss: 0.4803 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5283 - accuracy: 0.7292\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7684 - val_loss: 0.4789 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5053 - accuracy: 0.7917\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5119 - accuracy: 0.7723\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5119 - accuracy: 0.7723 - val_loss: 0.4776 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4950 - accuracy: 0.7396\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.5047 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5056 - accuracy: 0.7809 - val_loss: 0.4755 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4463 - accuracy: 0.8125\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4931 - accuracy: 0.7801\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4923 - accuracy: 0.7809 - val_loss: 0.4736 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4536 - accuracy: 0.8229\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4975 - accuracy: 0.7826\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7826 - val_loss: 0.4728 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5721 - accuracy: 0.7292\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5003 - accuracy: 0.7775\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5003 - accuracy: 0.7775 - val_loss: 0.4717 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4764 - accuracy: 0.7604\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.7814\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4977 - accuracy: 0.7814 - val_loss: 0.4698 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5471 - accuracy: 0.7396\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.7837\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4933 - accuracy: 0.7837 - val_loss: 0.4682 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4216 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.7758\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5080 - accuracy: 0.7758 - val_loss: 0.4658 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5580 - accuracy: 0.7500\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4831 - accuracy: 0.7870\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4810 - accuracy: 0.7894 - val_loss: 0.4648 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3870 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4884 - accuracy: 0.7940 - val_loss: 0.4635 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5435 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4759 - accuracy: 0.7911 - val_loss: 0.4623 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4090 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4805 - accuracy: 0.7980 - val_loss: 0.4605 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4564 - accuracy: 0.8542\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4890 - accuracy: 0.7830\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4900 - accuracy: 0.7832 - val_loss: 0.4595 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4440 - accuracy: 0.8229\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4741 - accuracy: 0.8032\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4734 - accuracy: 0.8036 - val_loss: 0.4579 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4198 - accuracy: 0.8542\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4823 - accuracy: 0.7865\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4825 - accuracy: 0.7871 - val_loss: 0.4569 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5179 - accuracy: 0.7188\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4705 - accuracy: 0.7923 - val_loss: 0.4552 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3870 - accuracy: 0.8542\n",
      "14/19 [=====================>........] - ETA: 0s - loss: 0.4632 - accuracy: 0.8036\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4680 - accuracy: 0.8036 - val_loss: 0.4541 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4374 - accuracy: 0.8229\n",
      "15/19 [======================>.......] - ETA: 0s - loss: 0.4622 - accuracy: 0.8132\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4761 - accuracy: 0.8036 - val_loss: 0.4529 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4188 - accuracy: 0.8229\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.8065\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4684 - accuracy: 0.8065 - val_loss: 0.4519 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4513 - accuracy: 0.7396\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4630 - accuracy: 0.7951\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4630 - accuracy: 0.7951 - val_loss: 0.4510 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5227 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4793 - accuracy: 0.7923 - val_loss: 0.4504 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4344 - accuracy: 0.8229\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4728 - accuracy: 0.7923\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4728 - accuracy: 0.7923 - val_loss: 0.4493 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5116 - accuracy: 0.7604\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4714 - accuracy: 0.7963\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4711 - accuracy: 0.7974 - val_loss: 0.4479 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3808 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4645 - accuracy: 0.8133\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4645 - accuracy: 0.8133 - val_loss: 0.4476 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4505 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4772 - accuracy: 0.7997\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4772 - accuracy: 0.7997 - val_loss: 0.4468 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4941 - accuracy: 0.8125\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4712 - accuracy: 0.8015\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4713 - accuracy: 0.8014 - val_loss: 0.4462 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5551 - accuracy: 0.7188\n",
      "14/19 [=====================>........] - ETA: 0s - loss: 0.4847 - accuracy: 0.7820\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.4791 - accuracy: 0.7894 - val_loss: 0.4456 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8646\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4485 - accuracy: 0.8108\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.8082 - val_loss: 0.4447 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4278 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.8116 - val_loss: 0.4443 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4348 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7997 - val_loss: 0.4435 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5592 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4648 - accuracy: 0.8122 - val_loss: 0.4427 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4872 - accuracy: 0.8021\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.8127\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.8127 - val_loss: 0.4421 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4530 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4662 - accuracy: 0.7980 - val_loss: 0.4411 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3987 - accuracy: 0.8333\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.8065\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.8065 - val_loss: 0.4404 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4026 - accuracy: 0.8646\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4588 - accuracy: 0.8154\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4592 - accuracy: 0.8145 - val_loss: 0.4397 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4844 - accuracy: 0.8021\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.8019\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4644 - accuracy: 0.8019 - val_loss: 0.4390 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3927 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.8105 - val_loss: 0.4386 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5795 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4530 - accuracy: 0.8048 - val_loss: 0.4381 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3805 - accuracy: 0.8646\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4601 - accuracy: 0.8073\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.8093 - val_loss: 0.4375 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4807 - accuracy: 0.7812\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.4466 - accuracy: 0.8168\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.8156 - val_loss: 0.4367 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5040 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4427 - accuracy: 0.8190 - val_loss: 0.4363 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4234 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4412 - accuracy: 0.8236 - val_loss: 0.4354 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3863 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.8048 - val_loss: 0.4347 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3871 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.8116 - val_loss: 0.4345 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3705 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.8127 - val_loss: 0.4339 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4442 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4437 - accuracy: 0.8213 - val_loss: 0.4333 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4774 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4576 - accuracy: 0.8127 - val_loss: 0.4327 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6025 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4675 - accuracy: 0.8088 - val_loss: 0.4326 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5519 - accuracy: 0.7500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.8088 - val_loss: 0.4323 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3538 - accuracy: 0.8542\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4435 - accuracy: 0.8093\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4435 - accuracy: 0.8093 - val_loss: 0.4320 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4789 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.8002 - val_loss: 0.4314 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4127 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.8042 - val_loss: 0.4308 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4212 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.8036 - val_loss: 0.4305 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4125 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4472 - accuracy: 0.8230 - val_loss: 0.4301 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4457 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.8253 - val_loss: 0.4302 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4439 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.8219 - val_loss: 0.4297 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3410 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.8207 - val_loss: 0.4294 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4749 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.8150 - val_loss: 0.4290 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4645 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8122 - val_loss: 0.4288 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3899 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4400 - accuracy: 0.8219 - val_loss: 0.4285 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3764 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.8139 - val_loss: 0.4285 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4547 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4483 - accuracy: 0.8099 - val_loss: 0.4280 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3849 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.8224 - val_loss: 0.4280 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3971 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4201 - accuracy: 0.8315 - val_loss: 0.4277 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4637 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4264 - accuracy: 0.8201 - val_loss: 0.4272 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5275 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.8247 - val_loss: 0.4271 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4984 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4490 - accuracy: 0.8139 - val_loss: 0.4267 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4113 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4240 - accuracy: 0.8241 - val_loss: 0.4267 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2848 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.8281 - val_loss: 0.4261 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4570 - accuracy: 0.8099 - val_loss: 0.4258 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3439 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.8116 - val_loss: 0.4256 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5208 - accuracy: 0.7500\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4487 - accuracy: 0.8150 - val_loss: 0.4255 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3813 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8207 - val_loss: 0.4248 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4546 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 0.8190 - val_loss: 0.4248 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5437 - accuracy: 0.7500\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8162 - val_loss: 0.4247 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5466 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.8179 - val_loss: 0.4244 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3444 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.8184 - val_loss: 0.4242 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4424 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4341 - accuracy: 0.8275 - val_loss: 0.4241 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4460 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8099 - val_loss: 0.4240 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4443 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4388 - accuracy: 0.8247 - val_loss: 0.4239 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4235 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4425 - accuracy: 0.8122 - val_loss: 0.4239 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3585 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4322 - accuracy: 0.8156 - val_loss: 0.4239 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3770 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.8167 - val_loss: 0.4234 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4444 - accuracy: 0.8229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4283 - accuracy: 0.8184 - val_loss: 0.4230 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3786 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4370 - accuracy: 0.8179 - val_loss: 0.4226 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4304 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4460 - accuracy: 0.8133 - val_loss: 0.4228 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5475 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.8184 - val_loss: 0.4228 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4447 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.8253 - val_loss: 0.4226 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3917 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8133 - val_loss: 0.4222 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3704 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4314 - accuracy: 0.8201 - val_loss: 0.4220 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5389 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.8122 - val_loss: 0.4220 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3874 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4327 - accuracy: 0.8258 - val_loss: 0.4218 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4042 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4291 - accuracy: 0.8150 - val_loss: 0.4213 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4655 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4285 - accuracy: 0.8184 - val_loss: 0.4213 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5724 - accuracy: 0.7500\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4247 - accuracy: 0.8258 - val_loss: 0.4213 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4326 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8190 - val_loss: 0.4211 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4700 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4387 - accuracy: 0.8105 - val_loss: 0.4210 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3441 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.8298 - val_loss: 0.4209 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3601 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4321 - accuracy: 0.8190 - val_loss: 0.4205 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3799 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.8281 - val_loss: 0.4202 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4133 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8253 - val_loss: 0.4200 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5013 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4379 - accuracy: 0.8133 - val_loss: 0.4199 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4220 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8219 - val_loss: 0.4201 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3922 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8219 - val_loss: 0.4198 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3213 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8247 - val_loss: 0.4199 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4668 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8122 - val_loss: 0.4199 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3707 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.8270 - val_loss: 0.4200 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4181 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.8224 - val_loss: 0.4199 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3276 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4235 - accuracy: 0.8224 - val_loss: 0.4198 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2792 - accuracy: 0.9271\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.8298\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.8298 - val_loss: 0.4197 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3840 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4328 - accuracy: 0.8241 - val_loss: 0.4194 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5225 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.8304 - val_loss: 0.4193 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4224 - accuracy: 0.8125\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8236\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4296 - accuracy: 0.8236 - val_loss: 0.4192 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3323 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4227 - accuracy: 0.8315 - val_loss: 0.4193 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5159 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4294 - accuracy: 0.8275 - val_loss: 0.4190 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4298 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8275 - val_loss: 0.4186 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4694 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4165 - accuracy: 0.8281 - val_loss: 0.4184 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4915 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4208 - accuracy: 0.8281 - val_loss: 0.4181 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4939 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8167 - val_loss: 0.4180 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4078 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4346 - accuracy: 0.8219 - val_loss: 0.4178 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4132 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4250 - accuracy: 0.8258 - val_loss: 0.4180 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4255 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.8321 - val_loss: 0.4179 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5413 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4271 - accuracy: 0.8213 - val_loss: 0.4177 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3812 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8207 - val_loss: 0.4178 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3408 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4170 - accuracy: 0.8321\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4170 - accuracy: 0.8321 - val_loss: 0.4177 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5058 - accuracy: 0.7917\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.8241\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.8241 - val_loss: 0.4174 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3478 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4122 - accuracy: 0.8389 - val_loss: 0.4172 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4030 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4260 - accuracy: 0.8270 - val_loss: 0.4169 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3001 - accuracy: 0.8854\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4169 - accuracy: 0.8281\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.8281 - val_loss: 0.4171 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3438 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4082 - accuracy: 0.8361 - val_loss: 0.4170 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3585 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4149 - accuracy: 0.8310 - val_loss: 0.4172 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4018 - accuracy: 0.8125\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.8321\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.8321 - val_loss: 0.4170 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3780 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4133 - accuracy: 0.8304 - val_loss: 0.4171 - val_accuracy: 0.8316\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 96.0, 'input_dropout': 0.2, 'optimizer': {'lr': 3.398601870720286e-05, 'type': 'adam'}}, logloss: 0.4171\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 30%|███▎       | 3/10 [00:39<01:36, 13.81s/trial, best loss: 0.417095622865065]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 17s - loss: 0.7250 - accuracy: 0.3906\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6992 - accuracy: 0.5000 \n",
      "28/28 [==============================] - 1s 8ms/step - loss: 0.6996 - accuracy: 0.4991 - val_loss: 0.6986 - val_accuracy: 0.4745\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6939 - accuracy: 0.5625\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.6905 - accuracy: 0.5445\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5447 - val_loss: 0.6907 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6950 - accuracy: 0.5000\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.6867 - accuracy: 0.5431\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6851 - accuracy: 0.5458 - val_loss: 0.6830 - val_accuracy: 0.6122\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6739 - accuracy: 0.6250\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6739 - accuracy: 0.6135\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6739 - accuracy: 0.6135 - val_loss: 0.6758 - val_accuracy: 0.6582\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6830 - accuracy: 0.5625\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6671 - accuracy: 0.6352 - val_loss: 0.6689 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6607 - accuracy: 0.6406\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6653 - val_loss: 0.6624 - val_accuracy: 0.7398\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6557 - accuracy: 0.6875\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6491 - accuracy: 0.6972 - val_loss: 0.6563 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6409 - accuracy: 0.6875\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.7211 - val_loss: 0.6504 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6510 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6346 - accuracy: 0.7353 - val_loss: 0.6448 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6208 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6284 - accuracy: 0.7467 - val_loss: 0.6395 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6107 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.7832 - val_loss: 0.6346 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6225 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6152 - accuracy: 0.7854 - val_loss: 0.6298 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6128 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7951 - val_loss: 0.6253 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6423 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.7883 - val_loss: 0.6212 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6072 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6004 - accuracy: 0.7991\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6004 - accuracy: 0.7991 - val_loss: 0.6172 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6192 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.7962 - val_loss: 0.6133 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5993 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5902 - accuracy: 0.8008 - val_loss: 0.6097 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5727 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.8025 - val_loss: 0.6063 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6190 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.8019 - val_loss: 0.6030 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5442 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5781 - accuracy: 0.8059 - val_loss: 0.5998 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5859 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.8059 - val_loss: 0.5969 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6171 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.8065 - val_loss: 0.5941 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5474 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5666 - accuracy: 0.8059 - val_loss: 0.5914 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6202 - accuracy: 0.7500\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5699 - accuracy: 0.8081\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5695 - accuracy: 0.8088 - val_loss: 0.5890 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5744 - accuracy: 0.7656\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5639 - accuracy: 0.8053\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.8065 - val_loss: 0.5866 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5763 - accuracy: 0.8125\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5587 - accuracy: 0.8082 - val_loss: 0.5842 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6375 - accuracy: 0.6875\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.8082 - val_loss: 0.5822 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5488 - accuracy: 0.8594\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5509 - accuracy: 0.8082 - val_loss: 0.5800 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5699 - accuracy: 0.7656\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5499 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5498 - accuracy: 0.8082 - val_loss: 0.5781 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5147 - accuracy: 0.8281\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5460 - accuracy: 0.8145\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5507 - accuracy: 0.8076 - val_loss: 0.5762 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5417 - accuracy: 0.7969\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5459 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5454 - accuracy: 0.8076 - val_loss: 0.5745 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5217 - accuracy: 0.8594\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.8082 - val_loss: 0.5728 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5594 - accuracy: 0.7812\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5383 - accuracy: 0.8101\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.8082 - val_loss: 0.5712 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5220 - accuracy: 0.8438\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5485 - accuracy: 0.8006\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5421 - accuracy: 0.8082 - val_loss: 0.5697 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5831 - accuracy: 0.7812\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5361 - accuracy: 0.8082 - val_loss: 0.5682 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4953 - accuracy: 0.8125\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.8076 - val_loss: 0.5668 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5890 - accuracy: 0.7500\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.8082 - val_loss: 0.5655 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5807 - accuracy: 0.7656\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5321 - accuracy: 0.8118\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.8082 - val_loss: 0.5642 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5330 - accuracy: 0.8125\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5331 - accuracy: 0.8105\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5357 - accuracy: 0.8082 - val_loss: 0.5630 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5338 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5326 - accuracy: 0.8061\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5311 - accuracy: 0.8082 - val_loss: 0.5620 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5536 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5339 - accuracy: 0.8082 - val_loss: 0.5609 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5089 - accuracy: 0.8125\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5345 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5322 - accuracy: 0.8082 - val_loss: 0.5599 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5483 - accuracy: 0.7969\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5302 - accuracy: 0.8035\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5268 - accuracy: 0.8082 - val_loss: 0.5589 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5622 - accuracy: 0.7500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5198 - accuracy: 0.8102\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.8082 - val_loss: 0.5580 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5704 - accuracy: 0.7344\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5227 - accuracy: 0.8096\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5237 - accuracy: 0.8082 - val_loss: 0.5571 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4552 - accuracy: 0.8594\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5215 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5214 - accuracy: 0.8082 - val_loss: 0.5563 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5141 - accuracy: 0.8281\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5263 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5260 - accuracy: 0.8082 - val_loss: 0.5555 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5143 - accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5168 - accuracy: 0.8113\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5194 - accuracy: 0.8082 - val_loss: 0.5547 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5994 - accuracy: 0.7188\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5168 - accuracy: 0.8108\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5190 - accuracy: 0.8082 - val_loss: 0.5540 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5772 - accuracy: 0.7656\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5250 - accuracy: 0.8083\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.8082 - val_loss: 0.5533 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5243 - accuracy: 0.8281\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5172 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5172 - accuracy: 0.8082 - val_loss: 0.5527 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5843 - accuracy: 0.7344\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5179 - accuracy: 0.8095\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5200 - accuracy: 0.8082 - val_loss: 0.5520 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4489 - accuracy: 0.8594\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5147 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.8082 - val_loss: 0.5514 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5066 - accuracy: 0.8281\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5146 - accuracy: 0.8102\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5170 - accuracy: 0.8082 - val_loss: 0.5509 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5010 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5185 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5164 - accuracy: 0.8082 - val_loss: 0.5504 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5247 - accuracy: 0.7812\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5086 - accuracy: 0.8090\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5093 - accuracy: 0.8082 - val_loss: 0.5498 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5048 - accuracy: 0.8125\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5179 - accuracy: 0.8054\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5149 - accuracy: 0.8082 - val_loss: 0.5493 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5283 - accuracy: 0.7969\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5125 - accuracy: 0.8071\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.8082 - val_loss: 0.5488 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6166 - accuracy: 0.7188\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5110 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.8082 - val_loss: 0.5483 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5689 - accuracy: 0.7656\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5061 - accuracy: 0.8113\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.8082 - val_loss: 0.5478 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5335 - accuracy: 0.7656\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5162 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5124 - accuracy: 0.8082 - val_loss: 0.5474 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5538 - accuracy: 0.7969\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5158 - accuracy: 0.8060\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5133 - accuracy: 0.8082 - val_loss: 0.5470 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5781 - accuracy: 0.7344\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5153 - accuracy: 0.8006\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.8082 - val_loss: 0.5465 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6239 - accuracy: 0.7188\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5117 - accuracy: 0.8068\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5089 - accuracy: 0.8082 - val_loss: 0.5461 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4473 - accuracy: 0.8750\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5102 - accuracy: 0.8081\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5118 - accuracy: 0.8082 - val_loss: 0.5457 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6029 - accuracy: 0.7344\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5133 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5084 - accuracy: 0.8082 - val_loss: 0.5454 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4367 - accuracy: 0.8281\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5076 - accuracy: 0.8089\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5082 - accuracy: 0.8082 - val_loss: 0.5450 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5252 - accuracy: 0.7969\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5036 - accuracy: 0.8119\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5071 - accuracy: 0.8082 - val_loss: 0.5447 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3696 - accuracy: 0.9219\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5121 - accuracy: 0.8050\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5078 - accuracy: 0.8082 - val_loss: 0.5444 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5451 - accuracy: 0.7656\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5054 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5102 - accuracy: 0.8082 - val_loss: 0.5440 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4555 - accuracy: 0.8438\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5088 - accuracy: 0.8065\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5072 - accuracy: 0.8082 - val_loss: 0.5437 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4489 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8082 - val_loss: 0.5434 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5293 - accuracy: 0.7812\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5057 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.8082 - val_loss: 0.5431 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5118 - accuracy: 0.8125\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5020 - accuracy: 0.8131\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.8082 - val_loss: 0.5428 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5320 - accuracy: 0.7969\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5050 - accuracy: 0.8075\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.8082 - val_loss: 0.5425 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5893 - accuracy: 0.7344\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5028 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.8082 - val_loss: 0.5422 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3885 - accuracy: 0.8750\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5090 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.8082 - val_loss: 0.5420 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4785 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5094 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.8082 - val_loss: 0.5417 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6083 - accuracy: 0.7031\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5026 - accuracy: 0.8082 - val_loss: 0.5414 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4063 - accuracy: 0.9062\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5058 - accuracy: 0.8061\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.8082 - val_loss: 0.5412 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5918 - accuracy: 0.7812\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5064 - accuracy: 0.8090\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5068 - accuracy: 0.8082 - val_loss: 0.5410 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5225 - accuracy: 0.7812\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.8082 - val_loss: 0.5407 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5511 - accuracy: 0.7656\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4999 - accuracy: 0.8144\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5061 - accuracy: 0.8082 - val_loss: 0.5405 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.8906\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5048 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.8082 - val_loss: 0.5402 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4634 - accuracy: 0.8438\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5051 - accuracy: 0.8096\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5065 - accuracy: 0.8082 - val_loss: 0.5400 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5688 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5056 - accuracy: 0.8082 - val_loss: 0.5397 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4964 - accuracy: 0.8125\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5070 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.8082 - val_loss: 0.5395 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5027 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5035 - accuracy: 0.8090\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.8082 - val_loss: 0.5393 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4392 - accuracy: 0.8438\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5029 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.8082 - val_loss: 0.5391 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5703 - accuracy: 0.7656\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5018 - accuracy: 0.8090\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5022 - accuracy: 0.8082 - val_loss: 0.5389 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4878 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4979 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4986 - accuracy: 0.8082 - val_loss: 0.5386 - val_accuracy: 0.7704\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5494 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.8082 - val_loss: 0.5384 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4018 - accuracy: 0.8750\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.8082 - val_loss: 0.5382 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5122 - accuracy: 0.7812\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.8082 - val_loss: 0.5380 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5817 - accuracy: 0.7656\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5014 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.8082 - val_loss: 0.5377 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4532 - accuracy: 0.8438\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.8082 - val_loss: 0.5375 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5514 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5039 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5039 - accuracy: 0.8082 - val_loss: 0.5373 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6197 - accuracy: 0.7188\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4999 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.8082 - val_loss: 0.5371 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6227 - accuracy: 0.7188\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4991 - accuracy: 0.8056\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8082 - val_loss: 0.5369 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5322 - accuracy: 0.7969\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5073 - accuracy: 0.8027\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.8082 - val_loss: 0.5367 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5069 - accuracy: 0.7969\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5032 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5007 - accuracy: 0.8082 - val_loss: 0.5365 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4134 - accuracy: 0.8906\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4960 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.8082 - val_loss: 0.5363 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4990 - accuracy: 0.8281\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4948 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4950 - accuracy: 0.8082 - val_loss: 0.5361 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5400 - accuracy: 0.7656\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4964 - accuracy: 0.8113\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5010 - accuracy: 0.8082 - val_loss: 0.5359 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4530 - accuracy: 0.8438\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4992 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8082 - val_loss: 0.5357 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5000 - accuracy: 0.8125\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4952 - accuracy: 0.8082 - val_loss: 0.5356 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4181 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.8082 - val_loss: 0.5354 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3914 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.8082 - val_loss: 0.5352 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4331 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8082 - val_loss: 0.5350 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4784 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8082 - val_loss: 0.5348 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6007 - accuracy: 0.7031\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.8082 - val_loss: 0.5346 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4682 - accuracy: 0.8281\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4977 - accuracy: 0.8082 - val_loss: 0.5344 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5388 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.8082 - val_loss: 0.5342 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4306 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.8082 - val_loss: 0.5340 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5665 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.8082 - val_loss: 0.5338 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5242 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.8082 - val_loss: 0.5336 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4682 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4969 - accuracy: 0.8082 - val_loss: 0.5334 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4937 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.8082 - val_loss: 0.5333 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5075 - accuracy: 0.7812\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.8082 - val_loss: 0.5331 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5274 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8082 - val_loss: 0.5329 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4911 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8082 - val_loss: 0.5327 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3897 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.8082 - val_loss: 0.5325 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5366 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4952 - accuracy: 0.8082 - val_loss: 0.5324 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4719 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.8082 - val_loss: 0.5322 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5523 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8082 - val_loss: 0.5320 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3640 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.8082 - val_loss: 0.5318 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4427 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4939 - accuracy: 0.8082 - val_loss: 0.5316 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4319 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.8082 - val_loss: 0.5314 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4919 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.8082 - val_loss: 0.5312 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4228 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.8082 - val_loss: 0.5311 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4611 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.8082 - val_loss: 0.5309 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6873 - accuracy: 0.6875\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.8082 - val_loss: 0.5307 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5909 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.8082 - val_loss: 0.5305 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4421 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8082 - val_loss: 0.5303 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5340 - accuracy: 0.7969\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.8082 - val_loss: 0.5301 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6082 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8082 - val_loss: 0.5300 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5903 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8082 - val_loss: 0.5298 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4159 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.8082 - val_loss: 0.5296 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4850 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8082 - val_loss: 0.5295 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4058 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4907 - accuracy: 0.8082 - val_loss: 0.5293 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5726 - accuracy: 0.7500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4885 - accuracy: 0.8113\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.8082 - val_loss: 0.5291 - val_accuracy: 0.7704\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4114 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8082 - val_loss: 0.5289 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4692 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8082 - val_loss: 0.5287 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4519 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.8082 - val_loss: 0.5285 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5347 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8082 - val_loss: 0.5284 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4747 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.8082 - val_loss: 0.5282 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4909 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.8082 - val_loss: 0.5280 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3715 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.8082 - val_loss: 0.5278 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5465 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8082 - val_loss: 0.5276 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4864 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8082 - val_loss: 0.5274 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5782 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.8082 - val_loss: 0.5273 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6314 - accuracy: 0.7031\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8082 - val_loss: 0.5271 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4784 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.8082 - val_loss: 0.5269 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4123 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8082 - val_loss: 0.5267 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5249 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.8082 - val_loss: 0.5265 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5316 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.8082 - val_loss: 0.5264 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4127 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.8082 - val_loss: 0.5262 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4919 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.8082 - val_loss: 0.5260 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3954 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8082 - val_loss: 0.5259 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4707 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4926 - accuracy: 0.8082 - val_loss: 0.5257 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5044 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8082 - val_loss: 0.5255 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4729 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8082 - val_loss: 0.5253 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5509 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8082 - val_loss: 0.5252 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4504 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4884 - accuracy: 0.8082 - val_loss: 0.5250 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5052 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.8082 - val_loss: 0.5248 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4804 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.8082 - val_loss: 0.5246 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4960 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.8082 - val_loss: 0.5245 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4697 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.8082 - val_loss: 0.5243 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6777 - accuracy: 0.6719\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4854 - accuracy: 0.8096\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.8082 - val_loss: 0.5241 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4545 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8082 - val_loss: 0.5240 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5351 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.8082 - val_loss: 0.5238 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5221 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.8082 - val_loss: 0.5236 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4964 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.8082 - val_loss: 0.5235 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4678 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8082 - val_loss: 0.5233 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5108 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.8082 - val_loss: 0.5231 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4957 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8082 - val_loss: 0.5230 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3603 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8082 - val_loss: 0.5228 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4032 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8082 - val_loss: 0.5227 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4148 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.8082 - val_loss: 0.5225 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4874 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8082 - val_loss: 0.5223 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4652 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8082 - val_loss: 0.5222 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4938 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8082 - val_loss: 0.5220 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4870 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.8082 - val_loss: 0.5218 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4589 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8082 - val_loss: 0.5217 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5200 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.8082 - val_loss: 0.5215 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3457 - accuracy: 0.9219\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4860 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.8082 - val_loss: 0.5213 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3976 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.8082 - val_loss: 0.5212 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4449 - accuracy: 0.8594\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4888 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.8082 - val_loss: 0.5210 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4822 - accuracy: 0.7969\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4865 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4851 - accuracy: 0.8082 - val_loss: 0.5208 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5499 - accuracy: 0.7656\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4841 - accuracy: 0.8119\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4884 - accuracy: 0.8082 - val_loss: 0.5207 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5337 - accuracy: 0.7500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4839 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.8082 - val_loss: 0.5205 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5671 - accuracy: 0.7812\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4945 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.8082 - val_loss: 0.5203 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5277 - accuracy: 0.7656\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.4958 - accuracy: 0.8011\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.4850 - accuracy: 0.8082 - val_loss: 0.5201 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3773 - accuracy: 0.8906\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4815 - accuracy: 0.8084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.8082 - val_loss: 0.5200 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6146 - accuracy: 0.7344\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4820 - accuracy: 0.8099\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.8082 - val_loss: 0.5198 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4449 - accuracy: 0.8438\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4881 - accuracy: 0.8057\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4859 - accuracy: 0.8082 - val_loss: 0.5197 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5274 - accuracy: 0.7656\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4845 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.8082 - val_loss: 0.5195 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5117 - accuracy: 0.7812\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4801 - accuracy: 0.8079\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.8082 - val_loss: 0.5193 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4688 - accuracy: 0.8125\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4850 - accuracy: 0.8077\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.8082 - val_loss: 0.5192 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5834 - accuracy: 0.7500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4834 - accuracy: 0.8090\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.8082 - val_loss: 0.5190 - val_accuracy: 0.7704\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.2, 'hidden_layers': 4.0, 'hidden_units': 128.0, 'input_dropout': 0.1, 'optimizer': {'lr': 4.98437672537045e-05, 'type': 'sgd'}}, logloss: 0.5190\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 40%|████▍      | 4/10 [00:56<01:30, 15.05s/trial, best loss: 0.417095622865065]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 26s - loss: 1.0418 - accuracy: 0.3594\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.9328 - accuracy: 0.3914 \n",
      "28/28 [==============================] - 1s 8ms/step - loss: 0.9114 - accuracy: 0.3961 - val_loss: 0.6851 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.8837 - accuracy: 0.4844\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.7820 - accuracy: 0.4896\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.7626 - accuracy: 0.5139 - val_loss: 0.6596 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7245 - accuracy: 0.5469\n",
      "16/28 [================>.............] - ETA: 0s - loss: 0.6730 - accuracy: 0.6084\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6640 - accuracy: 0.6192 - val_loss: 0.6348 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6549 - accuracy: 0.5938\n",
      "14/28 [==============>...............] - ETA: 0s - loss: 0.6053 - accuracy: 0.6819\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.5915 - accuracy: 0.7001 - val_loss: 0.6078 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5464 - accuracy: 0.7500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5453 - accuracy: 0.7457\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5404 - accuracy: 0.7553 - val_loss: 0.5808 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5272 - accuracy: 0.8906\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.4876 - accuracy: 0.8192\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.8179 - val_loss: 0.5581 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4888 - accuracy: 0.8281\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.4588 - accuracy: 0.8410\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.4503 - accuracy: 0.8423 - val_loss: 0.5330 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4232 - accuracy: 0.8281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.4132 - accuracy: 0.8497\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4179 - accuracy: 0.8463 - val_loss: 0.5097 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3646 - accuracy: 0.9062\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.3883 - accuracy: 0.8824\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3879 - accuracy: 0.8788 - val_loss: 0.4913 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3398 - accuracy: 0.9062\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.3783 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8799 - val_loss: 0.4753 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3410 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.3556 - accuracy: 0.8758\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3536 - accuracy: 0.8782 - val_loss: 0.4598 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4085 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.3334 - accuracy: 0.8997\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3364 - accuracy: 0.8924 - val_loss: 0.4482 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3264 - accuracy: 0.9062\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.3283 - accuracy: 0.8832\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3267 - accuracy: 0.8839 - val_loss: 0.4412 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2637 - accuracy: 0.9219\n",
      "16/28 [================>.............] - ETA: 0s - loss: 0.3040 - accuracy: 0.9023\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3053 - accuracy: 0.8970 - val_loss: 0.4319 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2914 - accuracy: 0.8750\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.3017 - accuracy: 0.8953\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3026 - accuracy: 0.8930 - val_loss: 0.4243 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2693 - accuracy: 0.9531\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.2776 - accuracy: 0.9133\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.8998 - val_loss: 0.4187 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2313 - accuracy: 0.9531\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2755 - accuracy: 0.9128\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2767 - accuracy: 0.9118 - val_loss: 0.4162 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2907 - accuracy: 0.9219\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2822 - accuracy: 0.8947\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2816 - accuracy: 0.8998 - val_loss: 0.4114 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2810 - accuracy: 0.9062\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.2659 - accuracy: 0.9099\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.9158 - val_loss: 0.4098 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3523 - accuracy: 0.8438\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.2613 - accuracy: 0.9078\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9089 - val_loss: 0.4052 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2066 - accuracy: 0.9531\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2595 - accuracy: 0.9087\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2559 - accuracy: 0.9084 - val_loss: 0.4052 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2176 - accuracy: 0.9219\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2560 - accuracy: 0.9021\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2494 - accuracy: 0.9067 - val_loss: 0.4018 - val_accuracy: 0.8316\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3341 - accuracy: 0.8281\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2385 - accuracy: 0.9178\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2389 - accuracy: 0.9118 - val_loss: 0.4002 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3101 - accuracy: 0.8438\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.2428 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2350 - accuracy: 0.9124 - val_loss: 0.3992 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1926 - accuracy: 0.9531\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2274 - accuracy: 0.9268\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9192 - val_loss: 0.4005 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1629 - accuracy: 0.9688\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.2168 - accuracy: 0.9312\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9186 - val_loss: 0.4006 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2210 - accuracy: 0.9375\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2237 - accuracy: 0.9243\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2234 - accuracy: 0.9243 - val_loss: 0.4029 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2236 - accuracy: 0.9375\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.2172 - accuracy: 0.9202\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9220 - val_loss: 0.3995 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1901 - accuracy: 0.9688\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.2006 - accuracy: 0.9375\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2115 - accuracy: 0.9277 - val_loss: 0.3994 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1713 - accuracy: 0.9531\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.2214 - accuracy: 0.9236\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2180 - accuracy: 0.9283 - val_loss: 0.4002 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1947 - accuracy: 0.9219\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.2101 - accuracy: 0.9227\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.2003 - accuracy: 0.9300 - val_loss: 0.4012 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1385 - accuracy: 0.9844\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.1892 - accuracy: 0.9366\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1915 - accuracy: 0.9351 - val_loss: 0.3998 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2539 - accuracy: 0.8906\n",
      "16/28 [================>.............] - ETA: 0s - loss: 0.1986 - accuracy: 0.9326\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9311 - val_loss: 0.4018 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2960 - accuracy: 0.8906\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1961 - accuracy: 0.9383\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9402 - val_loss: 0.4034 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1942 - accuracy: 0.9219\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1856 - accuracy: 0.9408\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9351 - val_loss: 0.4030 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1782 - accuracy: 0.9375\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1806 - accuracy: 0.9441\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1819 - accuracy: 0.9414 - val_loss: 0.4069 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2136 - accuracy: 0.9219\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.1866 - accuracy: 0.9375\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9408 - val_loss: 0.4086 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1572 - accuracy: 0.9531\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1837 - accuracy: 0.9367\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9425 - val_loss: 0.4017 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1444 - accuracy: 0.9844\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.1719 - accuracy: 0.9505\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9454 - val_loss: 0.4068 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1538 - accuracy: 0.9531\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1592 - accuracy: 0.9515\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9516 - val_loss: 0.4028 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1894 - accuracy: 0.9375\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1562 - accuracy: 0.9548\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9448 - val_loss: 0.4002 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1503 - accuracy: 0.9688\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.1516 - accuracy: 0.9548\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9482 - val_loss: 0.4030 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1866 - accuracy: 0.9219\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.1478 - accuracy: 0.9494\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9533 - val_loss: 0.4061 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.1251 - accuracy: 0.9844\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.1433 - accuracy: 0.9578\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9567 - val_loss: 0.4032 - val_accuracy: 0.8469\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.05, 'optimizer': {'lr': 7.9094022518724e-05, 'type': 'adam'}}, logloss: 0.3992\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 50%|████▌    | 5/10 [01:02<00:59, 11.86s/trial, best loss: 0.39924007248637094]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/19 [>.............................] - ETA: 7s - loss: 0.6971 - accuracy: 0.4896\n",
      "19/19 [==============================] - 1s 9ms/step - loss: 0.6781 - accuracy: 0.5697 - val_loss: 0.6650 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6507 - accuracy: 0.6875\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6401 - accuracy: 0.7240 - val_loss: 0.6418 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6055 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6135 - accuracy: 0.7746 - val_loss: 0.6222 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6024 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5947 - accuracy: 0.7951 - val_loss: 0.6062 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5918 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5723 - accuracy: 0.8042 - val_loss: 0.5931 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5438 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.8088 - val_loss: 0.5817 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5393 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5505 - accuracy: 0.8076 - val_loss: 0.5725 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5766 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5366 - accuracy: 0.8082 - val_loss: 0.5649 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4946 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.8082 - val_loss: 0.5583 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5217 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.8082 - val_loss: 0.5525 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4935 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5141 - accuracy: 0.8082 - val_loss: 0.5475 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5538 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.8082 - val_loss: 0.5431 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5352 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.8082 - val_loss: 0.5390 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4931 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8082\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.5031 - accuracy: 0.8082 - val_loss: 0.5351 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5321 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4940 - accuracy: 0.8082 - val_loss: 0.5317 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5192 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.8082 - val_loss: 0.5283 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4482 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.8082 - val_loss: 0.5251 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4166 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.8082 - val_loss: 0.5219 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4277 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.8082 - val_loss: 0.5186 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4554 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4760 - accuracy: 0.8082 - val_loss: 0.5156 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5290 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.8082 - val_loss: 0.5128 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4448 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4748 - accuracy: 0.8082 - val_loss: 0.5099 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4458 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.8082 - val_loss: 0.5070 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5210 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4682 - accuracy: 0.8082 - val_loss: 0.5039 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4602 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4694 - accuracy: 0.8082 - val_loss: 0.5011 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4583 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4624 - accuracy: 0.8082 - val_loss: 0.4980 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5212 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.8082 - val_loss: 0.4954 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6038 - accuracy: 0.7083\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.8082\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4591 - accuracy: 0.8082 - val_loss: 0.4928 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4085 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4510 - accuracy: 0.8088 - val_loss: 0.4903 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4591 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4518 - accuracy: 0.8082 - val_loss: 0.4875 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5271 - accuracy: 0.7396\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4504 - accuracy: 0.8082 - val_loss: 0.4850 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3437 - accuracy: 0.8958\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4485 - accuracy: 0.8105 - val_loss: 0.4827 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4598 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4459 - accuracy: 0.8093 - val_loss: 0.4803 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4738 - accuracy: 0.7812\n",
      "15/19 [======================>.......] - ETA: 0s - loss: 0.4467 - accuracy: 0.8062\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4444 - accuracy: 0.8093 - val_loss: 0.4781 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4303 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.8105 - val_loss: 0.4757 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3892 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4423 - accuracy: 0.8099 - val_loss: 0.4733 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5550 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4366 - accuracy: 0.8099 - val_loss: 0.4713 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3267 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4372 - accuracy: 0.8122 - val_loss: 0.4694 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4436 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4346 - accuracy: 0.8105 - val_loss: 0.4670 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8139 - val_loss: 0.4651 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3624 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.8156 - val_loss: 0.4630 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3369 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4319 - accuracy: 0.8167 - val_loss: 0.4614 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4583 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4318 - accuracy: 0.8122 - val_loss: 0.4597 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4103 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4242 - accuracy: 0.8150 - val_loss: 0.4578 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3537 - accuracy: 0.8750\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.4232 - accuracy: 0.8192\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4238 - accuracy: 0.8190 - val_loss: 0.4560 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4258 - accuracy: 0.8229\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.8145\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8145 - val_loss: 0.4545 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4796 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4186 - accuracy: 0.8184 - val_loss: 0.4526 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3984 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4166 - accuracy: 0.8224 - val_loss: 0.4510 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4770 - accuracy: 0.7917\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4195 - accuracy: 0.8219\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8219 - val_loss: 0.4496 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5111 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4173 - accuracy: 0.8219 - val_loss: 0.4485 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4195 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8264 - val_loss: 0.4472 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3659 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4143 - accuracy: 0.8230 - val_loss: 0.4459 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4434 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8293 - val_loss: 0.4446 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3942 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8241 - val_loss: 0.4433 - val_accuracy: 0.8010\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4059 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4142 - accuracy: 0.8275 - val_loss: 0.4420 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3698 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8304 - val_loss: 0.4410 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4709 - accuracy: 0.7917\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.8275\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.8275 - val_loss: 0.4400 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4231 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4078 - accuracy: 0.8332 - val_loss: 0.4389 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4433 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4081 - accuracy: 0.8281 - val_loss: 0.4379 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4817 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4089 - accuracy: 0.8327 - val_loss: 0.4371 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4885 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4063 - accuracy: 0.8310 - val_loss: 0.4364 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4018 - accuracy: 0.8378 - val_loss: 0.4356 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3882 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4019 - accuracy: 0.8344 - val_loss: 0.4349 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4411 - accuracy: 0.8333\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.4015 - accuracy: 0.8380\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4013 - accuracy: 0.8378 - val_loss: 0.4341 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8438\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8327\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4016 - accuracy: 0.8327 - val_loss: 0.4334 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4727 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3995 - accuracy: 0.8304 - val_loss: 0.4327 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4064 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4007 - accuracy: 0.8384 - val_loss: 0.4319 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3782 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8372 - val_loss: 0.4315 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4462 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.4007 - accuracy: 0.8344 - val_loss: 0.4312 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3406 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8395 - val_loss: 0.4306 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4183 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8469 - val_loss: 0.4302 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4813 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8389 - val_loss: 0.4298 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3174 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3974 - accuracy: 0.8355 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4079 - accuracy: 0.8021\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3861 - accuracy: 0.8426\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3904 - accuracy: 0.8401 - val_loss: 0.4287 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4420 - accuracy: 0.8125\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.3943 - accuracy: 0.8355\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3943 - accuracy: 0.8355 - val_loss: 0.4284 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4314 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8384 - val_loss: 0.4280 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4024 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3955 - accuracy: 0.8435 - val_loss: 0.4277 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2811 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3921 - accuracy: 0.8395 - val_loss: 0.4274 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3662 - accuracy: 0.8646\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3908 - accuracy: 0.8374\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3905 - accuracy: 0.8389 - val_loss: 0.4270 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4246 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8452 - val_loss: 0.4266 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3780 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3823 - accuracy: 0.8441 - val_loss: 0.4263 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3656 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8406 - val_loss: 0.4261 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4853 - accuracy: 0.7812\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.3814 - accuracy: 0.8444\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8423 - val_loss: 0.4259 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4261 - accuracy: 0.7917\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.3894 - accuracy: 0.8452\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3894 - accuracy: 0.8452 - val_loss: 0.4256 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4772 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3908 - accuracy: 0.8441 - val_loss: 0.4253 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4219 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3827 - accuracy: 0.8509 - val_loss: 0.4249 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2948 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3854 - accuracy: 0.8503 - val_loss: 0.4247 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8646\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.3905 - accuracy: 0.8406\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3905 - accuracy: 0.8406 - val_loss: 0.4244 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3740 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3913 - accuracy: 0.8361 - val_loss: 0.4243 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8486 - val_loss: 0.4242 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3140 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3918 - accuracy: 0.8429 - val_loss: 0.4243 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4404 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3951 - accuracy: 0.8446 - val_loss: 0.4239 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2906 - accuracy: 0.9271\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3879 - accuracy: 0.8441 - val_loss: 0.4236 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3498 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3825 - accuracy: 0.8458 - val_loss: 0.4233 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4584 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3874 - accuracy: 0.8515 - val_loss: 0.4233 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4381 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3846 - accuracy: 0.8429 - val_loss: 0.4232 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4780 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3885 - accuracy: 0.8475 - val_loss: 0.4229 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4134 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3849 - accuracy: 0.8503 - val_loss: 0.4229 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3536 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3861 - accuracy: 0.8503 - val_loss: 0.4227 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3000 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8469 - val_loss: 0.4227 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5308 - accuracy: 0.7396\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3879 - accuracy: 0.8441 - val_loss: 0.4226 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3865 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3818 - accuracy: 0.8469 - val_loss: 0.4223 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4751 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8446 - val_loss: 0.4224 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3290 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8446 - val_loss: 0.4222 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3450 - accuracy: 0.8958\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8554 - val_loss: 0.4222 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3990 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8480 - val_loss: 0.4219 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4165 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3774 - accuracy: 0.8509 - val_loss: 0.4219 - val_accuracy: 0.8367\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3904 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8503 - val_loss: 0.4220 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3011 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8458 - val_loss: 0.4218 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3228 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.8566 - val_loss: 0.4217 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4065 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.8452 - val_loss: 0.4216 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3925 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8509 - val_loss: 0.4216 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2805 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8429 - val_loss: 0.4215 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4040 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3817 - accuracy: 0.8486 - val_loss: 0.4212 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3966 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.8503 - val_loss: 0.4212 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4865 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3775 - accuracy: 0.8458 - val_loss: 0.4209 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4032 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3780 - accuracy: 0.8458 - val_loss: 0.4207 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3887 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8520 - val_loss: 0.4209 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4435 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3774 - accuracy: 0.8566 - val_loss: 0.4209 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4038 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8509 - val_loss: 0.4207 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3575 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3749 - accuracy: 0.8480 - val_loss: 0.4207 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4489 - accuracy: 0.7917\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3795 - accuracy: 0.8492 - val_loss: 0.4205 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3152 - accuracy: 0.8958\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8497 - val_loss: 0.4205 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4769 - accuracy: 0.7604\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3748 - accuracy: 0.8515 - val_loss: 0.4203 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3115 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8446 - val_loss: 0.4202 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3173 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8515 - val_loss: 0.4200 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4914 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8515 - val_loss: 0.4200 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4068 - accuracy: 0.8438\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.3824 - accuracy: 0.8548\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8532 - val_loss: 0.4201 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4255 - accuracy: 0.8333\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3744 - accuracy: 0.8466\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8475 - val_loss: 0.4202 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2661 - accuracy: 0.9062\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3797 - accuracy: 0.8507\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8509 - val_loss: 0.4202 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4374 - accuracy: 0.8333\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.3711 - accuracy: 0.8529\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3702 - accuracy: 0.8526 - val_loss: 0.4201 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4302 - accuracy: 0.8229\n",
      "18/19 [===========================>..] - ETA: 0s - loss: 0.3704 - accuracy: 0.8524\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.8497 - val_loss: 0.4200 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3321 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8526 - val_loss: 0.4199 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3635 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3752 - accuracy: 0.8526 - val_loss: 0.4199 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4825 - accuracy: 0.7812\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3759 - accuracy: 0.8509 - val_loss: 0.4200 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3189 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8537 - val_loss: 0.4198 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3270 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.8543 - val_loss: 0.4199 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3896 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3732 - accuracy: 0.8497 - val_loss: 0.4197 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3499 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8497 - val_loss: 0.4196 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3827 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3795 - accuracy: 0.8497 - val_loss: 0.4196 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2905 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3778 - accuracy: 0.8441 - val_loss: 0.4197 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3919 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3765 - accuracy: 0.8435 - val_loss: 0.4194 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3182 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3692 - accuracy: 0.8560 - val_loss: 0.4193 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2791 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3738 - accuracy: 0.8497 - val_loss: 0.4191 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3939 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.3727 - accuracy: 0.8526 - val_loss: 0.4189 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3603 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8549 - val_loss: 0.4191 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3331 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8549 - val_loss: 0.4192 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3641 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3738 - accuracy: 0.8475 - val_loss: 0.4191 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2942 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8549 - val_loss: 0.4190 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3556 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8532 - val_loss: 0.4189 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2844 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8497 - val_loss: 0.4187 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3419 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3738 - accuracy: 0.8452 - val_loss: 0.4186 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4573 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8520 - val_loss: 0.4185 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4314 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8446 - val_loss: 0.4186 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3934 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8503 - val_loss: 0.4186 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3717 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3697 - accuracy: 0.8520 - val_loss: 0.4184 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3099 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8543 - val_loss: 0.4185 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2904 - accuracy: 0.9062\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8554 - val_loss: 0.4184 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2962 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8589 - val_loss: 0.4184 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3735 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3716 - accuracy: 0.8589 - val_loss: 0.4184 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3692 - accuracy: 0.8526 - val_loss: 0.4184 - val_accuracy: 0.8418\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3047 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8566 - val_loss: 0.4186 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3100 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8589 - val_loss: 0.4183 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3269 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8583 - val_loss: 0.4183 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3876 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8458 - val_loss: 0.4181 - val_accuracy: 0.8469\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3345 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3683 - accuracy: 0.8520 - val_loss: 0.4182 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4305 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3669 - accuracy: 0.8571 - val_loss: 0.4183 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2785 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3662 - accuracy: 0.8560 - val_loss: 0.4183 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2931 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8577 - val_loss: 0.4182 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3333 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8571 - val_loss: 0.4182 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3830 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3690 - accuracy: 0.8537 - val_loss: 0.4181 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3507 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3607 - accuracy: 0.8532 - val_loss: 0.4182 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3473 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.8497 - val_loss: 0.4183 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3173 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8509 - val_loss: 0.4181 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3153 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3580 - accuracy: 0.8589 - val_loss: 0.4183 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4233 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3725 - accuracy: 0.8475 - val_loss: 0.4181 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3595 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8560 - val_loss: 0.4180 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3509 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8497 - val_loss: 0.4178 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.2849 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3674 - accuracy: 0.8589 - val_loss: 0.4178 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3141 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8526 - val_loss: 0.4176 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3399 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3658 - accuracy: 0.8606 - val_loss: 0.4175 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3575 - accuracy: 0.8646\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.8526 - val_loss: 0.4176 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3797 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8560 - val_loss: 0.4174 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8549 - val_loss: 0.4174 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3984 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3595 - accuracy: 0.8560 - val_loss: 0.4173 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3069 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3679 - accuracy: 0.8549 - val_loss: 0.4173 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3911 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8554 - val_loss: 0.4174 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3895 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.8458 - val_loss: 0.4172 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.5018 - accuracy: 0.7708\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3573 - accuracy: 0.8520 - val_loss: 0.4173 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4185 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3640 - accuracy: 0.8549 - val_loss: 0.4172 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3835 - accuracy: 0.8438\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 0.8566 - val_loss: 0.4173 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3251 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8475 - val_loss: 0.4170 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3142 - accuracy: 0.9167\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3582 - accuracy: 0.8577 - val_loss: 0.4172 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3998 - accuracy: 0.8125\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8526 - val_loss: 0.4170 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4050 - accuracy: 0.8021\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8543 - val_loss: 0.4173 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4347 - accuracy: 0.8333\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8571 - val_loss: 0.4171 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3013 - accuracy: 0.8854\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8589 - val_loss: 0.4171 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4102 - accuracy: 0.8542\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8566 - val_loss: 0.4169 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.4513 - accuracy: 0.8229\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3614 - accuracy: 0.8577 - val_loss: 0.4169 - val_accuracy: 0.8418\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.3209 - accuracy: 0.8750\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.3582 - accuracy: 0.8606 - val_loss: 0.4171 - val_accuracy: 0.8418\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 96.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.0, 'optimizer': {'lr': 1.4682558271483188e-05, 'type': 'adam'}}, logloss: 0.4171\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 60%|█████▍   | 6/10 [01:17<00:52, 13.07s/trial, best loss: 0.39924007248637094]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 22s - loss: 0.7848 - accuracy: 0.2969\n",
      "28/28 [==============================] - 1s 6ms/step - loss: 0.7334 - accuracy: 0.4200 - val_loss: 0.7014 - val_accuracy: 0.5306\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7211 - accuracy: 0.4844\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6810 - accuracy: 0.5999 - val_loss: 0.6636 - val_accuracy: 0.6327\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6539 - accuracy: 0.6094\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6388 - accuracy: 0.7006 - val_loss: 0.6327 - val_accuracy: 0.7398\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6096 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6025 - accuracy: 0.7752 - val_loss: 0.6078 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5863 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.7968 - val_loss: 0.5878 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5697 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.8059 - val_loss: 0.5722 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5417 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5289 - accuracy: 0.8082 - val_loss: 0.5606 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5784 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.8071 - val_loss: 0.5513 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5231 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.8076 - val_loss: 0.5440 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4647 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.8082 - val_loss: 0.5382 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4975 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.8082 - val_loss: 0.5333 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5778 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8082 - val_loss: 0.5291 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5324 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.8082 - val_loss: 0.5254 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4490 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.8082 - val_loss: 0.5218 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5856 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.8082 - val_loss: 0.5182 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4979 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.8082 - val_loss: 0.5148 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4659 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.8082 - val_loss: 0.5117 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6416 - accuracy: 0.6875\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.8082 - val_loss: 0.5083 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4050 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8093 - val_loss: 0.5054 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5130 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4505 - accuracy: 0.8093 - val_loss: 0.5019 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4609 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.8082 - val_loss: 0.4991 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4297 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4450 - accuracy: 0.8093 - val_loss: 0.4963 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4546 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8093 - val_loss: 0.4931 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4888 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.8093 - val_loss: 0.4908 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4796 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8110 - val_loss: 0.4878 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4349 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8162 - val_loss: 0.4856 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2853 - accuracy: 0.9219\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8127 - val_loss: 0.4835 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4045 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8173 - val_loss: 0.4811 - val_accuracy: 0.7806\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4149 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.8162 - val_loss: 0.4789 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3393 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8213 - val_loss: 0.4767 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4886 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8207 - val_loss: 0.4745 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4952 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8184 - val_loss: 0.4728 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4072 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8241 - val_loss: 0.4710 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4791 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8258 - val_loss: 0.4694 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4726 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8236 - val_loss: 0.4682 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4256 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8264 - val_loss: 0.4669 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4037 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.8298 - val_loss: 0.4653 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3602 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8270 - val_loss: 0.4634 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3069 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8241 - val_loss: 0.4619 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3896 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8332 - val_loss: 0.4604 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3474 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.8304 - val_loss: 0.4592 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3750 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4174 - accuracy: 0.8270 - val_loss: 0.4582 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3973 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8304 - val_loss: 0.4568 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3426 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8321 - val_loss: 0.4559 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3756 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8349 - val_loss: 0.4549 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5221 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8401 - val_loss: 0.4538 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3795 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8344 - val_loss: 0.4529 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3241 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8395 - val_loss: 0.4521 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4459 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8338 - val_loss: 0.4515 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3554 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8384 - val_loss: 0.4506 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3423 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8378 - val_loss: 0.4498 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4646 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8378 - val_loss: 0.4485 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3378 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8401 - val_loss: 0.4481 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2832 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8332 - val_loss: 0.4475 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4691 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8384 - val_loss: 0.4466 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3263 - accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8389 - val_loss: 0.4461 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3880 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8344 - val_loss: 0.4457 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3573 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8367 - val_loss: 0.4447 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5212 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8384 - val_loss: 0.4443 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3685 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8418 - val_loss: 0.4437 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4553 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8446 - val_loss: 0.4431 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3802 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8492 - val_loss: 0.4426 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5467 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8423 - val_loss: 0.4420 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3826 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8429 - val_loss: 0.4415 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4444 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8406 - val_loss: 0.4408 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4274 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8412 - val_loss: 0.4404 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3940 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8446 - val_loss: 0.4404 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5202 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8361 - val_loss: 0.4404 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3396 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8418 - val_loss: 0.4399 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3951 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8423 - val_loss: 0.4391 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4329 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8429 - val_loss: 0.4387 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3914 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8423 - val_loss: 0.4383 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3541 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8515 - val_loss: 0.4381 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4358 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8423 - val_loss: 0.4381 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4024 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8452 - val_loss: 0.4382 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4632 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8503 - val_loss: 0.4375 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3079 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.8401 - val_loss: 0.4373 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4225 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8406 - val_loss: 0.4367 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3637 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8463 - val_loss: 0.4372 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3017 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8469 - val_loss: 0.4368 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3336 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8406 - val_loss: 0.4367 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2783 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8389 - val_loss: 0.4364 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4288 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8435 - val_loss: 0.4359 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2430 - accuracy: 0.9219\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8384 - val_loss: 0.4358 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5489 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8486 - val_loss: 0.4353 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5046 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8435 - val_loss: 0.4351 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2758 - accuracy: 0.9219\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8469 - val_loss: 0.4351 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3637 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8492 - val_loss: 0.4349 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2667 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8515 - val_loss: 0.4344 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2951 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8435 - val_loss: 0.4341 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3388 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8429 - val_loss: 0.4337 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4089 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8458 - val_loss: 0.4338 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3492 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8469 - val_loss: 0.4335 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4720 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8441 - val_loss: 0.4335 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4744 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8486 - val_loss: 0.4332 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2907 - accuracy: 0.9375\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8537 - val_loss: 0.4327 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3619 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8554 - val_loss: 0.4326 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3648 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8520 - val_loss: 0.4324 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6104 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8475 - val_loss: 0.4323 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2920 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8526 - val_loss: 0.4323 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4780 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8452 - val_loss: 0.4322 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4426 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8452 - val_loss: 0.4324 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3568 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8429 - val_loss: 0.4318 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2814 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8515 - val_loss: 0.4322 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2153 - accuracy: 0.9375\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8486 - val_loss: 0.4314 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3739 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8480 - val_loss: 0.4316 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4202 - accuracy: 0.8281\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3758 - accuracy: 0.8549\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8549 - val_loss: 0.4316 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4443 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8446 - val_loss: 0.4317 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3731 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8475 - val_loss: 0.4315 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3332 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8532 - val_loss: 0.4317 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3723 - accuracy: 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8486 - val_loss: 0.4312 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4052 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3809 - accuracy: 0.8543 - val_loss: 0.4312 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3960 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8423 - val_loss: 0.4309 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4244 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8554 - val_loss: 0.4307 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3345 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8486 - val_loss: 0.4304 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4230 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8469 - val_loss: 0.4300 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3497 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8486 - val_loss: 0.4305 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3451 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8469 - val_loss: 0.4303 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3815 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8560 - val_loss: 0.4302 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4009 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8441 - val_loss: 0.4303 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3462 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8509 - val_loss: 0.4301 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3104 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3782 - accuracy: 0.8486 - val_loss: 0.4298 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4467 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8526 - val_loss: 0.4297 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4587 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8475 - val_loss: 0.4298 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3675 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8537 - val_loss: 0.4297 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4609 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8492 - val_loss: 0.4296 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4325 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8543 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3594 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8571 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4166 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8617 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3767 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8480 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3947 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.8537 - val_loss: 0.4295 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4139 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8486 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2531 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8611 - val_loss: 0.4290 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4231 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8497 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3829 - accuracy: 0.8594\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3729 - accuracy: 0.8588\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8606 - val_loss: 0.4287 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4064 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8543 - val_loss: 0.4286 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5450 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8497 - val_loss: 0.4290 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8281\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.3648 - accuracy: 0.8560\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.8566 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2627 - accuracy: 0.9062\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.3674 - accuracy: 0.8519\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8509 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2643 - accuracy: 0.9375\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3718 - accuracy: 0.8513\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8520 - val_loss: 0.4292 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3806 - accuracy: 0.8594\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.3686 - accuracy: 0.8588\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.8571 - val_loss: 0.4297 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3574 - accuracy: 0.8594\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3724 - accuracy: 0.8484\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.8480 - val_loss: 0.4293 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4029 - accuracy: 0.8125\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8463\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8463 - val_loss: 0.4291 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3849 - accuracy: 0.8594\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3720 - accuracy: 0.8509\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8509 - val_loss: 0.4293 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2548 - accuracy: 0.9375\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.8623\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3628 - accuracy: 0.8623 - val_loss: 0.4287 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4490 - accuracy: 0.8281\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.3718 - accuracy: 0.8510\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8526 - val_loss: 0.4284 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4561 - accuracy: 0.7969\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3684 - accuracy: 0.8532\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 0.8532 - val_loss: 0.4286 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2560 - accuracy: 0.9531\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3738 - accuracy: 0.8611\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.8611 - val_loss: 0.4286 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3808 - accuracy: 0.8750\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8566\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8566 - val_loss: 0.4278 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3777 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8520 - val_loss: 0.4282 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4050 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8526 - val_loss: 0.4278 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3394 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3744 - accuracy: 0.8515 - val_loss: 0.4276 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3100 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8566 - val_loss: 0.4281 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3828 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8480 - val_loss: 0.4283 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3537 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8600 - val_loss: 0.4283 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5507 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8537 - val_loss: 0.4277 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3808 - accuracy: 0.8281\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.8503\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8503 - val_loss: 0.4284 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2354 - accuracy: 0.9219\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3660 - accuracy: 0.8509 - val_loss: 0.4284 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2752 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3668 - accuracy: 0.8543 - val_loss: 0.4283 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2800 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8554 - val_loss: 0.4280 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3870 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3695 - accuracy: 0.8509 - val_loss: 0.4280 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3180 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3622 - accuracy: 0.8543 - val_loss: 0.4284 - val_accuracy: 0.8265\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2561 - accuracy: 0.9219\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8606 - val_loss: 0.4282 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2446 - accuracy: 0.9062\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 0.8554 - val_loss: 0.4283 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3384 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8611 - val_loss: 0.4279 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3003 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3637 - accuracy: 0.8537 - val_loss: 0.4283 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4268 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8554 - val_loss: 0.4282 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4602 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8509 - val_loss: 0.4282 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4082 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3646 - accuracy: 0.8537 - val_loss: 0.4277 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3987 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8560 - val_loss: 0.4277 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3673 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8543 - val_loss: 0.4272 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4675 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8549 - val_loss: 0.4279 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2417 - accuracy: 0.9375\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3623 - accuracy: 0.8589 - val_loss: 0.4279 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3590 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8560 - val_loss: 0.4276 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2796 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8589 - val_loss: 0.4278 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3682 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3579 - accuracy: 0.8571 - val_loss: 0.4274 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5585 - accuracy: 0.7188\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3674 - accuracy: 0.8515\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3674 - accuracy: 0.8515 - val_loss: 0.4279 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3657 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8549 - val_loss: 0.4279 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3261 - accuracy: 0.9062\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8560\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8560 - val_loss: 0.4279 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2862 - accuracy: 0.9062\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3661 - accuracy: 0.8559\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8577 - val_loss: 0.4276 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2787 - accuracy: 0.9219\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.3584 - accuracy: 0.8552\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3553 - accuracy: 0.8577 - val_loss: 0.4277 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2000 - accuracy: 0.9531\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.8657\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3520 - accuracy: 0.8657 - val_loss: 0.4282 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4049 - accuracy: 0.8281\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.3495 - accuracy: 0.8560\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3573 - accuracy: 0.8532 - val_loss: 0.4279 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2980 - accuracy: 0.8750\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.3539 - accuracy: 0.8612\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8571 - val_loss: 0.4287 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3041 - accuracy: 0.8750\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.3512 - accuracy: 0.8624\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8617 - val_loss: 0.4279 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3676 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8589 - val_loss: 0.4273 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3654 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.3591 - accuracy: 0.8582\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8589 - val_loss: 0.4277 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4463 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3636 - accuracy: 0.8560 - val_loss: 0.4275 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5693 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8600 - val_loss: 0.4276 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3133 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8583 - val_loss: 0.4274 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3563 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8566 - val_loss: 0.4271 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3924 - accuracy: 0.8281\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.8640\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8640 - val_loss: 0.4274 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4295 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3564 - accuracy: 0.8594 - val_loss: 0.4271 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3225 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3608 - accuracy: 0.8571 - val_loss: 0.4271 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3001 - accuracy: 0.8906\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3548 - accuracy: 0.8589\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8589 - val_loss: 0.4275 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.2313 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.8589 - val_loss: 0.4272 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3264 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8611 - val_loss: 0.4274 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3731 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8497 - val_loss: 0.4276 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3322 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8452 - val_loss: 0.4275 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3716 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8645 - val_loss: 0.4271 - val_accuracy: 0.8265\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.05, 'hidden_layers': 3.0, 'hidden_units': 96.0, 'input_dropout': 0.1, 'optimizer': {'lr': 3.360389349981267e-05, 'type': 'adam'}}, logloss: 0.4271\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 70%|██████▎  | 7/10 [01:33<00:41, 13.79s/trial, best loss: 0.39924007248637094]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 23s - loss: 0.7283 - accuracy: 0.6250\n",
      "28/28 [==============================] - 1s 7ms/step - loss: 0.6677 - accuracy: 0.6500 - val_loss: 0.6527 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6053 - accuracy: 0.6250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.6727 - val_loss: 0.6399 - val_accuracy: 0.6786\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7070 - accuracy: 0.5938\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6368 - accuracy: 0.6807 - val_loss: 0.6283 - val_accuracy: 0.7194\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6812 - accuracy: 0.7031\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.6807 - val_loss: 0.6192 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6367 - accuracy: 0.6719\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.7001 - val_loss: 0.6113 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5483 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6187 - accuracy: 0.6927 - val_loss: 0.6042 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6464 - accuracy: 0.6562\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6127 - accuracy: 0.7086 - val_loss: 0.5985 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5997 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6081 - accuracy: 0.7109 - val_loss: 0.5943 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6428 - accuracy: 0.6719\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6000 - accuracy: 0.7285 - val_loss: 0.5898 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5390 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5919 - accuracy: 0.7308 - val_loss: 0.5863 - val_accuracy: 0.7602\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5721 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.7456 - val_loss: 0.5825 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4974 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5743 - accuracy: 0.7445 - val_loss: 0.5794 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3851 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5980 - accuracy: 0.7405 - val_loss: 0.5770 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4599 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.7518 - val_loss: 0.5742 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6335 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6007 - accuracy: 0.7393 - val_loss: 0.5718 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6594 - accuracy: 0.6406\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7467 - val_loss: 0.5697 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7126 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.7592 - val_loss: 0.5684 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5125 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5769 - accuracy: 0.7655 - val_loss: 0.5662 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5465 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.7632 - val_loss: 0.5646 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4906 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5705 - accuracy: 0.7684 - val_loss: 0.5627 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4626 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.7655 - val_loss: 0.5609 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6306 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7758 - val_loss: 0.5595 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5444 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7763 - val_loss: 0.5579 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5154 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5529 - accuracy: 0.7752 - val_loss: 0.5565 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6028 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7826 - val_loss: 0.5548 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4937 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.7758 - val_loss: 0.5534 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4907 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7723 - val_loss: 0.5522 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5333 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5582 - accuracy: 0.7860 - val_loss: 0.5508 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6142 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5601 - accuracy: 0.7763 - val_loss: 0.5501 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5045 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7894 - val_loss: 0.5489 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4316 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7900 - val_loss: 0.5476 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5981 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7860 - val_loss: 0.5467 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4610 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5516 - accuracy: 0.7837 - val_loss: 0.5460 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6776 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5515 - accuracy: 0.7814 - val_loss: 0.5451 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5074 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7843 - val_loss: 0.5441 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5573 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7923 - val_loss: 0.5431 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6314 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7871 - val_loss: 0.5421 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5328 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7820 - val_loss: 0.5411 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3937 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5466 - accuracy: 0.7797 - val_loss: 0.5398 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5743 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7900 - val_loss: 0.5393 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5303 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5486 - accuracy: 0.7917 - val_loss: 0.5387 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4637 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.7866 - val_loss: 0.5381 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4824 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5431 - accuracy: 0.7837 - val_loss: 0.5377 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4626 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5301 - accuracy: 0.7900 - val_loss: 0.5369 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5769 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7928 - val_loss: 0.5362 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5570 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7962 - val_loss: 0.5355 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4410 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7940 - val_loss: 0.5352 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6310 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5467 - accuracy: 0.7928 - val_loss: 0.5345 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4731 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7917 - val_loss: 0.5340 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5417 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.7945 - val_loss: 0.5335 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5569 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5260 - accuracy: 0.7945 - val_loss: 0.5329 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5317 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7917 - val_loss: 0.5326 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4365 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5341 - accuracy: 0.7940 - val_loss: 0.5321 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4631 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.7917 - val_loss: 0.5316 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6067 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.8025 - val_loss: 0.5308 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5420 - accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7997 - val_loss: 0.5300 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4580 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.8014 - val_loss: 0.5297 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5701 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.8002 - val_loss: 0.5291 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5583 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.7974 - val_loss: 0.5285 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4906 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.8014 - val_loss: 0.5283 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5594 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7951 - val_loss: 0.5277 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5513 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7985 - val_loss: 0.5272 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5512 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7980 - val_loss: 0.5270 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4822 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7991 - val_loss: 0.5266 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4311 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7928 - val_loss: 0.5259 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6327 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7957 - val_loss: 0.5254 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6033 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7945 - val_loss: 0.5253 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4569 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7962 - val_loss: 0.5252 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4756 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7985 - val_loss: 0.5246 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5442 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.8031 - val_loss: 0.5242 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5159 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7968 - val_loss: 0.5241 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4707 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.7957 - val_loss: 0.5235 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6261 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7997 - val_loss: 0.5230 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5688 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.8008 - val_loss: 0.5225 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4697 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.8042 - val_loss: 0.5222 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5167 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7945 - val_loss: 0.5217 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4448 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.8059 - val_loss: 0.5213 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4784 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7985 - val_loss: 0.5211 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3729 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7980 - val_loss: 0.5206 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5101 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7997 - val_loss: 0.5201 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4845 - accuracy: 0.7656\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.7951\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 0.7951 - val_loss: 0.5199 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6025 - accuracy: 0.7812\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5220 - accuracy: 0.8019\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5220 - accuracy: 0.8019 - val_loss: 0.5199 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4577 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.8036 - val_loss: 0.5197 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3663 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.8042 - val_loss: 0.5190 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3765 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7962 - val_loss: 0.5187 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4732 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.8002 - val_loss: 0.5183 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5843 - accuracy: 0.7500\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5270 - accuracy: 0.7957\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5315 - accuracy: 0.7934 - val_loss: 0.5182 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4900 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7991 - val_loss: 0.5178 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4328 - accuracy: 0.8906\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7997 - val_loss: 0.5175 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5520 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.8002 - val_loss: 0.5169 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3884 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.7974 - val_loss: 0.5163 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 2s - loss: 0.4308 - accuracy: 0.8438\n",
      " 7/28 [======>.......................] - ETA: 0s - loss: 0.5079 - accuracy: 0.8013\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5160 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.5164 - accuracy: 0.8059 - val_loss: 0.5161 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5473 - accuracy: 0.7500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5378 - accuracy: 0.7951\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5187 - accuracy: 0.8025 - val_loss: 0.5158 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5483 - accuracy: 0.7500\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5152 - accuracy: 0.8024\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5293 - accuracy: 0.7911 - val_loss: 0.5154 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5989 - accuracy: 0.7344\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5160 - accuracy: 0.8033\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5192 - accuracy: 0.8038\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5180 - accuracy: 0.8023\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5168 - accuracy: 0.8017\n",
      "28/28 [==============================] - 0s 12ms/step - loss: 0.5152 - accuracy: 0.8031 - val_loss: 0.5152 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4895 - accuracy: 0.8750\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.4975 - accuracy: 0.8047\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5017 - accuracy: 0.8060\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5012 - accuracy: 0.8054\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.5012 - accuracy: 0.8054 - val_loss: 0.5148 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5747 - accuracy: 0.7656\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5140 - accuracy: 0.7905\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5055 - accuracy: 0.7951 - val_loss: 0.5146 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6524 - accuracy: 0.7812\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5147 - accuracy: 0.7961\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.5084 - accuracy: 0.7980 - val_loss: 0.5140 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6976 - accuracy: 0.7188\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5312 - accuracy: 0.7914\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5058 - accuracy: 0.8031 - val_loss: 0.5139 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5608 - accuracy: 0.7969\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5362 - accuracy: 0.7821\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5151 - accuracy: 0.8002 - val_loss: 0.5138 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5088 - accuracy: 0.7969\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5076 - accuracy: 0.7999\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.8025 - val_loss: 0.5132 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5118 - accuracy: 0.8125\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5173 - accuracy: 0.8019\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5160 - accuracy: 0.8036 - val_loss: 0.5134 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6130 - accuracy: 0.7812\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5163 - accuracy: 0.8104\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5211 - accuracy: 0.8093 - val_loss: 0.5132 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5017 - accuracy: 0.8125\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.4993 - accuracy: 0.8102\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.8036 - val_loss: 0.5126 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4946 - accuracy: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5063 - accuracy: 0.7955\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7991 - val_loss: 0.5127 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4951 - accuracy: 0.7812\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5122 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5112 - accuracy: 0.7962 - val_loss: 0.5122 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5315 - accuracy: 0.7656\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5075 - accuracy: 0.7997\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5004 - accuracy: 0.8076 - val_loss: 0.5118 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5258 - accuracy: 0.7969\n",
      "16/28 [================>.............] - ETA: 0s - loss: 0.5008 - accuracy: 0.7949\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.5015 - accuracy: 0.7968 - val_loss: 0.5115 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6838 - accuracy: 0.7188\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5260 - accuracy: 0.7981\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5185 - accuracy: 0.8019 - val_loss: 0.5109 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5015 - accuracy: 0.7656\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4831 - accuracy: 0.8027\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4896 - accuracy: 0.8002 - val_loss: 0.5109 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4521 - accuracy: 0.7969\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4882 - accuracy: 0.8132\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5089 - accuracy: 0.8048 - val_loss: 0.5105 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6122 - accuracy: 0.7812\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5052 - accuracy: 0.8087\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5076 - accuracy: 0.8088 - val_loss: 0.5100 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4272 - accuracy: 0.8438\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5031 - accuracy: 0.8033\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.8048 - val_loss: 0.5097 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4899 - accuracy: 0.7812\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5032 - accuracy: 0.7975\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.8031 - val_loss: 0.5098 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6014 - accuracy: 0.7812\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5060 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8071 - val_loss: 0.5097 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4397 - accuracy: 0.8438\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5099 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.8002 - val_loss: 0.5093 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3796 - accuracy: 0.8750\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4908 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4935 - accuracy: 0.8048 - val_loss: 0.5092 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4978 - accuracy: 0.8125\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5101 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.8036 - val_loss: 0.5091 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4963 - accuracy: 0.7969\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5071 - accuracy: 0.8025\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5045 - accuracy: 0.8031 - val_loss: 0.5088 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5986 - accuracy: 0.7344\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5008 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4976 - accuracy: 0.8065 - val_loss: 0.5082 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5705 - accuracy: 0.7812\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5153 - accuracy: 0.8095\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5143 - accuracy: 0.8088 - val_loss: 0.5081 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6117 - accuracy: 0.7188\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.5201 - accuracy: 0.7928\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5077 - accuracy: 0.8019 - val_loss: 0.5078 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5129 - accuracy: 0.8281\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4985 - accuracy: 0.8053\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4989 - accuracy: 0.8065 - val_loss: 0.5075 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5738 - accuracy: 0.7812\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.5023 - accuracy: 0.8008\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.8036 - val_loss: 0.5076 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5191 - accuracy: 0.7969\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.4918 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5087 - accuracy: 0.7991 - val_loss: 0.5074 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5863 - accuracy: 0.7344\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4926 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8048 - val_loss: 0.5069 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5218 - accuracy: 0.7656\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4993 - accuracy: 0.8119\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.8093 - val_loss: 0.5068 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5881 - accuracy: 0.7344\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4894 - accuracy: 0.8057\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.8036 - val_loss: 0.5065 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5048 - accuracy: 0.8125\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4975 - accuracy: 0.8083\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5049 - accuracy: 0.8036 - val_loss: 0.5059 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4649 - accuracy: 0.8125\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5052 - accuracy: 0.8083\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.8082 - val_loss: 0.5057 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4794 - accuracy: 0.8125\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5054 - accuracy: 0.8006\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5062 - accuracy: 0.8008 - val_loss: 0.5051 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4714 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4930 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4910 - accuracy: 0.8082 - val_loss: 0.5052 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4839 - accuracy: 0.8281\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.4812 - accuracy: 0.8070\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.8059 - val_loss: 0.5052 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4767 - accuracy: 0.7969\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4997 - accuracy: 0.8012\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.8002 - val_loss: 0.5050 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4425 - accuracy: 0.8438\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4956 - accuracy: 0.8035\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.8048 - val_loss: 0.5045 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4062 - accuracy: 0.8281\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4935 - accuracy: 0.8094\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.8071 - val_loss: 0.5044 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5499 - accuracy: 0.7656\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5014 - accuracy: 0.7987\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.7980 - val_loss: 0.5042 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4469 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4933 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.8042 - val_loss: 0.5042 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6469 - accuracy: 0.6875\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4912 - accuracy: 0.8065\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.8082 - val_loss: 0.5041 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4873 - accuracy: 0.8125\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.4992 - accuracy: 0.8006\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4957 - accuracy: 0.8002 - val_loss: 0.5036 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4697 - accuracy: 0.8594\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4943 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4984 - accuracy: 0.8065 - val_loss: 0.5033 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5747 - accuracy: 0.7656\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4928 - accuracy: 0.8091\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4935 - accuracy: 0.8099 - val_loss: 0.5033 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4190 - accuracy: 0.7656\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.4776 - accuracy: 0.8095\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.8093 - val_loss: 0.5034 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5256 - accuracy: 0.7812\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4987 - accuracy: 0.8041\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4958 - accuracy: 0.8076 - val_loss: 0.5028 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5379 - accuracy: 0.7969\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4983 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.4996 - accuracy: 0.7991 - val_loss: 0.5024 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4448 - accuracy: 0.8906\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.8059 - val_loss: 0.5019 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6561 - accuracy: 0.7188\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.4989 - accuracy: 0.8037\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.8099 - val_loss: 0.5017 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4397 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7985 - val_loss: 0.5018 - val_accuracy: 0.7755\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5078 - accuracy: 0.7969\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8014\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4913 - accuracy: 0.8014 - val_loss: 0.5018 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5806 - accuracy: 0.7812\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5038 - accuracy: 0.8108\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.8099 - val_loss: 0.5014 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4874 - accuracy: 0.7812\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4881 - accuracy: 0.8132\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.8105 - val_loss: 0.5010 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6208 - accuracy: 0.7500\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4895 - accuracy: 0.8050\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4871 - accuracy: 0.8059 - val_loss: 0.5010 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5429 - accuracy: 0.7812\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.4851 - accuracy: 0.8107\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.8059 - val_loss: 0.5010 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4985 - accuracy: 0.8281\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4884 - accuracy: 0.8084\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.8093 - val_loss: 0.5004 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4595 - accuracy: 0.7812\n",
      "25/28 [=========================>....] - ETA: 0s - loss: 0.5009 - accuracy: 0.8006\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.8019 - val_loss: 0.4998 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6654 - accuracy: 0.7031\n",
      "26/28 [==========================>...] - ETA: 0s - loss: 0.5125 - accuracy: 0.8041\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5129 - accuracy: 0.8042 - val_loss: 0.4996 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6530 - accuracy: 0.6719\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5040 - accuracy: 0.7992\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.8008 - val_loss: 0.4998 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5562 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4787 - accuracy: 0.8048 - val_loss: 0.4993 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4630 - accuracy: 0.8438\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.8076 - val_loss: 0.4990 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4272 - accuracy: 0.8438\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.4821 - accuracy: 0.8053\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.8025 - val_loss: 0.4987 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4625 - accuracy: 0.8125\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4888 - accuracy: 0.8032\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.8036 - val_loss: 0.4985 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5897 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.8042 - val_loss: 0.4986 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5181 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5066 - accuracy: 0.8002 - val_loss: 0.4981 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4137 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.8076 - val_loss: 0.4978 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3686 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.8099 - val_loss: 0.4976 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4395 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.8042 - val_loss: 0.4976 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4830 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8019 - val_loss: 0.4974 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4826 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8071 - val_loss: 0.4971 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5897 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8019 - val_loss: 0.4968 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4534 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4778 - accuracy: 0.8076 - val_loss: 0.4964 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4765 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.8008 - val_loss: 0.4959 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4631 - accuracy: 0.8438\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.4917 - accuracy: 0.8032\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4888 - accuracy: 0.8054 - val_loss: 0.4957 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4432 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.8036 - val_loss: 0.4958 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5831 - accuracy: 0.7188\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.8054 - val_loss: 0.4955 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4694 - accuracy: 0.7969\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8048\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.8048 - val_loss: 0.4952 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5088 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8110 - val_loss: 0.4953 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4091 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8065 - val_loss: 0.4950 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5369 - accuracy: 0.7812\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.8076 - val_loss: 0.4946 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5422 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8042 - val_loss: 0.4944 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5764 - accuracy: 0.7656\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8036 - val_loss: 0.4942 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4943 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8048 - val_loss: 0.4940 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6365 - accuracy: 0.7031\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.8054 - val_loss: 0.4942 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4222 - accuracy: 0.8594\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8025 - val_loss: 0.4942 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4944 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.8042 - val_loss: 0.4939 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4635 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.8105 - val_loss: 0.4935 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5228 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8042 - val_loss: 0.4935 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.3886 - accuracy: 0.8750\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.8048 - val_loss: 0.4932 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4805 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.8059 - val_loss: 0.4928 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5647 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.8082 - val_loss: 0.4926 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5268 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.8110 - val_loss: 0.4927 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5219 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.8071 - val_loss: 0.4924 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5914 - accuracy: 0.7500\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8008 - val_loss: 0.4921 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5003 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8099 - val_loss: 0.4918 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5209 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.8099 - val_loss: 0.4919 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4856 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.8184 - val_loss: 0.4919 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5001 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4783 - accuracy: 0.8071 - val_loss: 0.4914 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5026 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8054 - val_loss: 0.4911 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4425 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8059 - val_loss: 0.4909 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4301 - accuracy: 0.8438\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.8082 - val_loss: 0.4906 - val_accuracy: 0.7755\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4597 - accuracy: 0.8281\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.8048 - val_loss: 0.4908 - val_accuracy: 0.7755\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 3.0, 'hidden_units': 32.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00017229601625042316, 'type': 'sgd'}}, logloss: 0.4908\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 80%|███████▏ | 8/10 [01:51<00:30, 15.15s/trial, best loss: 0.39924007248637094]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 12s - loss: 0.6999 - accuracy: 0.5625\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7136 - accuracy: 0.4341 \n",
      "28/28 [==============================] - 1s 7ms/step - loss: 0.7141 - accuracy: 0.4314 - val_loss: 0.7067 - val_accuracy: 0.4337\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7228 - accuracy: 0.3750\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7127 - accuracy: 0.4332\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7129 - accuracy: 0.4303 - val_loss: 0.7043 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7083 - accuracy: 0.4219\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.7086 - accuracy: 0.4606\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7085 - accuracy: 0.4616 - val_loss: 0.7020 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7332 - accuracy: 0.3281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.7099 - accuracy: 0.4420\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7100 - accuracy: 0.4405 - val_loss: 0.6997 - val_accuracy: 0.4745\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6919 - accuracy: 0.4844\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7039 - accuracy: 0.4666\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7020 - accuracy: 0.4741 - val_loss: 0.6974 - val_accuracy: 0.4694\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7109 - accuracy: 0.4688\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.7036 - accuracy: 0.4844\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7034 - accuracy: 0.4866 - val_loss: 0.6951 - val_accuracy: 0.4949\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6942 - accuracy: 0.4844\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.7016 - accuracy: 0.4980\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.7013 - accuracy: 0.4929 - val_loss: 0.6929 - val_accuracy: 0.5051\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7124 - accuracy: 0.3906\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.7000 - accuracy: 0.4901\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6993 - accuracy: 0.4946 - val_loss: 0.6907 - val_accuracy: 0.5255\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7057 - accuracy: 0.4844\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6927 - accuracy: 0.5258\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.5344 - val_loss: 0.6886 - val_accuracy: 0.5408\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6923 - accuracy: 0.5156\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6928 - accuracy: 0.5266\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6924 - accuracy: 0.5299 - val_loss: 0.6864 - val_accuracy: 0.5510\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6776 - accuracy: 0.6094\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6878 - accuracy: 0.5611\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5618 - val_loss: 0.6844 - val_accuracy: 0.5765\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6827 - accuracy: 0.5469\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6881 - accuracy: 0.5455\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6881 - accuracy: 0.5373 - val_loss: 0.6823 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6845 - accuracy: 0.6406\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6826 - accuracy: 0.5803\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5766 - val_loss: 0.6803 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.7024 - accuracy: 0.4844\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6785 - accuracy: 0.6016\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.6090 - val_loss: 0.6783 - val_accuracy: 0.6224\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6676 - accuracy: 0.6406\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6741 - accuracy: 0.6207\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.6170 - val_loss: 0.6764 - val_accuracy: 0.6173\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6921 - accuracy: 0.4531\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6766 - accuracy: 0.6073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6747 - accuracy: 0.6118 - val_loss: 0.6745 - val_accuracy: 0.6429\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6911 - accuracy: 0.4844\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6726 - accuracy: 0.6338\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.6375 - val_loss: 0.6726 - val_accuracy: 0.6480\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6807 - accuracy: 0.6406\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6709 - accuracy: 0.6155\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.6124 - val_loss: 0.6708 - val_accuracy: 0.6582\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6721 - accuracy: 0.5469\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6696 - accuracy: 0.6352\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.6283 - val_loss: 0.6690 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6533 - accuracy: 0.7188\n",
      "23/28 [=======================>......] - ETA: 0s - loss: 0.6641 - accuracy: 0.6556\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6657 - accuracy: 0.6505 - val_loss: 0.6671 - val_accuracy: 0.6684\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6586 - accuracy: 0.7500\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6634 - accuracy: 0.6626\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6648 - accuracy: 0.6631 - val_loss: 0.6654 - val_accuracy: 0.6735\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6563 - accuracy: 0.7344\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6622 - accuracy: 0.6669\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.6596 - val_loss: 0.6636 - val_accuracy: 0.6888\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6502 - accuracy: 0.6719\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6600 - accuracy: 0.6697\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.6682 - val_loss: 0.6619 - val_accuracy: 0.6990\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6789 - accuracy: 0.5469\n",
      "24/28 [========================>.....] - ETA: 0s - loss: 0.6573 - accuracy: 0.6823\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.6887 - val_loss: 0.6603 - val_accuracy: 0.7092\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6729 - accuracy: 0.5781\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6555 - accuracy: 0.6853\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6562 - accuracy: 0.6836 - val_loss: 0.6586 - val_accuracy: 0.7143\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6510 - accuracy: 0.7344\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6561 - accuracy: 0.6957\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.6955 - val_loss: 0.6569 - val_accuracy: 0.7245\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6478 - accuracy: 0.7188\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6508 - accuracy: 0.7159\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6502 - accuracy: 0.7171 - val_loss: 0.6553 - val_accuracy: 0.7194\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6675 - accuracy: 0.7031\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6501 - accuracy: 0.7081\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 0.7154 - val_loss: 0.6537 - val_accuracy: 0.7194\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6656 - accuracy: 0.7031\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6490 - accuracy: 0.7247\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6469 - accuracy: 0.7325 - val_loss: 0.6522 - val_accuracy: 0.7245\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6474 - accuracy: 0.7031\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6449 - accuracy: 0.7266\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6450 - accuracy: 0.7325 - val_loss: 0.6506 - val_accuracy: 0.7296\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6544 - accuracy: 0.6719\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6417 - accuracy: 0.7344\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6449 - accuracy: 0.7234 - val_loss: 0.6490 - val_accuracy: 0.7398\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6521 - accuracy: 0.6875\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6383 - accuracy: 0.7391\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6410 - accuracy: 0.7359 - val_loss: 0.6475 - val_accuracy: 0.7398\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6538 - accuracy: 0.6094\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6396 - accuracy: 0.7508\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6400 - accuracy: 0.7490 - val_loss: 0.6460 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6434 - accuracy: 0.7656\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6362 - accuracy: 0.7522\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.7524 - val_loss: 0.6446 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6426 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6366 - accuracy: 0.7484\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6365 - accuracy: 0.7473 - val_loss: 0.6431 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6503 - accuracy: 0.6875\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6352 - accuracy: 0.7508\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6334 - accuracy: 0.7547 - val_loss: 0.6417 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6163 - accuracy: 0.7812\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6282 - accuracy: 0.7783\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.7758 - val_loss: 0.6403 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6372 - accuracy: 0.7344\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6288 - accuracy: 0.7746\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6314 - accuracy: 0.7684 - val_loss: 0.6389 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6138 - accuracy: 0.8281\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6307 - accuracy: 0.7594\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6295 - accuracy: 0.7666 - val_loss: 0.6376 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5961 - accuracy: 0.8594\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6271 - accuracy: 0.7720\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6275 - accuracy: 0.7723 - val_loss: 0.6362 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6457 - accuracy: 0.7031\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6270 - accuracy: 0.7628\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6254 - accuracy: 0.7678 - val_loss: 0.6349 - val_accuracy: 0.7653\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6339 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6292 - accuracy: 0.7586\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6268 - accuracy: 0.7706 - val_loss: 0.6336 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6319 - accuracy: 0.7969\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6239 - accuracy: 0.7771\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6246 - accuracy: 0.7792 - val_loss: 0.6324 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6032 - accuracy: 0.8281\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.6242 - accuracy: 0.7784\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.6224 - accuracy: 0.7803 - val_loss: 0.6311 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6202 - accuracy: 0.7656\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6211 - accuracy: 0.7820\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6181 - accuracy: 0.7860 - val_loss: 0.6299 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6264 - accuracy: 0.7188\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6143 - accuracy: 0.7961\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6153 - accuracy: 0.7849 - val_loss: 0.6286 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6224 - accuracy: 0.7969\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6160 - accuracy: 0.7936\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6173 - accuracy: 0.7877 - val_loss: 0.6274 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6293 - accuracy: 0.7031\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6154 - accuracy: 0.7862\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6165 - accuracy: 0.7814 - val_loss: 0.6262 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6320 - accuracy: 0.6875\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6159 - accuracy: 0.7781\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6141 - accuracy: 0.7883 - val_loss: 0.6251 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6345 - accuracy: 0.7812\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.6212 - accuracy: 0.7684\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6119 - accuracy: 0.7917 - val_loss: 0.6239 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6010 - accuracy: 0.7812\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6121 - accuracy: 0.7992\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6123 - accuracy: 0.8025 - val_loss: 0.6228 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6143 - accuracy: 0.7188\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.6093 - accuracy: 0.7960\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6091 - accuracy: 0.7945 - val_loss: 0.6216 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5992 - accuracy: 0.8281\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.6112 - accuracy: 0.7925\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6088 - accuracy: 0.7945 - val_loss: 0.6205 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6434 - accuracy: 0.7031\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.6037 - accuracy: 0.7995\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6061 - accuracy: 0.7991 - val_loss: 0.6194 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5794 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6041 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6051 - accuracy: 0.7980 - val_loss: 0.6183 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 56/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6294 - accuracy: 0.7344\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.6010 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6039 - accuracy: 0.7974 - val_loss: 0.6172 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 57/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6257 - accuracy: 0.7500\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.6003 - accuracy: 0.8082\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6055 - accuracy: 0.7962 - val_loss: 0.6162 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 58/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5764 - accuracy: 0.8750\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5966 - accuracy: 0.8086\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5996 - accuracy: 0.8019 - val_loss: 0.6152 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 59/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5770 - accuracy: 0.8750\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.6025 - accuracy: 0.8056\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.6013 - accuracy: 0.8042 - val_loss: 0.6142 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 60/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5946 - accuracy: 0.7969\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5981 - accuracy: 0.8003\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.8059 - val_loss: 0.6132 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 61/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5881 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5969 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.8031 - val_loss: 0.6122 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 62/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6159 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.6003 - accuracy: 0.7976\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.8042 - val_loss: 0.6112 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 63/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5960 - accuracy: 0.7969\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5897 - accuracy: 0.8088\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5938 - accuracy: 0.8019 - val_loss: 0.6102 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 64/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5890 - accuracy: 0.8438\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5914 - accuracy: 0.8109\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.8048 - val_loss: 0.6093 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 65/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6095 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.6012 - accuracy: 0.7922\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5955 - accuracy: 0.8042 - val_loss: 0.6084 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 66/200                                                                    \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5982 - accuracy: 0.7344\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5866 - accuracy: 0.8117\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5903 - accuracy: 0.8042 - val_loss: 0.6074 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 67/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6103 - accuracy: 0.7812\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5847 - accuracy: 0.8151\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5891 - accuracy: 0.8019 - val_loss: 0.6065 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 68/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5612 - accuracy: 0.8438\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5854 - accuracy: 0.8088\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5866 - accuracy: 0.8042 - val_loss: 0.6056 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 69/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5962 - accuracy: 0.8125\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5875 - accuracy: 0.8064\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5881 - accuracy: 0.8025 - val_loss: 0.6047 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 70/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5615 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5871 - accuracy: 0.8026\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5861 - accuracy: 0.8054 - val_loss: 0.6039 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 71/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6024 - accuracy: 0.7812\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5834 - accuracy: 0.8134\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5850 - accuracy: 0.8065 - val_loss: 0.6030 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 72/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6115 - accuracy: 0.7500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5860 - accuracy: 0.8018\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5845 - accuracy: 0.8065 - val_loss: 0.6022 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 73/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5959 - accuracy: 0.7656\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5805 - accuracy: 0.8109\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5832 - accuracy: 0.8059 - val_loss: 0.6013 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 74/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5943 - accuracy: 0.7656\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5837 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5811 - accuracy: 0.8065 - val_loss: 0.6005 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 75/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5532 - accuracy: 0.8594\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5752 - accuracy: 0.8142\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5804 - accuracy: 0.8059 - val_loss: 0.5997 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 76/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6109 - accuracy: 0.7344\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5796 - accuracy: 0.8088\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5795 - accuracy: 0.8082 - val_loss: 0.5989 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 77/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5851 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5794 - accuracy: 0.8043\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.8071 - val_loss: 0.5981 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 78/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5666 - accuracy: 0.8438\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5830 - accuracy: 0.8065\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5808 - accuracy: 0.8071 - val_loss: 0.5973 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 79/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5700 - accuracy: 0.8281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5735 - accuracy: 0.8140\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5770 - accuracy: 0.8065 - val_loss: 0.5965 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 80/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5709 - accuracy: 0.8281\n",
      "16/28 [================>.............] - ETA: 0s - loss: 0.5731 - accuracy: 0.8096\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5761 - accuracy: 0.8065 - val_loss: 0.5958 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 81/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6054 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5739 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5741 - accuracy: 0.8076 - val_loss: 0.5950 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 82/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6005 - accuracy: 0.7656\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5701 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.8036 - val_loss: 0.5942 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 83/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5439 - accuracy: 0.8438\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5757 - accuracy: 0.8054\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5721 - accuracy: 0.8076 - val_loss: 0.5935 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 84/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6015 - accuracy: 0.7812\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5716 - accuracy: 0.8097\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.8088 - val_loss: 0.5928 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 85/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5563 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5720 - accuracy: 0.8132\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.8065 - val_loss: 0.5921 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 86/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5894 - accuracy: 0.7656\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5741 - accuracy: 0.8031\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5721 - accuracy: 0.8065 - val_loss: 0.5914 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 87/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5426 - accuracy: 0.8594\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5741 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5726 - accuracy: 0.8076 - val_loss: 0.5907 - val_accuracy: 0.7704\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5884 - accuracy: 0.7812\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5654 - accuracy: 0.8111\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5685 - accuracy: 0.8082 - val_loss: 0.5900 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 89/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5756 - accuracy: 0.8125\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5642 - accuracy: 0.8133\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.8082 - val_loss: 0.5893 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 90/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5360 - accuracy: 0.8438\n",
      "22/28 [======================>.......] - ETA: 0s - loss: 0.5676 - accuracy: 0.8089\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5684 - accuracy: 0.8076 - val_loss: 0.5887 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 91/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5694 - accuracy: 0.7969\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5672 - accuracy: 0.8070\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5676 - accuracy: 0.8082 - val_loss: 0.5880 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 92/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5647 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5651 - accuracy: 0.8147\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5697 - accuracy: 0.8082 - val_loss: 0.5874 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 93/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5880 - accuracy: 0.7812\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5660 - accuracy: 0.8021\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.8082 - val_loss: 0.5867 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 94/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5670 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5657 - accuracy: 0.8103\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.8082 - val_loss: 0.5861 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 95/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5265 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5681 - accuracy: 0.7985\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5646 - accuracy: 0.8082 - val_loss: 0.5855 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 96/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5600 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5654 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5645 - accuracy: 0.8065 - val_loss: 0.5848 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 97/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5124 - accuracy: 0.8906\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5579 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5602 - accuracy: 0.8082 - val_loss: 0.5842 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 98/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5470 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5570 - accuracy: 0.8166\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5619 - accuracy: 0.8082 - val_loss: 0.5836 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 99/200                                                                    \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5481 - accuracy: 0.8438\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5603 - accuracy: 0.8064\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.8082 - val_loss: 0.5830 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 100/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5315 - accuracy: 0.8594\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5641 - accuracy: 0.8030\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5604 - accuracy: 0.8082 - val_loss: 0.5824 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 101/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5919 - accuracy: 0.7500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5567 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5578 - accuracy: 0.8076 - val_loss: 0.5819 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 102/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5297 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5582 - accuracy: 0.8043\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5572 - accuracy: 0.8082 - val_loss: 0.5813 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 103/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5392 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5595 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5581 - accuracy: 0.8076 - val_loss: 0.5807 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 104/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5955 - accuracy: 0.7656\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5620 - accuracy: 0.7985\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5571 - accuracy: 0.8082 - val_loss: 0.5802 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 105/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5529 - accuracy: 0.8281\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5483 - accuracy: 0.8183\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5553 - accuracy: 0.8082 - val_loss: 0.5796 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 106/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5932 - accuracy: 0.7344\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5499 - accuracy: 0.8160\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5544 - accuracy: 0.8082 - val_loss: 0.5791 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 107/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5213 - accuracy: 0.8750\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5617 - accuracy: 0.8008\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5540 - accuracy: 0.8082 - val_loss: 0.5785 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 108/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5460 - accuracy: 0.8281\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5499 - accuracy: 0.8168\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5534 - accuracy: 0.8082 - val_loss: 0.5780 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 109/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5486 - accuracy: 0.8125\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5543 - accuracy: 0.8064\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5528 - accuracy: 0.8076 - val_loss: 0.5775 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 110/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5707 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5527 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5508 - accuracy: 0.8082 - val_loss: 0.5770 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 111/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6267 - accuracy: 0.7031\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5607 - accuracy: 0.7960\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5511 - accuracy: 0.8076 - val_loss: 0.5765 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 112/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5204 - accuracy: 0.8438\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5574 - accuracy: 0.7987\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5524 - accuracy: 0.8082 - val_loss: 0.5760 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 113/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5655 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5533 - accuracy: 0.7993\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5479 - accuracy: 0.8082 - val_loss: 0.5755 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 114/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5288 - accuracy: 0.8438\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5490 - accuracy: 0.8134\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5511 - accuracy: 0.8082 - val_loss: 0.5750 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 115/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5451 - accuracy: 0.7812\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5544 - accuracy: 0.8015\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 0.5493 - accuracy: 0.8082 - val_loss: 0.5745 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 116/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5583 - accuracy: 0.7812\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5482 - accuracy: 0.8116\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5486 - accuracy: 0.8082 - val_loss: 0.5740 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 117/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5659 - accuracy: 0.7656\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5565 - accuracy: 0.7969\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5501 - accuracy: 0.8082 - val_loss: 0.5736 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 118/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6099 - accuracy: 0.7344\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5461 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5496 - accuracy: 0.8088 - val_loss: 0.5731 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 119/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5221 - accuracy: 0.8125\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5556 - accuracy: 0.7986\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5472 - accuracy: 0.8082 - val_loss: 0.5726 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 120/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5239 - accuracy: 0.8125\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5425 - accuracy: 0.8078\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5437 - accuracy: 0.8076 - val_loss: 0.5722 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 121/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5326 - accuracy: 0.8281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5460 - accuracy: 0.8065\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5447 - accuracy: 0.8082 - val_loss: 0.5717 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 122/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5621 - accuracy: 0.7812\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5587 - accuracy: 0.7917\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5476 - accuracy: 0.8082 - val_loss: 0.5713 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 123/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5208 - accuracy: 0.8594\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5480 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5452 - accuracy: 0.8082 - val_loss: 0.5708 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 124/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5051 - accuracy: 0.8750\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5468 - accuracy: 0.8015\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5442 - accuracy: 0.8082 - val_loss: 0.5704 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 125/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5516 - accuracy: 0.7969\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5368 - accuracy: 0.8151\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5426 - accuracy: 0.8082 - val_loss: 0.5700 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 126/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5577 - accuracy: 0.7969\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5397 - accuracy: 0.8118\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.8082 - val_loss: 0.5696 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 127/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4387 - accuracy: 0.9219\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5431 - accuracy: 0.8088\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5432 - accuracy: 0.8082 - val_loss: 0.5691 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 128/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5507 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5414 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.8082 - val_loss: 0.5687 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 129/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4690 - accuracy: 0.9062\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5375 - accuracy: 0.8117\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5421 - accuracy: 0.8082 - val_loss: 0.5683 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 130/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5189 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5282 - accuracy: 0.8150\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5366 - accuracy: 0.8082 - val_loss: 0.5679 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 131/200                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5077 - accuracy: 0.8438\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5405 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.8082 - val_loss: 0.5675 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 132/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5852 - accuracy: 0.7969\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5401 - accuracy: 0.8028\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5368 - accuracy: 0.8082 - val_loss: 0.5671 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 133/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5842 - accuracy: 0.7500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5396 - accuracy: 0.8110\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5402 - accuracy: 0.8088 - val_loss: 0.5667 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 134/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5935 - accuracy: 0.7500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5479 - accuracy: 0.7944\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.8082 - val_loss: 0.5663 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 135/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5524 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5406 - accuracy: 0.8058\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.8082 - val_loss: 0.5660 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 136/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5105 - accuracy: 0.8438\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5375 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5357 - accuracy: 0.8082 - val_loss: 0.5656 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 137/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5792 - accuracy: 0.7656\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5405 - accuracy: 0.8043\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5364 - accuracy: 0.8082 - val_loss: 0.5652 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 138/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5322 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5346 - accuracy: 0.8100\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5371 - accuracy: 0.8082 - val_loss: 0.5648 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 139/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5502 - accuracy: 0.7969\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5369 - accuracy: 0.8064\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5359 - accuracy: 0.8082 - val_loss: 0.5645 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 140/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5331 - accuracy: 0.7969\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5345 - accuracy: 0.8108\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5359 - accuracy: 0.8082 - val_loss: 0.5641 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 141/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5068 - accuracy: 0.8281\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5311 - accuracy: 0.8150\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5345 - accuracy: 0.8082 - val_loss: 0.5638 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 142/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5945 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5386 - accuracy: 0.8070\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5375 - accuracy: 0.8082 - val_loss: 0.5634 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 143/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5683 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5267 - accuracy: 0.8158\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5336 - accuracy: 0.8082 - val_loss: 0.5631 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 144/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5451 - accuracy: 0.7656\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5358 - accuracy: 0.8003\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5289 - accuracy: 0.8082 - val_loss: 0.5627 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 145/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5188 - accuracy: 0.8281\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5276 - accuracy: 0.8158\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5340 - accuracy: 0.8082 - val_loss: 0.5624 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 146/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5609 - accuracy: 0.7500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5469 - accuracy: 0.7919\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5349 - accuracy: 0.8082 - val_loss: 0.5621 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 147/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4303 - accuracy: 0.9219\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5325 - accuracy: 0.8094\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.8082 - val_loss: 0.5617 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 148/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4982 - accuracy: 0.8594\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5350 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.8082 - val_loss: 0.5614 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 149/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5587 - accuracy: 0.7812\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5447 - accuracy: 0.7930\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5304 - accuracy: 0.8082 - val_loss: 0.5611 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 150/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5460 - accuracy: 0.7969\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5309 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.8082 - val_loss: 0.5608 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 151/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6165 - accuracy: 0.7188\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5336 - accuracy: 0.8067\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5320 - accuracy: 0.8082 - val_loss: 0.5605 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 152/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5550 - accuracy: 0.7969\n",
      "17/28 [=================>............] - ETA: 0s - loss: 0.5339 - accuracy: 0.8042\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5290 - accuracy: 0.8082 - val_loss: 0.5602 - val_accuracy: 0.7704\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6119 - accuracy: 0.7031\n",
      "18/28 [==================>...........] - ETA: 0s - loss: 0.5305 - accuracy: 0.8064\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5321 - accuracy: 0.8082 - val_loss: 0.5599 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 154/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5232 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5297 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5314 - accuracy: 0.8082 - val_loss: 0.5596 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 155/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5159 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5404 - accuracy: 0.7984\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5309 - accuracy: 0.8082 - val_loss: 0.5593 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 156/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5705 - accuracy: 0.7500\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5347 - accuracy: 0.8028\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5312 - accuracy: 0.8082 - val_loss: 0.5590 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 157/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5252 - accuracy: 0.8281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5255 - accuracy: 0.8088\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5270 - accuracy: 0.8082 - val_loss: 0.5587 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 158/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5605 - accuracy: 0.7656\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5307 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5279 - accuracy: 0.8082 - val_loss: 0.5584 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 159/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4995 - accuracy: 0.8281\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5235 - accuracy: 0.8195\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5290 - accuracy: 0.8082 - val_loss: 0.5581 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 160/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5675 - accuracy: 0.7656\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5297 - accuracy: 0.8094\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5301 - accuracy: 0.8082 - val_loss: 0.5578 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 161/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4842 - accuracy: 0.8594\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5299 - accuracy: 0.8036\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.8082 - val_loss: 0.5575 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 162/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5563 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5243 - accuracy: 0.8133\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5288 - accuracy: 0.8082 - val_loss: 0.5572 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 163/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5803 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5321 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5274 - accuracy: 0.8082 - val_loss: 0.5569 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 164/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5402 - accuracy: 0.7969\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5265 - accuracy: 0.8086\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.8082 - val_loss: 0.5567 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 165/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5320 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5201 - accuracy: 0.8158\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5264 - accuracy: 0.8082 - val_loss: 0.5564 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 166/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4492 - accuracy: 0.9062\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5331 - accuracy: 0.7961\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5250 - accuracy: 0.8082 - val_loss: 0.5561 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 167/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5110 - accuracy: 0.8125\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5270 - accuracy: 0.8080\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5258 - accuracy: 0.8082 - val_loss: 0.5559 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 168/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6087 - accuracy: 0.7031\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5296 - accuracy: 0.8023\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5237 - accuracy: 0.8082 - val_loss: 0.5556 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 169/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4520 - accuracy: 0.8906\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5264 - accuracy: 0.8035\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5216 - accuracy: 0.8082 - val_loss: 0.5553 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 170/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4503 - accuracy: 0.8906\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5169 - accuracy: 0.8164\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5247 - accuracy: 0.8082 - val_loss: 0.5551 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 171/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4840 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5150 - accuracy: 0.8199\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5239 - accuracy: 0.8082 - val_loss: 0.5548 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 172/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4411 - accuracy: 0.9062\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5239 - accuracy: 0.8073\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.8082 - val_loss: 0.5546 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 173/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5123 - accuracy: 0.8125\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5226 - accuracy: 0.8078\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5235 - accuracy: 0.8082 - val_loss: 0.5543 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 174/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5672 - accuracy: 0.7812\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5224 - accuracy: 0.8062\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5222 - accuracy: 0.8082 - val_loss: 0.5541 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 175/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4954 - accuracy: 0.8281\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5230 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.8082 - val_loss: 0.5538 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 176/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5701 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5064 - accuracy: 0.8258\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.8082 - val_loss: 0.5536 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 177/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5666 - accuracy: 0.7812\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5249 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5206 - accuracy: 0.8082 - val_loss: 0.5534 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 178/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5717 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5223 - accuracy: 0.8070\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5205 - accuracy: 0.8082 - val_loss: 0.5531 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 179/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5669 - accuracy: 0.7500\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5260 - accuracy: 0.7977\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5184 - accuracy: 0.8082 - val_loss: 0.5529 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 180/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5552 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5227 - accuracy: 0.8059\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5195 - accuracy: 0.8082 - val_loss: 0.5527 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 181/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5518 - accuracy: 0.7812\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5149 - accuracy: 0.8164\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5229 - accuracy: 0.8082 - val_loss: 0.5524 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 182/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5065 - accuracy: 0.8281\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5146 - accuracy: 0.8156\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.8082 - val_loss: 0.5522 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 183/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5574 - accuracy: 0.7969\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5122 - accuracy: 0.8166\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5218 - accuracy: 0.8082 - val_loss: 0.5520 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 184/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5568 - accuracy: 0.7500\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5134 - accuracy: 0.8125\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5190 - accuracy: 0.8082 - val_loss: 0.5517 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 185/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6162 - accuracy: 0.7344\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5182 - accuracy: 0.8092\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5211 - accuracy: 0.8082 - val_loss: 0.5515 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 186/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5613 - accuracy: 0.7656\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5247 - accuracy: 0.8055\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5209 - accuracy: 0.8082 - val_loss: 0.5513 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 187/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5141 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5168 - accuracy: 0.8100\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5182 - accuracy: 0.8082 - val_loss: 0.5511 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 188/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4525 - accuracy: 0.8906\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5086 - accuracy: 0.8133\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5164 - accuracy: 0.8082 - val_loss: 0.5509 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 189/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5117 - accuracy: 0.8281\n",
      "21/28 [=====================>........] - ETA: 0s - loss: 0.5184 - accuracy: 0.8103\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5210 - accuracy: 0.8082 - val_loss: 0.5507 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 190/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5180 - accuracy: 0.7969\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5155 - accuracy: 0.8117\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5171 - accuracy: 0.8082 - val_loss: 0.5505 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 191/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5980 - accuracy: 0.7031\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5251 - accuracy: 0.7993\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.8082 - val_loss: 0.5503 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 192/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5318 - accuracy: 0.8125\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5246 - accuracy: 0.8051\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5207 - accuracy: 0.8082 - val_loss: 0.5501 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 193/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4773 - accuracy: 0.8594\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5193 - accuracy: 0.8086\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5183 - accuracy: 0.8082 - val_loss: 0.5499 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 194/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5296 - accuracy: 0.7812\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5167 - accuracy: 0.8076\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5169 - accuracy: 0.8082 - val_loss: 0.5497 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 195/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4320 - accuracy: 0.8750\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5046 - accuracy: 0.8172\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5126 - accuracy: 0.8082 - val_loss: 0.5494 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 196/200                                                                   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4451 - accuracy: 0.8594\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5109 - accuracy: 0.8141\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5142 - accuracy: 0.8082 - val_loss: 0.5493 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 197/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6234 - accuracy: 0.6875\n",
      "19/28 [===================>..........] - ETA: 0s - loss: 0.5149 - accuracy: 0.8100\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5172 - accuracy: 0.8082 - val_loss: 0.5491 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 198/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.6194 - accuracy: 0.7344\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5138 - accuracy: 0.8102\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5180 - accuracy: 0.8082 - val_loss: 0.5489 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 199/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.4716 - accuracy: 0.8438\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5180 - accuracy: 0.8047\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5148 - accuracy: 0.8082 - val_loss: 0.5487 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 200/200                                                                   \n",
      "\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.5161 - accuracy: 0.8125\n",
      "20/28 [====================>.........] - ETA: 0s - loss: 0.5137 - accuracy: 0.8102\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.5139 - accuracy: 0.8082 - val_loss: 0.5485 - val_accuracy: 0.7704\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.1, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 1.0559414881608205e-05, 'type': 'sgd'}}, logloss: 0.5485\n",
      "Epoch 1/200                                                                     \n",
      "\n",
      " 90%|████████ | 9/10 [02:13<00:17, 17.31s/trial, best loss: 0.39924007248637094]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yongchanchun/opt/anaconda3/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 13s - loss: 0.6612 - accuracy: 0.6250\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.5950 - accuracy: 0.7188 \n",
      "14/14 [==============================] - 1s 19ms/step - loss: 0.5910 - accuracy: 0.7302 - val_loss: 0.6311 - val_accuracy: 0.7704\n",
      "\n",
      "Epoch 2/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.6063 - accuracy: 0.7969\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.5257 - accuracy: 0.8004\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5215 - accuracy: 0.8014 - val_loss: 0.6171 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 3/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5283 - accuracy: 0.7422\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4769 - accuracy: 0.7976\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4875 - accuracy: 0.7928 - val_loss: 0.5739 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 4/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4535 - accuracy: 0.8047\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4692 - accuracy: 0.8004\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4647 - accuracy: 0.8054 - val_loss: 0.5393 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 5/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3913 - accuracy: 0.8594\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4538 - accuracy: 0.8097\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4491 - accuracy: 0.8116 - val_loss: 0.5110 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 6/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4142 - accuracy: 0.8359\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4380 - accuracy: 0.8139\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4452 - accuracy: 0.8156 - val_loss: 0.4874 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 7/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4859 - accuracy: 0.7969\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4543 - accuracy: 0.8139\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4493 - accuracy: 0.8173 - val_loss: 0.4731 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 8/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4204 - accuracy: 0.8359\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.4400 - accuracy: 0.8216\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4411 - accuracy: 0.8190 - val_loss: 0.4633 - val_accuracy: 0.7959\n",
      "\n",
      "Epoch 9/200                                                                     \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4152 - accuracy: 0.8125\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.4342 - accuracy: 0.8188\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.4344 - accuracy: 0.8179 - val_loss: 0.4559 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 10/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3909 - accuracy: 0.8438\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4239 - accuracy: 0.8246\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4170 - accuracy: 0.8247 - val_loss: 0.4470 - val_accuracy: 0.7908\n",
      "\n",
      "Epoch 11/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4899 - accuracy: 0.7891\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4166 - accuracy: 0.8345\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4137 - accuracy: 0.8338 - val_loss: 0.4397 - val_accuracy: 0.8010\n",
      "\n",
      "Epoch 12/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4706 - accuracy: 0.7891\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4422 - accuracy: 0.8175\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4333 - accuracy: 0.8224 - val_loss: 0.4352 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 13/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4647 - accuracy: 0.7969\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.4023 - accuracy: 0.8385\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4086 - accuracy: 0.8361 - val_loss: 0.4298 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 14/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3429 - accuracy: 0.8828\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4109 - accuracy: 0.8324\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4141 - accuracy: 0.8310 - val_loss: 0.4265 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 15/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8594\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.4104 - accuracy: 0.8288\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4110 - accuracy: 0.8293 - val_loss: 0.4246 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 16/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4482 - accuracy: 0.8125\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4193 - accuracy: 0.8253\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4108 - accuracy: 0.8310 - val_loss: 0.4231 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 17/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4265 - accuracy: 0.7891\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.4161 - accuracy: 0.8219\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.4088 - accuracy: 0.8298 - val_loss: 0.4203 - val_accuracy: 0.8367\n",
      "\n",
      "Epoch 18/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3228 - accuracy: 0.9062\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.3820 - accuracy: 0.8484\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.4093 - accuracy: 0.8355 - val_loss: 0.4187 - val_accuracy: 0.8316\n",
      "\n",
      "Epoch 19/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3924 - accuracy: 0.8359\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.4036 - accuracy: 0.8367\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.4015 - accuracy: 0.8401 - val_loss: 0.4182 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 20/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3882 - accuracy: 0.8516\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.3989 - accuracy: 0.8477\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.4014 - accuracy: 0.8412 - val_loss: 0.4173 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 21/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.2998 - accuracy: 0.9062\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3796 - accuracy: 0.8551\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3920 - accuracy: 0.8458 - val_loss: 0.4161 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 22/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3000 - accuracy: 0.9141\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3807 - accuracy: 0.8523\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3858 - accuracy: 0.8475 - val_loss: 0.4153 - val_accuracy: 0.8214\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3770 - accuracy: 0.8438\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3794 - accuracy: 0.8473\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.3886 - accuracy: 0.8406 - val_loss: 0.4153 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 24/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4742 - accuracy: 0.7969\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.4008 - accuracy: 0.8367\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4003 - accuracy: 0.8384 - val_loss: 0.4163 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 25/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3965 - accuracy: 0.8594\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.4039 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3922 - accuracy: 0.8412 - val_loss: 0.4167 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 26/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3142 - accuracy: 0.8672\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3859 - accuracy: 0.8459\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3952 - accuracy: 0.8423 - val_loss: 0.4160 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 27/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4047 - accuracy: 0.8359\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3909 - accuracy: 0.8416\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4022 - accuracy: 0.8355 - val_loss: 0.4162 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 28/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3561 - accuracy: 0.8516\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3999 - accuracy: 0.8409\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3963 - accuracy: 0.8401 - val_loss: 0.4152 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 29/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.5129 - accuracy: 0.7578\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.4072 - accuracy: 0.8267\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4004 - accuracy: 0.8321 - val_loss: 0.4158 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 30/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3843 - accuracy: 0.8359\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3833 - accuracy: 0.8459\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3962 - accuracy: 0.8372 - val_loss: 0.4136 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 31/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3273 - accuracy: 0.8828\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.3945 - accuracy: 0.8430\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3852 - accuracy: 0.8497 - val_loss: 0.4147 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 32/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4001 - accuracy: 0.8516\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3723 - accuracy: 0.8565\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3838 - accuracy: 0.8475 - val_loss: 0.4132 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 33/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3937 - accuracy: 0.8281\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3800 - accuracy: 0.8516\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.3902 - accuracy: 0.8463 - val_loss: 0.4131 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 34/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3052 - accuracy: 0.9062\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3907 - accuracy: 0.8501\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3870 - accuracy: 0.8503 - val_loss: 0.4129 - val_accuracy: 0.8265\n",
      "\n",
      "Epoch 35/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4330 - accuracy: 0.8281\n",
      "10/14 [====================>.........] - ETA: 0s - loss: 0.3753 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3816 - accuracy: 0.8441 - val_loss: 0.4125 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 36/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3622 - accuracy: 0.8516\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3897 - accuracy: 0.8445\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3931 - accuracy: 0.8429 - val_loss: 0.4132 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 37/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4518 - accuracy: 0.8438\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3906 - accuracy: 0.8438\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3849 - accuracy: 0.8469 - val_loss: 0.4137 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 38/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3966 - accuracy: 0.8359\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3860 - accuracy: 0.8359\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3901 - accuracy: 0.8349 - val_loss: 0.4153 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 39/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3933 - accuracy: 0.8281\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.3946 - accuracy: 0.8366\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3923 - accuracy: 0.8395 - val_loss: 0.4172 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 40/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4336 - accuracy: 0.8047\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3808 - accuracy: 0.8416\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3803 - accuracy: 0.8418 - val_loss: 0.4178 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 41/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4246 - accuracy: 0.8359\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.3726 - accuracy: 0.8490\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3731 - accuracy: 0.8486 - val_loss: 0.4190 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 42/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4256 - accuracy: 0.8281\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3891 - accuracy: 0.8452\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3840 - accuracy: 0.8463 - val_loss: 0.4188 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 43/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4099 - accuracy: 0.8281\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3728 - accuracy: 0.8445\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3777 - accuracy: 0.8423 - val_loss: 0.4179 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 44/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3761 - accuracy: 0.8125\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3794 - accuracy: 0.8452\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3832 - accuracy: 0.8446 - val_loss: 0.4167 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 45/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4194 - accuracy: 0.8203\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3832 - accuracy: 0.8445\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3757 - accuracy: 0.8497 - val_loss: 0.4166 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 46/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3821 - accuracy: 0.8438\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.3731 - accuracy: 0.8483\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3786 - accuracy: 0.8452 - val_loss: 0.4172 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 47/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3226 - accuracy: 0.8438\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3777 - accuracy: 0.8402\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3824 - accuracy: 0.8423 - val_loss: 0.4183 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 48/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3401 - accuracy: 0.8672\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3696 - accuracy: 0.8509\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3721 - accuracy: 0.8520 - val_loss: 0.4189 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 49/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4264 - accuracy: 0.8203\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3816 - accuracy: 0.8551\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3693 - accuracy: 0.8600 - val_loss: 0.4192 - val_accuracy: 0.8112\n",
      "\n",
      "Epoch 50/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3504 - accuracy: 0.8984\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3592 - accuracy: 0.8686\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3751 - accuracy: 0.8600 - val_loss: 0.4199 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 51/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4403 - accuracy: 0.7812\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3604 - accuracy: 0.8565\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3693 - accuracy: 0.8515 - val_loss: 0.4204 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 52/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3346 - accuracy: 0.8594\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3753 - accuracy: 0.8501\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3749 - accuracy: 0.8503 - val_loss: 0.4189 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 53/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4345 - accuracy: 0.8047\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3874 - accuracy: 0.8423\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3881 - accuracy: 0.8423 - val_loss: 0.4184 - val_accuracy: 0.8214\n",
      "\n",
      "Epoch 54/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.4121 - accuracy: 0.8359\n",
      "11/14 [======================>.......] - ETA: 0s - loss: 0.3846 - accuracy: 0.8452\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3781 - accuracy: 0.8469 - val_loss: 0.4180 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 55/200                                                                    \n",
      "\n",
      " 1/14 [=>............................] - ETA: 0s - loss: 0.3305 - accuracy: 0.8594\n",
      "12/14 [========================>.....] - ETA: 0s - loss: 0.3718 - accuracy: 0.8561\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.3723 - accuracy: 0.8549 - val_loss: 0.4196 - val_accuracy: 0.8163\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.006674274015281253, 'type': 'sgd'}}, logloss: 0.4125\n",
      "100%|████████| 10/10 [02:20<00:00, 14.04s/trial, best loss: 0.39924007248637094]\n",
      "best params:{'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.05, 'optimizer': {'lr': 7.9094022518724e-05, 'type': 'adam'}}, score:0.3992\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q6/vm_27f310vd292vc5pl2yrzw0000gn/T/ipykernel_91097/3527946358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m# 2021/04/27 - 전체 프로그램 실행 시간 확인용.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mtime_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from hyperopt import hp\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 기본이 되는 매개변수\n",
    "base_param = {\n",
    "    'input_dropout': 0.0,\n",
    "    'hidden_layers': 3,\n",
    "    'hidden_units': 96,\n",
    "    'hidden_activation': 'relu',\n",
    "    'hidden_dropout': 0.2,\n",
    "    'batch_norm': 'before_act',\n",
    "    'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "# 탐색할 매개변수 공간을 지정\n",
    "param_space = {\n",
    "    'input_dropout': hp.quniform('input_dropout', 0, 0.2, 0.05),\n",
    "    'hidden_layers': hp.quniform('hidden_layers', 2, 4, 1),\n",
    "    'hidden_units': hp.quniform('hidden_units', 32, 256, 32),\n",
    "    'hidden_activation': hp.choice('hidden_activation', ['prelu', 'relu']),\n",
    "    'hidden_dropout': hp.quniform('hidden_dropout', 0, 0.3, 0.05),\n",
    "    'batch_norm': hp.choice('batch_norm', ['before_act', 'no']),\n",
    "    'optimizer': hp.choice('optimizer',\n",
    "                           [{'type': 'adam',\n",
    "                             'lr': hp.loguniform('adam_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'sgd',\n",
    "                             'lr': hp.loguniform('sgd_lr', np.log(0.00001), np.log(0.01))}]),\n",
    "    'batch_size': hp.quniform('batch_size', 32, 128, 32),\n",
    "}\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "\n",
    "        # 매개변수\n",
    "        input_dropout = self.params['input_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "\n",
    "        # 표준화\n",
    "#         self.scaler = StandardScaler()\n",
    "#         tr_x = self.scaler.fit_transform(tr_x)\n",
    "#         va_x = self.scaler.transform(va_x)\n",
    "\n",
    "        self.model = Sequential()\n",
    "\n",
    "        # 입력계층\n",
    "        self.model.add(Dropout(input_dropout, input_shape=(tr_x.shape[1],)))\n",
    "\n",
    "        # 은닉계층\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 출력 계층\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # 최적화(옵티마이저)\n",
    "        if optimizer_type == 'sgd':\n",
    "            optimizer = SGD(lr=optimizer_lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, decay=0.)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 목적함수, 평가지표 등의 설정\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                           optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # 에폭 수, 조기 종료\n",
    "        # 에폭을 너무 크게 하면 작은 학습률일 때 끝나지 않을 수 있으므로 주의\n",
    "        nb_epoch = 200\n",
    "        patience = 20\n",
    "        early_stopping = EarlyStopping(patience=patience, restore_best_weights=True)\n",
    "\n",
    "        # 학습의 실행\n",
    "        history = self.model.fit(tr_x, tr_y,\n",
    "                                 epochs=nb_epoch,\n",
    "                                 batch_size=batch_size, verbose=1,\n",
    "                                 validation_data=(va_x, va_y),\n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 예측\n",
    "        # x = self.scaler.transform(x)\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred = y_pred.flatten()\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 매개변수 튜닝의 실행\n",
    "# -----------------------------------\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    # 매개변수 셋을 지정했을 때, 최소화해야 할 함수를 지정\n",
    "    # 모델의 매개변수 탐색에서는 모델에 매개변수를 지정하여 학습예측한 경우의 점수로 함\n",
    "    model = MLP(params)\n",
    "    model.fit(train_x, train_y, valid_x, valid_y)\n",
    "    valid_pred = model.predict(valid_x)\n",
    "    score = log_loss(valid_y, valid_pred)\n",
    "    print(f'params: {params}, logloss: {score:.4f}')\n",
    "\n",
    "    # 정보를 기록\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# hyperopt에 의한 매개변수 탐색의 실행\n",
    "max_evals = 10\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "\n",
    "# 기록한 정보에서 매개변수와 점수를 출력\n",
    "# trials에서도 정보를 취득할 수 있지만 매개변수를 취득하기 어려움\n",
    "history = sorted(history, key=lambda tpl: tpl[1])\n",
    "best = history[0]\n",
    "print(f'best params:{best[0]}, score:{best[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "87073c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>TypeofContact</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>DurationOfPitch</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Gender</th>\n",
       "      <th>NumberOfPersonVisiting</th>\n",
       "      <th>NumberOfFollowups</th>\n",
       "      <th>ProductPitched</th>\n",
       "      <th>PreferredPropertyStar</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfTrips</th>\n",
       "      <th>Passport</th>\n",
       "      <th>PitchSatisfactionScore</th>\n",
       "      <th>OwnCar</th>\n",
       "      <th>NumberOfChildrenVisiting</th>\n",
       "      <th>Designation</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19599.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21274.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>19907.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31595.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>22218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17853.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1564 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  TypeofContact  CityTier  DurationOfPitch  Occupation  Gender  \\\n",
       "id                                                                         \n",
       "1     28.0              0         1             10.0           3       2   \n",
       "2     34.0              1         3              NaN           3       1   \n",
       "3     45.0              0         1              NaN           2       2   \n",
       "4     29.0              0         1              7.0           3       2   \n",
       "5     42.0              1         3              6.0           2       2   \n",
       "...    ...            ...       ...              ...         ...     ...   \n",
       "1951  28.0              1         1             10.0           3       2   \n",
       "1952  41.0              1         3              8.0           2       1   \n",
       "1953  38.0              0         3             28.0           3       1   \n",
       "1954  28.0              1         3             30.0           3       1   \n",
       "1955  22.0              0         1              9.0           2       2   \n",
       "\n",
       "      NumberOfPersonVisiting  NumberOfFollowups  ProductPitched  \\\n",
       "id                                                                \n",
       "1                          3                4.0               0   \n",
       "2                          2                4.0               1   \n",
       "3                          2                3.0               1   \n",
       "4                          3                5.0               0   \n",
       "5                          2                3.0               1   \n",
       "...                      ...                ...             ...   \n",
       "1951                       3                5.0               0   \n",
       "1952                       3                3.0               4   \n",
       "1953                       3                4.0               0   \n",
       "1954                       3                5.0               1   \n",
       "1955                       2                4.0               0   \n",
       "\n",
       "      PreferredPropertyStar  MaritalStatus  NumberOfTrips  Passport  \\\n",
       "id                                                                    \n",
       "1                       3.0              1            3.0         0   \n",
       "2                       4.0              2            1.0         1   \n",
       "3                       4.0              1            2.0         0   \n",
       "4                       4.0              1            3.0         0   \n",
       "5                       3.0              0            2.0         0   \n",
       "...                     ...            ...            ...       ...   \n",
       "1951                    3.0              2            2.0         0   \n",
       "1952                    5.0              0            1.0         0   \n",
       "1953                    3.0              0            7.0         0   \n",
       "1954                    3.0              1            3.0         0   \n",
       "1955                    3.0              0            1.0         1   \n",
       "\n",
       "      PitchSatisfactionScore  OwnCar  NumberOfChildrenVisiting  Designation  \\\n",
       "id                                                                            \n",
       "1                          1       0                       1.0            1   \n",
       "2                          5       1                       0.0            2   \n",
       "3                          4       1                       0.0            2   \n",
       "4                          4       0                       1.0            1   \n",
       "5                          3       1                       0.0            2   \n",
       "...                      ...     ...                       ...          ...   \n",
       "1951                       1       1                       2.0            1   \n",
       "1952                       5       1                       1.0            0   \n",
       "1953                       2       1                       2.0            1   \n",
       "1954                       1       1                       2.0            2   \n",
       "1955                       3       0                       0.0            1   \n",
       "\n",
       "      MonthlyIncome  \n",
       "id                   \n",
       "1           20384.0  \n",
       "2           19599.0  \n",
       "3               NaN  \n",
       "4           21274.0  \n",
       "5           19907.0  \n",
       "...             ...  \n",
       "1951        20723.0  \n",
       "1952        31595.0  \n",
       "1953        21651.0  \n",
       "1954        22218.0  \n",
       "1955        17853.0  \n",
       "\n",
       "[1564 rows x 18 columns]"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "96ebb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_2 = pd.read_csv(os.path.join(file_path, train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "1076a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col:id, uniuqe: [   1    2    3 ... 1953 1954 1955]\n",
      "col:Age, uniuqe: [28. 34. 45. 29. 42. 32. 43. 36. 35. 31. 49. nan 52. 33. 22. 50. 23. 41.\n",
      " 37. 40. 56. 54. 39. 20. 46. 27. 38. 25. 26. 24. 30. 21. 51. 47. 55. 44.\n",
      " 53. 48. 18. 57. 60. 59. 19. 58. 61.]\n",
      "col:TypeofContact, uniuqe: ['Company Invited' 'Self Enquiry' nan]\n",
      "col:CityTier, uniuqe: [1 3 2]\n",
      "col:DurationOfPitch, uniuqe: [10. nan  7.  6. 29.  8. 20. 14.  9. 16. 15. 23. 21. 11. 25. 34. 17. 13.\n",
      " 28. 12. 22. 30. 24. 27. 31. 35. 32. 33. 36. 19. 26. 18.  5.]\n",
      "col:Occupation, uniuqe: ['Small Business' 'Salaried' 'Large Business' 'Free Lancer']\n",
      "col:Gender, uniuqe: ['Male' 'Female' 'Fe Male']\n",
      "col:NumberOfPersonVisiting, uniuqe: [3 2 4 1 5]\n",
      "col:NumberOfFollowups, uniuqe: [ 4.  3.  5.  1.  6.  2. nan]\n",
      "col:ProductPitched, uniuqe: ['Basic' 'Deluxe' 'King' 'Standard' 'Super Deluxe']\n",
      "col:PreferredPropertyStar, uniuqe: [ 3.  4.  5. nan]\n",
      "col:MaritalStatus, uniuqe: ['Married' 'Single' 'Divorced' 'Unmarried']\n",
      "col:NumberOfTrips, uniuqe: [ 3.  1.  2.  7.  4.  6.  5.  8. nan 19.]\n",
      "col:Passport, uniuqe: [0 1]\n",
      "col:PitchSatisfactionScore, uniuqe: [1 5 4 3 2]\n",
      "col:OwnCar, uniuqe: [0 1]\n",
      "col:NumberOfChildrenVisiting, uniuqe: [ 1.  0.  2.  3. nan]\n",
      "col:Designation, uniuqe: ['Executive' 'Manager' 'VP' 'Senior Manager' 'AVP']\n",
      "col:MonthlyIncome, uniuqe: [20384. 19599.    nan ... 31595. 22218. 17853.]\n",
      "col:ProdTaken, uniuqe: [0 1]\n"
     ]
    }
   ],
   "source": [
    "for col in train_df_2.columns:\n",
    "    print(f'col:{col}, uniuqe: {train_df_2[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae24c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
